{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c2bef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d368d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MATEOCRACK\\AppData\\Local\\Temp\\ipykernel_14056\\1861678415.py:1: DtypeWarning: Columns (124,171,196,354,355,356,357,358,365) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv('Dataset_csv_unificado_completo/Totalizadores_TODO.csv')\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('Dataset_csv_unificado_completo/Totalizadores_TODO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "758f062f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(662416, 414)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8452c1b",
   "metadata": {},
   "source": [
    "# Construcci√≥n del dataset\n",
    "El dataset cuenta con multiples registros sobre diferentes features relacionadas con una fabrica de cerveza que producen diferentes tipos. El dataset cuenta con (multiples) registros diarios sobre el estado de estas diferentes features, pero tambi√©n con contenido basura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cfaedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset():\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"Retorna la forma ACTUAL del dataframe interno.\"\"\"\n",
    "        return self.df.shape\n",
    "\n",
    "    def columnas_problematicas(self):\n",
    "        \"\"\"\n",
    "        Identifica, elimina y reporta columnas problem√°ticas:\n",
    "        1. Columnas completamente vac√≠as (todos sus valores son nulos).\n",
    "        2. Columnas que contienen 'Unnamed' en su nombre (comunes en CSVs).\n",
    "        3. Columnas cuyo nombre es un n√∫mero (ej. \"1\", \"2.5\").\n",
    "        4. Columnas cuyo nombre es un espacio en blanco (' ').\n",
    "        5. Columnas de metadatos espec√≠ficas (ej. 'Ultimo Dato del Dia').\n",
    "        \n",
    "        Retorna:\n",
    "            (list): Una lista con los nombres de todas las columnas eliminadas.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"--------- Buscando columnas problem√°ticas ---------\")\n",
    "        \n",
    "        # --- L√≥gica 1: Detectar columnas TOTALMENTE NULAS ---\n",
    "        columnas_totalmente_nulas = self.df.columns[self.df.isnull().all()].tolist()\n",
    "        \n",
    "        if columnas_totalmente_nulas:\n",
    "            print(f\"üßπ Detectadas {len(columnas_totalmente_nulas)} columnas totalmente nulas:\")\n",
    "            print(f\"   -> {columnas_totalmente_nulas}\")\n",
    "        else:\n",
    "            print(\"üëç No se encontraron columnas totalmente nulas.\")\n",
    "            \n",
    "        # --- L√≥gica 2: Detectar columnas 'Unnamed' ---\n",
    "        # Usamos str(col).lower() para ser robustos y case-insensitive\n",
    "        columnas_unnamed = [col for col in self.df.columns if 'unnamed' in str(col).lower()]\n",
    "        \n",
    "        if columnas_unnamed:\n",
    "            print(f\"üßπ Detectadas {len(columnas_unnamed)} columnas 'Unnamed':\")\n",
    "            print(f\"   -> {columnas_unnamed}\")\n",
    "        else:\n",
    "            print(\"üëç No se encontraron columnas 'Unnamed'.\")\n",
    "\n",
    "        # --- L√≥gica 3: Detectar columnas con nombre num√©rico ---\n",
    "        columnas_nombre_numerico = []\n",
    "        for col in self.df.columns:\n",
    "            try:\n",
    "                # Convertir a str por si el nombre ya es un int o float\n",
    "                float(str(col))\n",
    "                # Si esto funciona, el nombre de la columna es un n√∫mero\n",
    "                columnas_nombre_numerico.append(col)\n",
    "            except ValueError:\n",
    "                # Si falla, el nombre no es un n√∫mero (ej. \"Planta (Kw)\"), \n",
    "                # lo cual es correcto.\n",
    "                pass\n",
    "        \n",
    "        if columnas_nombre_numerico:\n",
    "            print(f\"üßπ Detectadas {len(columnas_nombre_numerico)} columnas con nombre num√©rico:\")\n",
    "            print(f\"   -> {columnas_nombre_numerico}\")\n",
    "        else:\n",
    "            print(\"üëç No se encontraron columnas con nombre num√©rico.\")\n",
    "\n",
    "        # --- L√≥gica 4: Detectar columnas con nombre de espacio ---\n",
    "        columnas_espacio_en_blanco = [col for col in self.df.columns if str(col) == ' ']\n",
    "        \n",
    "        if columnas_espacio_en_blanco:\n",
    "            print(f\"üßπ Detectadas {len(columnas_espacio_en_blanco)} columnas con nombre de espacio (' '):\")\n",
    "            print(f\"   -> {columnas_espacio_en_blanco}\")\n",
    "        else:\n",
    "            print(\"üëç No se encontraron columnas con nombre de espacio.\")\n",
    "\n",
    "        # --- L√≥gica 5: Detectar columnas de metadatos (NUEVO) ---\n",
    "        columna_metadato_especifica = 'Ultimo Dato del Dia'\n",
    "        columnas_metadatos = []\n",
    "        if columna_metadato_especifica in self.df.columns:\n",
    "            columnas_metadatos.append(columna_metadato_especifica)\n",
    "            print(f\"üßπ Detectada columna de metadatos: ['{columna_metadato_especifica}']\")\n",
    "        else:\n",
    "            print(\"üëç No se encontr√≥ la columna de metadatos ('Ultimo Dato del Dia').\")\n",
    "\n",
    "\n",
    "        # --- Combinar y Eliminar ---\n",
    "        # Usamos un 'set' para combinar las listas y evitar duplicados\n",
    "        columnas_a_eliminar = list(set(\n",
    "            columnas_totalmente_nulas + \n",
    "            columnas_unnamed + \n",
    "            columnas_nombre_numerico +\n",
    "            columnas_espacio_en_blanco +\n",
    "            columnas_metadatos  # <-- A√±adimos la nueva lista\n",
    "        ))\n",
    "        \n",
    "        if columnas_a_eliminar:\n",
    "            print(f\"\\nüóëÔ∏è Eliminando un total de {len(columnas_a_eliminar)} columnas...\")\n",
    "            self.df.drop(columns=columnas_a_eliminar, inplace=True)\n",
    "            print(f\"üìä Limpieza de columnas finalizada.\")\n",
    "        else:\n",
    "            print(\"\\nüëç No hay columnas problem√°ticas para eliminar.\")\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "        # Retornamos la lista de lo que se elimin√≥\n",
    "        return columnas_a_eliminar\n",
    "\n",
    "    # -----------------------\n",
    "    # Limpieza de FILAS\n",
    "    # -----------------------\n",
    "    def filas_problematicas(self, columnas_clave):\n",
    "        \"\"\"\n",
    "        Elimina y reporta filas donde columnas esenciales (como 'DIA' o 'HORA')\n",
    "        tienen valores nulos.\n",
    "        \n",
    "        Par√°metros:\n",
    "            columnas_clave (list): Lista de columnas que no pueden tener nulos.\n",
    "        \n",
    "        Retorna:\n",
    "            (int): El n√∫mero total de filas eliminadas.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"--------- Buscando filas problem√°ticas en {columnas_clave} ---------\")\n",
    "        filas_eliminadas_total = 0\n",
    "        \n",
    "        for col in columnas_clave:\n",
    "            if col in self.df.columns:\n",
    "                filas_antes = len(self.df)\n",
    "                self.df.dropna(subset=[col], inplace=True)\n",
    "                filas_despues = len(self.df)\n",
    "                filas_eliminadas_ronda = filas_antes - filas_despues\n",
    "                \n",
    "                if filas_eliminadas_ronda > 0:\n",
    "                    print(f\"‚úÖ Eliminadas {filas_eliminadas_ronda} filas con '{col}' vac√≠o\")\n",
    "                    filas_eliminadas_total += filas_eliminadas_ronda\n",
    "            \n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Aviso: La columna clave '{col}' no existe. No se limpiaron filas.\")\n",
    "\n",
    "        if filas_eliminadas_total == 0:\n",
    "            print(\"üëç No se encontraron filas problem√°ticas para eliminar.\")\n",
    "        else:\n",
    "            print(f\"üìä Limpieza de filas finalizada. Total eliminadas: {filas_eliminadas_total}.\")\n",
    "            \n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "        return filas_eliminadas_total\n",
    "        \n",
    "\n",
    "    # -----------------------\n",
    "    # Valores negativos\n",
    "    # -----------------------\n",
    "    def valores_negativos_por_columna(self, columnas_permiten_negativos: list):\n",
    "        \"\"\"\n",
    "        Identifica todas las columnas num√©ricas que tienen valores negativos\n",
    "        y los reemplaza por np.nan.\n",
    "        \n",
    "        Excluye del reemplazo a las columnas especificadas en la lista\n",
    "        'columnas_permiten_negativos' (ej. 'Temp Tq Intermedio').\n",
    "        \n",
    "        Par√°metros:\n",
    "            columnas_permiten_negativos (list): Lista de columnas que S√ç \n",
    "                                                pueden ser negativas.\n",
    "        \n",
    "        Retorna:\n",
    "            (int): El n√∫mero total de celdas reemplazadas.\n",
    "        \"\"\"\n",
    "        print(f\"--------- Tratando valores negativos ---------\")\n",
    "        \n",
    "        # 1. Identificar todas las columnas num√©ricas del dataframe\n",
    "        columnas_numericas = self.df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        # 2. Convertir la lista de excepciones a un 'set' para b√∫squedas m√°s r√°pidas\n",
    "        set_excepciones = set(columnas_permiten_negativos)\n",
    "\n",
    "        total_reemplazos = 0\n",
    "        columnas_afectadas = 0\n",
    "        \n",
    "        print(f\"‚ÑπÔ∏è Excepciones (columnas que se ignorar√°n): {list(set_excepciones)}\")\n",
    "        \n",
    "        # 3. Iterar sobre todas las columnas num√©ricas\n",
    "        for col in columnas_numericas:\n",
    "            \n",
    "            # 4. Si la columna est√° en las excepciones, la saltamos\n",
    "            if col in set_excepciones:\n",
    "                continue\n",
    "                \n",
    "            # 5. Si no es una excepci√≥n, buscamos negativos\n",
    "            \n",
    "            # Creamos una m√°scara (un True/False) para los valores < 0\n",
    "            # Usamos .loc para asegurarnos de que no haya errores de tipo\n",
    "            try:\n",
    "                mascara_negativos = self.df[col] < 0\n",
    "            except TypeError:\n",
    "                # Esto puede pasar si la columna es num√©rica pero tiene\n",
    "                # objetos mixtos. Mejor saltarla.\n",
    "                print(f\"‚ö†Ô∏è - Omitiendo columna '{col}' (posible tipo de dato mixto).\")\n",
    "                continue\n",
    "\n",
    "            # Contamos cu√°ntos negativos hay\n",
    "            negativos_count = mascara_negativos.sum()\n",
    "            \n",
    "            if negativos_count > 0:\n",
    "                # 6. Reemplazamos esos valores por np.nan\n",
    "                # Usamos .loc para modificar self.df de forma segura y directa\n",
    "                self.df.loc[mascara_negativos, col] = np.nan\n",
    "                \n",
    "                print(f\"  - Columna '{col}': {negativos_count} valores negativos convertidos a NaN.\")\n",
    "                total_reemplazos += negativos_count\n",
    "                columnas_afectadas += 1\n",
    "\n",
    "        if total_reemplazos == 0:\n",
    "            print(\"üëç No se encontraron valores negativos no deseados.\")\n",
    "        else:\n",
    "             print(f\"üìä Tratamiento finalizado. {total_reemplazos} celdas reemplazadas en {columnas_afectadas} columnas.\")\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "        # Retornamos el conteo por si es √∫til\n",
    "        return total_reemplazos\n",
    "\n",
    "    # -----------------------\n",
    "    # Redondeo horarios\n",
    "    # -----------------------\n",
    "    def redondear_horarios(self):\n",
    "        col = \"HORA\"\n",
    "        if col not in self.df.columns:\n",
    "            return self.df\n",
    "\n",
    "        # Convertir a datetime\n",
    "        self.df[col] = pd.to_datetime(self.df[col], format=\"%H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "        def redondear(hora):\n",
    "            if pd.isna(hora):\n",
    "                return None\n",
    "\n",
    "            # √öltima hora del d√≠a\n",
    "            if hora.hour == 23 and hora.minute == 59 and hora.second !=59:\n",
    "                return \"23:59:59\" #aca cambie, en teoria no deberia afectar: \n",
    "\n",
    "            # Redondear a la hora m√°s cercana\n",
    "            if hora.minute >= 30:\n",
    "                nueva_hora = (hora + pd.Timedelta(hours=1)).replace(minute=0, second=0)\n",
    "            else:\n",
    "                nueva_hora = hora.replace(minute=0, second=0)\n",
    "\n",
    "            # Si se pasa de d√≠a, dejar 23:59:59\n",
    "            if nueva_hora.hour == 0 and hora.hour == 23:\n",
    "                return \"23:59:59\"\n",
    "\n",
    "            return nueva_hora.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        self.df[col] = self.df[col].apply(redondear)\n",
    "        return self.df\n",
    "\n",
    "    def add_estacion(self):\n",
    "      \"\"\"\n",
    "      Agrega una columna 'ESTACION' al dataframe con la estaci√≥n del a√±o\n",
    "      (Primavera, Verano, Oto√±o, Invierno) seg√∫n la fecha en M√©xico (hemisferio norte).\n",
    "      Requiere que self.df tenga la columna 'DIA' en formato datetime.\n",
    "      \"\"\"\n",
    "      if \"DIA\" not in self.df.columns:\n",
    "          print(\"‚ùå No existe la columna 'DIA' en el dataframe.\")\n",
    "          return\n",
    "\n",
    "      # Asegurar formato datetime\n",
    "      self.df[\"DIA\"] = pd.to_datetime(self.df[\"DIA\"], errors=\"coerce\")\n",
    "\n",
    "      def obtener_estacion(fecha):\n",
    "          if pd.isna(fecha):\n",
    "              return None\n",
    "\n",
    "          a√±o = fecha.year\n",
    "          # Fechas de cambio de estaci√≥n (en hemisferio norte)\n",
    "          primavera = (pd.Timestamp(year=a√±o, month=3, day=21), pd.Timestamp(year=a√±o, month=6, day=20))\n",
    "          verano = (pd.Timestamp(year=a√±o, month=6, day=21), pd.Timestamp(year=a√±o, month=9, day=22))\n",
    "          otonio = (pd.Timestamp(year=a√±o, month=9, day=23), pd.Timestamp(year=a√±o, month=12, day=20))\n",
    "          # Invierno cruza el a√±o\n",
    "          invierno_1 = (pd.Timestamp(year=a√±o, month=12, day=21), pd.Timestamp(year=a√±o + 1, month=3, day=20))\n",
    "\n",
    "          if primavera[0] <= fecha <= primavera[1]:\n",
    "              return \"Primavera\"\n",
    "          elif verano[0] <= fecha <= verano[1]:\n",
    "              return \"Verano\"\n",
    "          elif otonio[0] <= fecha <= otonio[1]:\n",
    "              return \"Oto√±o\"\n",
    "          elif (fecha >= invierno_1[0]) or (fecha <= invierno_1[1]):\n",
    "              return \"Invierno\"\n",
    "          else:\n",
    "              return None\n",
    "\n",
    "      self.df[\"ESTACION\"] = self.df[\"DIA\"].apply(obtener_estacion)\n",
    "      print(\"‚úÖ Columna 'ESTACION' agregada correctamente.\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Agregar temperatura y d√≠a de la semana\n",
    "    # -----------------------\n",
    "    def add_temp_y_dia(self, temp_df):\n",
    "    # Columnas fecha\n",
    "      if 'DIA' not in self.df.columns or 'fecha' not in temp_df.columns:\n",
    "          return\n",
    "\n",
    "      self.df[\"DIA\"] = pd.to_datetime(self.df[\"DIA\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "      temp_df[\"fecha\"] = pd.to_datetime(temp_df[\"fecha\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "      # Columna combinada fecha-hora\n",
    "      self.df[\"fecha_hora\"] = pd.to_datetime(\n",
    "          self.df[\"DIA\"].astype(str) + \" \" + self.df[\"HORA\"].astype(str),\n",
    "          format=\"%Y-%m-%d %H:%M:%S\",\n",
    "          errors=\"coerce\"\n",
    "      )\n",
    "      temp_df[\"fecha_hora\"] = pd.to_datetime(\n",
    "          temp_df[\"fecha\"].astype(str) + \" \" + temp_df[\"hora\"].astype(str),\n",
    "          format=\"%Y-%m-%d %H:%M:%S\",\n",
    "          errors=\"coerce\"\n",
    "      )\n",
    "\n",
    "      # Caso especial 23:59:59 ‚Üí d√≠a siguiente 00:00:00\n",
    "      mask_2359 = self.df[\"HORA\"] == \"23:59:59\"\n",
    "      self.df.loc[mask_2359, \"fecha_hora\"] = self.df.loc[mask_2359, \"fecha_hora\"] + pd.Timedelta(days=1)\n",
    "      self.df.loc[mask_2359, \"fecha_hora\"] = self.df.loc[mask_2359, \"fecha_hora\"].dt.floor(\"D\")\n",
    "\n",
    "      # Merge para traer temperatura horaria\n",
    "      self.df = pd.merge(\n",
    "          self.df,\n",
    "          temp_df[[\"fecha_hora\", \"temperature_2m\"]],\n",
    "          on=\"fecha_hora\",\n",
    "          how=\"left\"\n",
    "      )\n",
    "\n",
    "      # Limpiar columna auxiliar\n",
    "      self.df.drop(columns=[\"fecha_hora\"], inplace=True)\n",
    "\n",
    "      # Agregar d√≠a de la semana\n",
    "      if \"DIA\" in self.df.columns:\n",
    "          self.df[\"DIA_SEMANA\"] = self.df[\"DIA\"].dt.day_name()\n",
    "\n",
    "      # --- NUEVO: agregar temperatura promedio por d√≠a ---\n",
    "      temp_promedio = temp_df.groupby(\"fecha\")[\"temperature_2m\"].mean().reset_index()\n",
    "      temp_promedio.rename(columns={\"temperature_2m\": \"temperature_promedio_dia\"}, inplace=True)\n",
    "\n",
    "      self.df = pd.merge(\n",
    "          self.df,\n",
    "          temp_promedio,\n",
    "          left_on=\"DIA\",\n",
    "          right_on=\"fecha\",\n",
    "          how=\"left\"\n",
    "      )\n",
    "      # Limpiar columna auxiliar\n",
    "      self.df.drop(columns=[\"fecha\"], inplace=True)\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # Promedio de EE Frio / Hl por hora\n",
    "    # -----------------------\n",
    "    def promedio_frio_por_hora(self):\n",
    "        if \"HORA\" not in self.df.columns or \"EE Frio / Hl\" not in self.df.columns:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        self.df[\"HORA\"] = pd.to_datetime(self.df[\"HORA\"], format=\"%H:%M:%S\", errors=\"coerce\").dt.time\n",
    "        resumen = (\n",
    "            self.df.groupby(\"HORA\", as_index=False)[\"EE Frio / Hl\"]\n",
    "            .mean()\n",
    "            .rename(columns={\"EE Frio / Hl\": \"frio_promedio\"})\n",
    "            .sort_values(\"HORA\")\n",
    "        )\n",
    "        return resumen\n",
    "    \n",
    "    def verificar_duplicado(self, col1, col2):\n",
    "        \"\"\"\n",
    "        Verifica si dos columnas del DataFrame interno (self.df) son duplicadas.\n",
    "        \n",
    "        Casos que comprueba:\n",
    "        1. Duplicado Perfecto (id√©nticas, incluyendo NaNs).\n",
    "        2. Duplicado con NaNs (id√©nticas en todos los valores no-nulos).\n",
    "        3. No duplicadas.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n--- Verificando: '{col1}' vs '{col2}' ---\")\n",
    "        \n",
    "        # --- 1. Chequear existencia ---\n",
    "        columnas_faltantes = []\n",
    "        if col1 not in self.df.columns:\n",
    "            columnas_faltantes.append(col1)\n",
    "        if col2 not in self.df.columns:\n",
    "            columnas_faltantes.append(col2)\n",
    "            \n",
    "        if columnas_faltantes:\n",
    "            print(f\"  ‚ùå Error: Las siguientes columnas no se encontraron: {columnas_faltantes}.\")\n",
    "            print(\"     Por favor, revisa que los nombres sean exactos (may√∫sculas, espacios, etc.)\")\n",
    "            return\n",
    "\n",
    "        # --- 2. Test 1: Duplicado Perfecto (con .equals()) ---\n",
    "        if self.df[col1].equals(self.df[col2]):\n",
    "            print(\"  ‚úÖ Resultado: ID√âNTICAS (Perfect match).\")\n",
    "            print(f\"     (Puedes eliminar una, por ejemplo '{col2}')\")\n",
    "            return\n",
    "\n",
    "        # --- 3. Test 2: Duplicado con diferencia de NaNs ---\n",
    "        mask_non_null = self.df[col1].notna() & self.df[col2].notna()\n",
    "        \n",
    "        if mask_non_null.sum() == 0:\n",
    "            print(\"  ‚ö†Ô∏è Resultado: No se pueden comparar (nunca tienen valores al mismo tiempo).\")\n",
    "            return\n",
    "            \n",
    "        diferencias = (self.df.loc[mask_non_null, col1] != self.df.loc[mask_non_null, col2])\n",
    "        num_diferencias = diferencias.sum()\n",
    "\n",
    "        if num_diferencias == 0:\n",
    "            print(\"  ‚ö†Ô∏è Resultado: DUPLICADAS (Difieren solo en la posici√≥n de los NaNs).\")\n",
    "            print(\"     (Son id√©nticas en todas las filas donde ambas tienen datos).\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Resultado: NO SON DUPLICADAS.\")\n",
    "            print(f\"     (Tienen {num_diferencias} valores diferentes donde ambas tienen datos).\")\n",
    "\n",
    "        nan_1 = self.df[col1].isna().sum()\n",
    "        nan_2 = self.df[col2].isna().sum()\n",
    "        print(f\"     Contexto: '{col1}' tiene {nan_1} NaNs. '{col2}' tiene {nan_2} NaNs.\")\n",
    "\n",
    "\n",
    "    def verificar_relevo(self, col1, col2, assume_sorted_index=True):\n",
    "        \"\"\"\n",
    "        Verifica si una columna 'toma el relevo' de otra en self.df.\n",
    "        \n",
    "        Retorna True si se cumplen dos condiciones:\n",
    "        1. Ambas columnas tienen *algunos* datos (no est√°n 100% vac√≠as).\n",
    "        2. NUNCA tienen datos en la misma fila (no hay superposici√≥n).\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n--- Verificando relevo: '{col1}' vs '{col2}' ---\")\n",
    "        \n",
    "        # --- 1. Chequear existencia ---\n",
    "        if col1 not in self.df.columns or col2 not in self.df.columns:\n",
    "            print(f\"  ‚ùå Error: Una o ambas columnas no se encontraron.\")\n",
    "            return False\n",
    "\n",
    "        # --- 2. Obtener m√°scaras de datos v√°lidos (no-NaN) ---\n",
    "        validos_col1 = self.df[col1].notna()\n",
    "        validos_col2 = self.df[col2].notna()\n",
    "\n",
    "        # --- 3. Condici√≥n 1: ¬øAmbas columnas tienen *algunos* datos? ---\n",
    "        ambas_tienen_datos = validos_col1.any() and validos_col2.any()\n",
    "        \n",
    "        if not ambas_tienen_datos:\n",
    "            print(\"  ‚ùå FALSO: Una o ambas columnas est√°n 100% vac√≠as (NaN).\")\n",
    "            return False\n",
    "\n",
    "        # --- 4. Condici√≥n 2: ¬øHay superposici√≥n? (Debe ser 0) ---\n",
    "        superposicion = (validos_col1 & validos_col2).sum()\n",
    "        \n",
    "        if superposicion > 0:\n",
    "            print(f\"  ‚ùå FALSO: Las columnas se superponen en {superposicion} filas.\")\n",
    "            return False\n",
    "\n",
    "        # Si llegamos aqu√≠, ambas condiciones se cumplen.\n",
    "        print(\"  ‚úÖ VERDADERO: Las columnas tienen datos pero nunca se superponen.\")\n",
    "        print(\"     (Esto confirma el patr√≥n de 'relevo' de datos).\")\n",
    "        \n",
    "        # --- 5. (Opcional) Proveer m√°s contexto si el √≠ndice est√° ordenado ---\n",
    "        if assume_sorted_index:\n",
    "            try:\n",
    "                fin_col1 = self.df.index[validos_col1].max()\n",
    "                inicio_col2 = self.df.index[validos_col2].min()\n",
    "                fin_col2 = self.df.index[validos_col2].max()\n",
    "                inicio_col1 = self.df.index[validos_col1].min()\n",
    "\n",
    "                if fin_col1 < inicio_col2:\n",
    "                    print(f\"     (Patr√≥n limpio: '{col1}' termina en {fin_col1}, luego '{col2}' empieza en {inicio_col2})\")\n",
    "                elif fin_col2 < inicio_col1:\n",
    "                    print(f\"     (Patr√≥n limpio: '{col2}' termina en {fin_col2}, luego '{col1}' empieza en {inicio_col1})\")\n",
    "                else:\n",
    "                    print(\"     (Los datos est√°n intercalados, no en un bloque 'antes' y 'despu√©s')\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"     (No se pudo verificar el orden cronol√≥gico del relevo: {e})\")\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    # -----------------------\n",
    "    # Verificaci√≥n de Consistencia\n",
    "    # -----------------------\n",
    "\n",
    "    def verificar_consistencia_binaria(self, columnas_binarias: list):\n",
    "        \"\"\"\n",
    "        Verifica si hay inconsistencias en columnas binarias para tuplas (DIA, HORA) duplicadas.\n",
    "\n",
    "        Una inconsistencia ocurre si para el mismo (DIA, HORA), una columna binaria\n",
    "        tiene valores diferentes (ej. 0 y 1), ignorando los NaN.\n",
    "        \n",
    "        Args:\n",
    "            columnas_binarias (list): La lista de columnas binarias a verificar.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (bool, dict)\n",
    "                - bool: True si se encontraron inconsistencias, False si no.\n",
    "                - dict: Un diccionario donde las claves son las tuplas (DIA, HORA)\n",
    "                        problem√°ticas y los valores son las columnas con\n",
    "                        inconsistencias.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"--- Iniciando verificaci√≥n de consistencia en tuplas (DIA, HORA) duplicadas ---\")\n",
    "        \n",
    "        # 1. Agrupar por la tupla clave\n",
    "        # Usamos el DataFrame interno 'self.df'\n",
    "        grupos = self.df.groupby(['DIA', 'HORA'])\n",
    "        \n",
    "        problematic_groups = {}\n",
    "        hay_inconsistencia = False\n",
    "        \n",
    "        grupos_con_duplicados = 0\n",
    "        \n",
    "        # 2. Iterar sobre cada grupo (DIA, HORA)\n",
    "        for (dia, hora), group in grupos:\n",
    "            \n",
    "            # Solo nos interesan los grupos con duplicados\n",
    "            if len(group) > 1:\n",
    "                grupos_con_duplicados += 1\n",
    "                inconsistencias_en_col = []\n",
    "                \n",
    "                # 3. Revisar cada columna binaria dentro de ese grupo\n",
    "                for col in columnas_binarias:\n",
    "                    if col not in group.columns:\n",
    "                        continue # Seguridad por si la columna no existe\n",
    "                    \n",
    "                    # 4. Obtener los valores √∫nicos NO-NAN\n",
    "                    # Esta es la l√≥gica clave: ignoramos los NaN\n",
    "                    valores_unicos = group[col].dropna().unique()\n",
    "                    \n",
    "                    # 5. Si hay m√°s de 1 valor √∫nico (ej. [0.0, 1.0]), es una inconsistencia\n",
    "                    if len(valores_unicos) > 1:\n",
    "                        hay_inconsistencia = True\n",
    "                        inconsistencias_en_col.append(col)\n",
    "                \n",
    "                # 6. Si encontramos columnas inconsistentes, guardamos el reporte\n",
    "                if inconsistencias_en_col:\n",
    "                    problematic_groups[(dia, hora)] = inconsistencias_en_col\n",
    "\n",
    "        print(f\"\\nRevisados {len(grupos)} grupos ('DIA', 'HORA') √∫nicos.\")\n",
    "        print(f\"Se encontraron {grupos_con_duplicados} grupos con filas duplicadas.\")\n",
    "        \n",
    "        if hay_inconsistencia:\n",
    "            print(f\"‚ùå ¬°Problema! Se encontraron {len(problematic_groups)} tuplas (DIA, HORA) con datos binarios inconsistentes.\")\n",
    "        else:\n",
    "            print(\"‚úÖ ¬°√âxito! Todas las tuplas (DIA, HORA) duplicadas son consistentes en las columnas binarias.\")\n",
    "            \n",
    "        return hay_inconsistencia, problematic_groups\n",
    "    \n",
    "    def verificar_suma_jerarquica(self, hipotesis_dict):\n",
    "        \"\"\"\n",
    "        Verifica si columnas \"Totales\" son la suma de sus \"partes\".\n",
    "        \n",
    "        Recibe un diccionario donde cada clave es la columna \"Total\" y\n",
    "        cada valor es una lista de las columnas \"Parte\".\n",
    "        \n",
    "        Args:\n",
    "            hipotesis_dict (dict): Diccionario de hip√≥tesis. \n",
    "                                   Ej: {'Total': ['Parte1', 'Parte2']}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"--- Iniciando verificaci√≥n de {len(hipotesis_dict)} sumas jer√°rquicas ---\")\n",
    "        \n",
    "        for col_total, lista_partes in hipotesis_dict.items():\n",
    "            print(f\"\\n--- Verificando: '{col_total}' vs Suma de {len(lista_partes)} partes ---\")\n",
    "            \n",
    "            # --- 1. Verificar que todas las columnas existan ---\n",
    "            columnas_a_chequear = [col_total] + lista_partes\n",
    "            columnas_faltantes = [col for col in columnas_a_chequear if col not in self.df.columns]\n",
    "            \n",
    "            if columnas_faltantes:\n",
    "                print(f\"  ‚ùå Error: Faltan columnas: {columnas_faltantes}. Saltando.\")\n",
    "                continue\n",
    "                \n",
    "            # --- 2. Sumamos las partes ---\n",
    "            # fillna(0) es crucial por si una de las partes tiene NaN\n",
    "            suma_calculada = self.df[lista_partes].fillna(0).sum(axis=1)\n",
    "            \n",
    "            # --- 3. Comparamos el total reportado vs. la suma calculada ---\n",
    "            # Usamos np.allclose para comparar n√∫meros flotantes, lo que\n",
    "            # permite peque√±os errores de precisi√≥n (tolerancia).\n",
    "            son_iguales = np.allclose(\n",
    "                self.df[col_total].fillna(0),  # Rellenamos NaNs en el total\n",
    "                suma_calculada, \n",
    "                rtol=1e-03, # Tolerancia relativa (0.1%)\n",
    "                atol=1e-05  # Tolerancia absoluta\n",
    "            )\n",
    "\n",
    "            if son_iguales:\n",
    "                print(f\"  ‚úÖ Resultado: REDUNDANCIA CONFIRMADA.\")\n",
    "                print(f\"     ('{col_total}' es la suma de sus partes).\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Resultado: NO SON REDUNDANTES.\")\n",
    "                print(f\"     ('{col_total}' NO es la suma simple de sus partes).\")\n",
    "                \n",
    "                # Mostramos un resumen de la diferencia\n",
    "                try:\n",
    "                    diferencia = (self.df[col_total] - suma_calculada).abs()\n",
    "                    print(f\"     Diferencia promedio: {diferencia.mean():.4f}\")\n",
    "                    print(f\"     Diferencia m√°xima:   {diferencia.max():.4f}\")\n",
    "                except TypeError:\n",
    "                    print(\"     (No se pudo calcular la diferencia, posible error de tipos).\")\n",
    "\n",
    "    def verificar_exclusividad_mutua(self, hipotesis_grupos):\n",
    "        \"\"\"\n",
    "        Verifica si grupos de columnas binarias (dummies) son mutuamente excluyentes.\n",
    "\n",
    "        La prueba comprueba que para cualquier fila, la suma de las columnas\n",
    "        del grupo sea como m√°ximo 1 (es decir, solo una puede estar \"activa\").\n",
    "        \n",
    "        Args:\n",
    "            hipotesis_grupos (dict): Diccionario de hip√≥tesis.\n",
    "                                   Ej: {'Grupo Mostos': ['Mosto1', 'Mosto2']}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"--- Iniciando verificaci√≥n de {len(hipotesis_grupos)} grupos mutuamente excluyentes ---\")\n",
    "\n",
    "        for nombre_grupo, lista_columnas in hipotesis_grupos.items():\n",
    "            print(f\"\\n--- Verificando Grupo: '{nombre_grupo}' ({len(lista_columnas)} columnas) ---\")\n",
    "\n",
    "            # --- 1. Verificar que todas las columnas existan ---\n",
    "            columnas_faltantes = [col for col in lista_columnas if col not in self.df.columns]\n",
    "            \n",
    "            if columnas_faltantes:\n",
    "                print(f\"  ‚ùå Error: Faltan columnas: {columnas_faltantes}. Saltando.\")\n",
    "                continue\n",
    "            \n",
    "            # --- 2. Calcular la suma por fila (asumiendo NaN=0) ---\n",
    "            try:\n",
    "                # fillna(0) es clave para tratar los NaNs como \"inactivo\"\n",
    "                suma_por_fila = self.df[lista_columnas].fillna(0).sum(axis=1)\n",
    "            except TypeError:\n",
    "                print(f\"  ‚ùå Error: No se pudo sumar. ¬øAlguna columna no es num√©rica? Saltando.\")\n",
    "                # Esto puede pasar si las columnas est√°n mal identificadas (ej. 'Totalizador Bba P1' \n",
    "                # que sali√≥ como 'object' pero se quiere tratar como binaria)\n",
    "                continue\n",
    "\n",
    "            # --- 3. Analizar el resultado ---\n",
    "            max_suma = suma_por_fila.max()\n",
    "            \n",
    "            if max_suma <= 1:\n",
    "                print(f\"  ‚úÖ Resultado: SON MUTUAMENTE EXCLUYENTES.\")\n",
    "                print(f\"     (Nunca hay m√°s de una columna 'activa' (1) al mismo tiempo).\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Resultado: NO SON MUTUAMENTE EXCLUYENTES.\")\n",
    "                filas_violadas = (suma_por_fila > 1).sum()\n",
    "                print(f\"     (Se encontraron {filas_violadas} filas donde la suma es > 1).\")\n",
    "                print(f\"     (La suma m√°xima encontrada en una fila fue: {max_suma}).\")\n",
    "\n",
    "    def forzar_conversion_numerica(self, columnas_a_convertir: list):\n",
    "        \"\"\"\n",
    "        Fuerza la conversi√≥n de una lista de columnas a tipo num√©rico.\n",
    "        \n",
    "        Utiliza pd.to_numeric con errors='coerce', que convertir√° \n",
    "        autom√°ticamente cualquier valor no num√©rico (ej. \"Error\") en NaN.\n",
    "        \n",
    "        Args:\n",
    "            columnas_a_convertir (list): Lista de nombres de columnas.\n",
    "            \n",
    "        Returns:\n",
    "            int: El n√∫mero de columnas que cambiaron su tipo de dato.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"--- Forzando conversi√≥n a num√©rico en {len(columnas_a_convertir)} columnas ---\")\n",
    "        \n",
    "        columnas_convertidas = 0\n",
    "        for col in columnas_a_convertir:\n",
    "            if col in self.df.columns:\n",
    "                tipo_anterior = self.df[col].dtype\n",
    "                \n",
    "                # Aplicamos la conversi√≥n robusta\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "                \n",
    "                tipo_nuevo = self.df[col].dtype\n",
    "                \n",
    "                if tipo_anterior != tipo_nuevo:\n",
    "                    print(f\"  ‚úÖ Columna '{col}': Convertida ({tipo_anterior} -> {tipo_nuevo})\")\n",
    "                    columnas_convertidas += 1\n",
    "                else:\n",
    "                    print(f\"  ‚ÑπÔ∏è Columna '{col}': Ya era num√©rica ({tipo_nuevo}).\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"  ‚ùå Error: No se encontr√≥ la columna '{col}'.\")\n",
    "\n",
    "        print(f\"\\nConversi√≥n completada. Se convirtieron {columnas_convertidas} columnas.\")\n",
    "        return columnas_convertidas\n",
    "    \n",
    "\n",
    "    # -----------------------\n",
    "    # Agrupaci√≥n y Consolidaci√≥n\n",
    "    # -----------------------\n",
    "\n",
    "    def agrupar_por_media(self, columnas_binarias: list, columnas_texto: list = None):\n",
    "        \"\"\"\n",
    "        [MODIFICADO] Agrupa el DataFrame interno (self.df) por (DIA) para \n",
    "        consolidar todas las filas en un PROMEDIO DIARIO.\n",
    "        \n",
    "        ¬°ADVERTENCIA: Esto elimina la resoluci√≥n horaria!\n",
    "        \n",
    "        - Usa 'mean' (promedio) para columnas num√©ricas.\n",
    "        - Usa 'max' para columnas binarias/dummies (si hubo un 1 en el d√≠a, queda 1).\n",
    "        - Usa 'first' para columnas de texto.\n",
    "        \n",
    "        El DataFrame self.df se actualiza internamente.\n",
    "        \n",
    "        Args:\n",
    "            columnas_binarias (list): Lista de nombres de las columnas binarias.\n",
    "            columnas_texto (list, optional): Lista de otras columnas de texto \n",
    "                                              (aparte de DIA) que deban mantenerse.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"--- Iniciando agrupaci√≥n por (DIA) usando la media (Promedio Diario) ---\")\n",
    "        \n",
    "        # 1. Definir las columnas clave (MODIFICADO)\n",
    "        columnas_clave = ['DIA'] # Agrupamos solo por D√çA\n",
    "        \n",
    "        # 2. Crear el diccionario de agregaci√≥n\n",
    "        agg_dict = {}\n",
    "        \n",
    "        # Convertir listas a 'sets' para b√∫squedas r√°pidas\n",
    "        set_binarias = set(columnas_binarias)\n",
    "        set_texto = set(columnas_texto) if columnas_texto else set()\n",
    "        \n",
    "        filas_antes = self.df.shape[0]\n",
    "\n",
    "        # 3. Iterar sobre todas las columnas para construir el diccionario\n",
    "        for col in self.df.columns:\n",
    "            # Ignorar las columnas clave\n",
    "            if col in columnas_clave:\n",
    "                continue\n",
    "            \n",
    "            # Ignorar HORA expl√≠citamente, ya que la estamos perdiendo\n",
    "            if col == 'HORA':\n",
    "                continue\n",
    "                \n",
    "            # Si es una columna binaria...\n",
    "            if col in set_binarias:\n",
    "                agg_dict[col] = 'max'\n",
    "            \n",
    "            # Si es una columna de texto...\n",
    "            elif col in set_texto:\n",
    "                agg_dict[col] = 'first'\n",
    "            \n",
    "            # Si es una columna num√©rica (no-binaria y no-clave)...\n",
    "            elif pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                agg_dict[col] = 'mean'\n",
    "            \n",
    "            # Si es cualquier otra cosa (texto no especificado)...\n",
    "            else:\n",
    "                agg_dict[col] = 'first'\n",
    "                \n",
    "        # 4. Ejecutar la agregaci√≥n\n",
    "        if not agg_dict:\n",
    "            print(\"No hay columnas para agrupar. Abortando.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Agrupando {len(agg_dict)} columnas por (DIA)...\")\n",
    "        \n",
    "        # Agrupamos y reseteamos el √≠ndice\n",
    "        df_agrupado = self.df.groupby(columnas_clave).agg(agg_dict).reset_index()\n",
    "        \n",
    "        # 5. Actualizar el DataFrame interno\n",
    "        self.df = df_agrupado\n",
    "        filas_despues = self.df.shape[0]\n",
    "        \n",
    "        print(\"\\n‚úÖ Agrupaci√≥n Diaria completada.\")\n",
    "        print(f\"   Filas reducidas de {filas_antes} a {filas_despues}.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87dcbd2",
   "metadata": {},
   "source": [
    "Vamos a eliminar columnas problematicas. Para ello consideraremos:\n",
    "- Columnas con todos sus valores nulls o bien alguna alternativa en string (\"none\", \"null\") etc\n",
    "- COlumnas probleamticas (nombres indefinidos como unnamed, .numero, ' ')\n",
    "\n",
    "Mismo analisis para las filas, donde consideraremos:\n",
    "- Filas donde datos claves como dia/hora son nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1ce8a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Buscando columnas problem√°ticas ---------\n",
      "üßπ Detectadas 42 columnas totalmente nulas:\n",
      "   -> ['EE Linea 5 / Hl', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 35', 'Unnamed: 46', 'Unnamed: 65', 'Unnamed: 66', 'Unnamed: 67', 'Unnamed: 72', 'Unnamed: 84', 'Unnamed: 103', 'Unnamed: 113', 'Unnamed: 123', 'Unnamed: 14', ' .3', ' .4', ' .6', ' .8', ' .9', ' .10', ' .11', ' .12', ' .13', ' .14', ' .15', ' .16', ' .17', ' .18', ' .19', ' .20', ' .21', ' .22', ' .23', ' .24', ' .25', ' .26', ' .27', ' .28', ' .29', ' .30', ' .7', ' .31']\n",
      "üßπ Detectadas 27 columnas 'Unnamed':\n",
      "   -> ['Unnamed: 21', 'Unnamed: 22', 'Unnamed: 35', 'Unnamed: 46', 'Unnamed: 65', 'Unnamed: 66', 'Unnamed: 67', 'Unnamed: 72', 'Unnamed: 84', 'Unnamed: 103', 'Unnamed: 113', 'Unnamed: 123', 'Unnamed: 124', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 0', 'Unnamed: 3', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 4']\n",
      "üßπ Detectadas 31 columnas con nombre num√©rico:\n",
      "   -> [' .1', ' .2', ' .3', ' .4', ' .5', ' .6', ' .8', ' .9', ' .10', ' .11', ' .12', ' .13', ' .14', ' .15', ' .16', ' .17', ' .18', ' .19', ' .20', ' .21', ' .22', ' .23', ' .24', ' .25', ' .26', ' .27', ' .28', ' .29', ' .30', ' .7', ' .31']\n",
      "üßπ Detectadas 1 columnas con nombre de espacio (' '):\n",
      "   -> [' ']\n",
      "üßπ Detectada columna de metadatos: ['Ultimo Dato del Dia']\n",
      "\n",
      "üóëÔ∏è Eliminando un total de 61 columnas...\n",
      "üìä Limpieza de columnas finalizada.\n",
      "--------------------------------------------------\n",
      "--------- Buscando filas problem√°ticas en ['DIA', 'HORA'] ---------\n",
      "‚úÖ Eliminadas 3618 filas con 'DIA' vac√≠o\n",
      "‚úÖ Eliminadas 1 filas con 'HORA' vac√≠o\n",
      "üìä Limpieza de filas finalizada. Total eliminadas: 3619.\n",
      "--------------------------------------------------\n",
      "(662416, 414)\n",
      "(658797, 353)\n"
     ]
    }
   ],
   "source": [
    "obj = make_dataset(dataframe)\n",
    "\n",
    "columnas_eliminadas = obj.columnas_problematicas()\n",
    "filas_eliminadas = obj.filas_problematicas(['DIA', 'HORA'])\n",
    "\n",
    "print(dataframe.shape)\n",
    "print(obj.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de5d18",
   "metadata": {},
   "source": [
    "**Valores negativos**\n",
    "\n",
    "Filas con valores negativos: Hay muchas filas con valores negativos. Para ello nos ponemos a inspeccionar si alguna de estas es l√≥gica o probable que alberguen valores negativos y no se tratan de errores.\n",
    "    - Variables de temperatura en centigrados pueden ser negativas\n",
    "    - las variables totalizadoras NO pueden ser negativas\n",
    "    - columnas asociadas a tasas de volumen (/hl) no deberian ser negativas\n",
    "    - Columnas totalizadoras no tienen que ser negativas\n",
    "    - Columnas de potencia en Kw no deber√≠an ser negativas.\n",
    "    - Tot Vap Paset L3/hora y vapor de caldera deben ser positivos\n",
    "    - Retorno PLanta C=2. Es posible que sea negativo\n",
    "    - Red paste L3 y Agua lavadora L3. flujos probablemente positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7be26979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Tratando valores negativos ---------\n",
      "‚ÑπÔ∏è Excepciones (columnas que se ignorar√°n): ['Temp Tq Intermedio', 'Retorno Planta CO2']\n",
      "  - Columna 'EE Planta / Hl': 28 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Elaboracion / Hl': 93 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Bodega / Hl': 295 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Linea 2 / Hl': 190 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Servicios / Hl': 4 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Sala Maq / Hl': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Aire / Hl': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Agua / Hl': 193 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Resto Serv / Hl': 1720 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Resto Planta / Hl': 793 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Planta / Hl': 36 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Servicios/Hl': 655 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Planta de Agua/Hl': 656 valores negativos convertidos a NaN.\n",
      "  - Columna 'Produccion Agua / Hl': 38 valores negativos convertidos a NaN.\n",
      "  - Columna 'ET Linea 3/Hl': 58 valores negativos convertidos a NaN.\n",
      "  - Columna 'Aire Servicios / Hl': 778 valores negativos convertidos a NaN.\n",
      "  - Columna 'CO 2 / Hl': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'CO 2 Filtro / Hl': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'CO 2 linea 3 / Hl': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'CO 2 Linea 4 / Hl': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Paste L3 / Hl': 4 valores negativos convertidos a NaN.\n",
      "  - Columna 'Planta (Kw)': 31 valores negativos convertidos a NaN.\n",
      "  - Columna 'Elaboracion (Kw)': 177 valores negativos convertidos a NaN.\n",
      "  - Columna 'Bodega (Kw)': 376 valores negativos convertidos a NaN.\n",
      "  - Columna 'Linea 2 (Kw)': 370 valores negativos convertidos a NaN.\n",
      "  - Columna 'Servicios (Kw)': 5 valores negativos convertidos a NaN.\n",
      "  - Columna 'Sala Maq (Kw)': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Aire (Kw)': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Pta Agua / Eflu (Kw)': 64 valores negativos convertidos a NaN.\n",
      "  - Columna 'Prod Agua (Kw)': 195 valores negativos convertidos a NaN.\n",
      "  - Columna 'Resto Serv (Kw)': 1795 valores negativos convertidos a NaN.\n",
      "  - Columna 'Restos Planta (Kw)': 932 valores negativos convertidos a NaN.\n",
      "  - Columna 'KW Mycom 1': 1 valores negativos convertidos a NaN.\n",
      "  - Columna 'KW Planta de Agua': 15 valores negativos convertidos a NaN.\n",
      "  - Columna 'KW Filtr Carbon': 48 valores negativos convertidos a NaN.\n",
      "  - Columna 'KW Atlas 3': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'KW Toma Agua': 1 valores negativos convertidos a NaN.\n",
      "  - Columna 'KW Obrador Contratistas': 15 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Planta (Hl)': 38 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Servicios (Hl)': 694 valores negativos convertidos a NaN.\n",
      "  - Columna 'Planta de agua (Hl)': 31 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Paste L3': 836 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Lavadora L3': 1037 valores negativos convertidos a NaN.\n",
      "  - Columna 'Red Paste L3': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Tot Vap Paste L3 / Hora': 633 valores negativos convertidos a NaN.\n",
      "  - Columna 'VAPOR DE CALDERA 4 KG': 9 valores negativos convertidos a NaN.\n",
      "  - Columna 'Aire Servicios (M3)': 786 valores negativos convertidos a NaN.\n",
      "  - Columna 'Totalizador_L3': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Totaliador_Latas': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Totalizador_Bodega': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Totalizador_Tq_Pulmon': 8 valores negativos convertidos a NaN.\n",
      "  - Columna 'Totalizador_Filtracion': 2 valores negativos convertidos a NaN.\n",
      "üìä Tratamiento finalizado. 13666 celdas reemplazadas en 52 columnas.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columnas_excepcion = [\n",
    "    'Temp Tq Intermedio',\n",
    "    'Retorno Planta CO2'\n",
    "]\n",
    "total_reemplazos = obj.valores_negativos_por_columna(columnas_excepcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26572d",
   "metadata": {},
   "source": [
    "**Columnas duplicadas**\n",
    "Vamos a inspeccionar si hay algunas columnas repetidas. \n",
    "Las hipotesis que planteamos son las siguientes:\n",
    "- hl de mosto y hl de mosto copia. Si resultaron ser iguales\n",
    "- tot l3 la4 y planta de c02 vs tot l3. l4 y planta de co2: \n",
    "- planta(kw) y KW gral planta: No son iguales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2443203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "--- 1. Verificando Hip√≥tesis de Duplicados ---\n",
      "======================================================\n",
      "\n",
      "--- Verificando: 'Hl de Mosto' vs 'Hl de Mosto Copia' ---\n",
      "  ‚ö†Ô∏è Resultado: DUPLICADAS (Difieren solo en la posici√≥n de los NaNs).\n",
      "     (Son id√©nticas en todas las filas donde ambas tienen datos).\n",
      "     Contexto: 'Hl de Mosto' tiene 615899 NaNs. 'Hl de Mosto Copia' tiene 631472 NaNs.\n",
      "\n",
      "--- Verificando: 'Tot L3, L4 y Planta de CO2' vs 'Tot L3. L4 y Planta de CO2' ---\n",
      "  ‚ö†Ô∏è Resultado: No se pueden comparar (nunca tienen valores al mismo tiempo).\n",
      "\n",
      "--- Verificando: 'Planta (Kw)' vs 'KW Gral Planta' ---\n",
      "  ‚ùå Resultado: NO SON DUPLICADAS.\n",
      "     (Tienen 41151 valores diferentes donde ambas tienen datos).\n",
      "     Contexto: 'Planta (Kw)' tiene 615928 NaNs. 'KW Gral Planta' tiene 573001 NaNs.\n",
      "\n",
      "\n",
      "======================================================\n",
      "--- 2. Verificando Hip√≥tesis de Relevo de Datos ---\n",
      "======================================================\n",
      "\n",
      "--- Verificando relevo: 'Tot L3, L4 y Planta de CO2' vs 'Tot L3. L4 y Planta de CO2' ---\n",
      "  ‚úÖ VERDADERO: Las columnas tienen datos pero nunca se superponen.\n",
      "     (Esto confirma el patr√≥n de 'relevo' de datos).\n",
      "     (Patr√≥n limpio: 'Tot L3, L4 y Planta de CO2' termina en 168135, luego 'Tot L3. L4 y Planta de CO2' empieza en 380004)\n",
      "\n",
      "--- Unificando columnas con relevo... ---\n",
      "‚úÖ Columnas unificadas exitosamente en 'Tot_L3_L4_CO2_Unificado'.\n",
      "   Columnas originales eliminadas.\n",
      "\n",
      "======================================================\n",
      "--- Verificaci√≥n y unificaci√≥n completadas ---\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2. Lista de hip√≥tesis a probar\n",
    "hipotesis_duplicados = [\n",
    "    ('Hl de Mosto', 'Hl de Mosto Copia'),\n",
    "    ('Tot L3, L4 y Planta de CO2', 'Tot L3. L4 y Planta de CO2'),\n",
    "    ('Planta (Kw)', 'KW Gral Planta')\n",
    "]\n",
    "\n",
    "# --- Secci√≥n 1: Verificaci√≥n de Duplicados ---\n",
    "print(\"======================================================\")\n",
    "print(\"--- 1. Verificando Hip√≥tesis de Duplicados ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "for col1, col2 in hipotesis_duplicados:\n",
    "    # Llamamos al M√âTODO de la clase\n",
    "    obj.verificar_duplicado(col1, col2)\n",
    "\n",
    "# --- Secci√≥n 2: Verificaci√≥n de Relevo (Handoff) ---\n",
    "print(\"\\n\\n======================================================\")\n",
    "print(\"--- 2. Verificando Hip√≥tesis de Relevo de Datos ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "col_relevo_1 = 'Tot L3, L4 y Planta de CO2'\n",
    "col_relevo_2 = 'Tot L3. L4 y Planta de CO2'\n",
    "\n",
    "# Llamamos al M√âTODO de la clase y guardamos el resultado\n",
    "hubo_relevo = obj.verificar_relevo(\n",
    "    col_relevo_1, \n",
    "    col_relevo_2, \n",
    "    assume_sorted_index=True\n",
    ")\n",
    "\n",
    "# --- Secci√≥n 3: Unificaci√≥n basada en el resultado de Relevo ---\n",
    "if hubo_relevo:\n",
    "    print(\"\\n--- Unificando columnas con relevo... ---\")\n",
    "    \n",
    "    col_unificada = 'Tot_L3_L4_CO2_Unificado' # Nombre nuevo y limpio\n",
    "\n",
    "    # 2. Verificar que existan antes de unificar (aunque 'verificar_relevo' ya lo hizo)\n",
    "    if col_relevo_1 in obj.df.columns and col_relevo_2 in obj.df.columns:\n",
    "        \n",
    "        # 3. Crear la columna unificada\n",
    "        # Rellena los NaN de la columna 1 con los valores de la columna 2\n",
    "        obj.df[col_unificada] = obj.df[col_relevo_1].fillna(obj.df[col_relevo_2])\n",
    "\n",
    "        # 4. Eliminar las columnas originales\n",
    "        obj.df.drop(columns=[col_relevo_1, col_relevo_2], inplace=True)\n",
    "        \n",
    "        print(f\"‚úÖ Columnas unificadas exitosamente en '{col_unificada}'.\")\n",
    "        print(f\"   Columnas originales eliminadas.\")\n",
    "        \n",
    "    else:\n",
    "        # Esto no deber√≠a pasar si hubo_relevo es True, pero es una buena pr√°ctica\n",
    "        print(f\"‚ùå Error: No se encontraron las columnas '{col_relevo_1}' o '{col_relevo_2}' para unificar.\")\n",
    "else:\n",
    "     print(f\"\\n--- No se detect√≥ relevo para '{col_relevo_1}', no se unificar√°n. ---\")\n",
    "\n",
    "\n",
    "print(\"\\n======================================================\")\n",
    "print(\"--- Verificaci√≥n y unificaci√≥n completadas ---\")\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aaebeb",
   "metadata": {},
   "source": [
    "**columnas compuestas**\n",
    "Hay columnas que puedens ser la suma de dos columnas: \n",
    "A su vez analizaremos si hay columnas que son suma de columnas. Las hipotesis:\n",
    "- M3_tot_gas vs TOT GAS\n",
    "- KW linea 3 y 4 quisas sean la suma de KW linea 3 + kw linea4\n",
    "- Hl mosto = hl mosto + hl budweiser + hlmoste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45853a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================\n",
      "--- 3. Verificando Hip√≥tesis de Sumas Jer√°rquicas ---\n",
      "======================================================\n",
      "--- Iniciando verificaci√≥n de 3 sumas jer√°rquicas ---\n",
      "\n",
      "--- Verificando: 'KW Linea 3 y 4' vs Suma de 2 partes ---\n",
      "  ‚ùå Resultado: NO SON REDUNDANTES.\n",
      "     ('KW Linea 3 y 4' NO es la suma simple de sus partes).\n",
      "     Diferencia promedio: 3057.1161\n",
      "     Diferencia m√°xima:   15621.0000\n",
      "\n",
      "--- Verificando: 'Planta (Kw)' vs Suma de 6 partes ---\n",
      "  ‚ùå Resultado: NO SON REDUNDANTES.\n",
      "     ('Planta (Kw)' NO es la suma simple de sus partes).\n",
      "     Diferencia promedio: 21403804865883.3516\n",
      "     Diferencia m√°xima:   32771277123286440.0000\n",
      "\n",
      "--- Verificando: 'Servicios (Kw)' vs Suma de 8 partes ---\n",
      "  ‚ùå Resultado: NO SON REDUNDANTES.\n",
      "     ('Servicios (Kw)' NO es la suma simple de sus partes).\n",
      "     Diferencia promedio: 3055953131700.0864\n",
      "     Diferencia m√°xima:   65542554247149816.0000\n",
      "\n",
      "======================================================\n",
      "--- Verificaci√≥n completada ---\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# --- EJECUCI√ìN DEL SCRIPT DE VERIFICACI√ìN ---\n",
    "# ======================================================\n",
    "\n",
    "# --- Secci√≥n 3: Verificaci√≥n de Sumas Jer√°rquicas (NUEVO) ---\n",
    "print(\"\\n\\n======================================================\")\n",
    "print(\"--- 3. Verificando Hip√≥tesis de Sumas Jer√°rquicas ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "# Definimos el diccionario de hip√≥tesis\n",
    "hipotesis_sumas = {\n",
    "    \n",
    "    'KW Linea 3 y 4': [\n",
    "        'KW Linea 3', \n",
    "        'KW Linea 4'\n",
    "    ],\n",
    "    \n",
    "    'Planta (Kw)': [\n",
    "        'Elaboracion (Kw)', \n",
    "        'Bodega (Kw)', \n",
    "        'Cocina (Kw)', \n",
    "        'Envasado (Kw)', \n",
    "        'Servicios (Kw)', \n",
    "        'Restos Planta (Kw)'\n",
    "    ],\n",
    "    \n",
    "    'Servicios (Kw)': [\n",
    "         'Sala Maq (Kw)', \n",
    "         'Aire (Kw)', \n",
    "         'Calderas (Kw)', \n",
    "         'Efluentes (Kw)', \n",
    "         'Frio (Kw)', \n",
    "         'Pta Agua / Eflu (Kw)', \n",
    "         'Prod Agua (Kw)', \n",
    "         'Resto Serv (Kw)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Llamamos al nuevo m√©todo de la clase\n",
    "obj.verificar_suma_jerarquica(hipotesis_sumas)\n",
    "\n",
    "print(\"\\n======================================================\")\n",
    "print(\"--- Verificaci√≥n completada ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "#como no verificaron ser sumas, damos por hecho que no son redundantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c13cd",
   "metadata": {},
   "source": [
    "**Columnas categoricas**\n",
    "Vamos a inspeccionar las columnas categoricas. \n",
    "- Buscamos las columnas que son del tipo 'Objetct' o 'Category'\n",
    "- Que solamente tienen 1 y 0 de valores posibles. Esperamos encontrar aca la categoria de cerveza.\n",
    "\n",
    "dentro de las columnas objetos:\n",
    "- muchas son numericas, las cambiamos\n",
    "\n",
    "Dentro de las columnas binarias:\n",
    "- tenemos dummies como mosto, combustible, vapor\n",
    "- banderas de estados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48d327b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando b√∫squeda eficiente de columnas binarias...\n",
      "B√∫squeda de binarias completada.\n",
      "\n",
      "Columnas categ√≥ricas o binarias (0/1):\n",
      " Categorias texto: ['DIA', 'HORA', 'Nivel Silo Bagazo Norte (1)', 'KW Trafo 8', 'Totalizador Bba P1', 'Totalizador Bba P2', 'Totalizador Bba P4', 'Totalizador Bba Envasado', 'Totalizador Bba P51']\n",
      " Binarias num√©ricas: ['HL Mosto Budweiser', 'HL Mosto Local', 'HL Mosto Fuerte', 'HL Mosto Indio', 'HL Mosto Palermo', 'HL Mosto Bieckert', 'HL Mosto Malta', 'HL Mosto Frost', 'Hl Session IPA', 'Hl Reserva 8', 'KW Linea 4', 'Kw llum/Serv L2', 'Red Barriles', 'Agua Filt FMaCist CE', 'Rep Agua Cist CE', 'Tot_Vapor_Caldera 3', 'VAPOR DE CALDERA 1 KG', 'Fuel Oil Tk1 (Kg)', 'Fuel Oil Tk2 (Kg)', 'Hl Reserva 7']\n",
      "\n",
      "\n",
      "======================================================\n",
      "--- 4. Verificando Hip√≥tesis de Exclusividad Mutua (Dummies) ---\n",
      "======================================================\n",
      "--- Iniciando verificaci√≥n de 4 grupos mutuamente excluyentes ---\n",
      "\n",
      "--- Verificando Grupo: 'Grupo Mostos' (10 columnas) ---\n",
      "  ‚úÖ Resultado: SON MUTUAMENTE EXCLUYENTES.\n",
      "     (Nunca hay m√°s de una columna 'activa' (1) al mismo tiempo).\n",
      "\n",
      "--- Verificando Grupo: 'Grupo Fuel Oil Tanks' (2 columnas) ---\n",
      "  ‚úÖ Resultado: SON MUTUAMENTE EXCLUYENTES.\n",
      "     (Nunca hay m√°s de una columna 'activa' (1) al mismo tiempo).\n",
      "\n",
      "--- Verificando Grupo: 'Grupo Calderas (Vapor)' (2 columnas) ---\n",
      "  ‚úÖ Resultado: SON MUTUAMENTE EXCLUYENTES.\n",
      "     (Nunca hay m√°s de una columna 'activa' (1) al mismo tiempo).\n",
      "\n",
      "--- Verificando Grupo: 'Grupo Reservas' (2 columnas) ---\n",
      "  ‚úÖ Resultado: SON MUTUAMENTE EXCLUYENTES.\n",
      "     (Nunca hay m√°s de una columna 'activa' (1) al mismo tiempo).\n",
      "\n",
      "======================================================\n",
      "--- Verificaci√≥n completada ---\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Columnas categ√≥ricas (texto, objeto) ---\n",
    "cols_categoricas_texto = obj.df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# --- 2. Columnas binarias (num√©ricas 0/1) ---\n",
    "print(\"Iniciando b√∫squeda eficiente de columnas binarias...\")\n",
    "cols_binarias_numericas = []\n",
    "\n",
    "# Bucle eficiente (LA SOLUCI√ìN):\n",
    "# Iteramos sobre todos los nombres de columnas (una lista ligera)\n",
    "for col in obj.df.columns:\n",
    "    \n",
    "    # Verificamos el tipo de la columna SIN copiarla\n",
    "    if pd.api.types.is_numeric_dtype(obj.df[col]):\n",
    "        \n",
    "        # Si es num√©rica, aplicamos tu l√≥gica original\n",
    "        # .unique() es r√°pido en columnas individuales\n",
    "        try:\n",
    "            valores_unicos = obj.df[col].dropna().unique()\n",
    "            \n",
    "            if set(valores_unicos).issubset({0, 1}):\n",
    "                cols_binarias_numericas.append(col)\n",
    "        except Exception as e:\n",
    "            print(f\"  - Advertencia: No se pudo procesar la columna '{col}'. Error: {e}\")\n",
    "\n",
    "print(\"B√∫squeda de binarias completada.\")\n",
    "\n",
    "# --- 3. Combinar ambas listas ---\n",
    "columnas_finales = list(set(cols_categoricas_texto + cols_binarias_numericas))\n",
    "\n",
    "print(\"\\nColumnas categ√≥ricas o binarias (0/1):\")\n",
    "print(f\" Categorias texto: {cols_categoricas_texto}\")\n",
    "print(f\" Binarias num√©ricas: {cols_binarias_numericas}\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# --- EJECUCI√ìN DEL SCRIPT DE VERIFICACI√ìN ---\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n\\n======================================================\")\n",
    "print(\"--- 4. Verificando Hip√≥tesis de Exclusividad Mutua (Dummies) ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "# Definimos el diccionario de hip√≥tesis\n",
    "hipotesis_dummies = {\n",
    "    \n",
    "    'Grupo Mostos': [\n",
    "        'HL Mosto Budweiser', 'HL Mosto Local', 'HL Mosto Fuerte', \n",
    "        'HL Mosto Indio', 'HL Mosto Palermo', 'HL Mosto Bieckert', \n",
    "        'HL Mosto Malta', 'HL Mosto Frost', 'Hl Session IPA', 'Hl Reserva 8'\n",
    "    ],\n",
    "    \n",
    "    'Grupo Fuel Oil Tanks': [\n",
    "        'Fuel Oil Tk1 (Kg)', \n",
    "        'Fuel Oil Tk2 (Kg)'\n",
    "    ],\n",
    "    \n",
    "    'Grupo Calderas (Vapor)': [\n",
    "         'Tot_Vapor_Caldera 3',\n",
    "         'VAPOR DE CALDERA 1 KG'\n",
    "    ], \n",
    "    'Grupo Reservas': [\n",
    "        'Hl Reserva 7', \n",
    "        'Hl Reserva 8'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Llamamos al nuevo m√©todo de la clase\n",
    "# (Aseg√∫rate de que las columnas en hipotesis_dummies existan \n",
    "# y se hayan identificado correctamente como binarias)\n",
    "obj.verificar_exclusividad_mutua(hipotesis_dummies)\n",
    "\n",
    "print(\"\\n======================================================\")\n",
    "print(\"--- Verificaci√≥n completada ---\")\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02c262d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Revisando valores √∫nicos de columnas 'texto' sospechosas ---\n",
      "\n",
      "Valores en 'Nivel Silo Bagazo Norte (1)':\n",
      "[nan 0.0 1.91 1.77 1.62 1.49 1.36 1.22 1.09 0.95 0.81 0.68 0.55 0.39 0.27\n",
      " 0.12 0.01 0.36 7.34 12.95]\n",
      "\n",
      "Valores en 'KW Trafo 8':\n",
      "[nan 0.0 26.0 51.0 76.0 100.0 123.0 147.0 170.0 193.0 214.0 235.0 237.0\n",
      " 249.0 412.0 153.0 301.25 447.5 610.0 754.5]\n",
      "\n",
      "Valores en 'Totalizador Bba P1':\n",
      "[nan 0.0 21.0 43.0 64.0 85.0 107.0 128.0 150.0 172.0 193.0 214.0 236.0\n",
      " 258.0 281.0 307.0 356.0 377.0 399.0 422.0]\n",
      "\n",
      "Valores en 'Totalizador Bba P2':\n",
      "[nan 0.0 28.0 56.0 83.0 108.0 132.0 157.0 182.0 209.0 234.0 260.0 282.0\n",
      " 304.0 326.0 347.0 371.0 392.0 411.0 428.0]\n",
      "\n",
      "Valores en 'Totalizador Bba P4':\n",
      "[nan 0.0 56.0 111.0 166.0 222.0 277.0 333.0 387.0 439.0 492.0 543.0 593.0\n",
      " 639.0 685.0 717.0 747.0 777.0 805.0 833.0]\n",
      "\n",
      "Valores en 'Totalizador Bba Envasado':\n",
      "[nan 0.0 22.0 44.0 66.0 88.0 111.0 134.0 156.0 179.0 201.0 224.0 247.0\n",
      " 269.0 293.0 324.0 374.0 396.0 419.0 441.0]\n",
      "\n",
      "Valores en 'Totalizador Bba P51':\n",
      "[nan 0.0 56.0 112.0 169.0 225.0 281.0 337.0 394.0 450.0 506.0 562.0 619.0\n",
      " 675.0 731.0 787.0 844.0 900.0 956.0 1012.0]\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Forzando conversi√≥n a num√©rico en 7 columnas ---\n",
      "  ‚úÖ Columna 'Nivel Silo Bagazo Norte (1)': Convertida (object -> float64)\n",
      "  ‚úÖ Columna 'KW Trafo 8': Convertida (object -> float64)\n",
      "  ‚úÖ Columna 'Totalizador Bba P1': Convertida (object -> float64)\n",
      "  ‚úÖ Columna 'Totalizador Bba P2': Convertida (object -> float64)\n",
      "  ‚úÖ Columna 'Totalizador Bba P4': Convertida (object -> float64)\n",
      "  ‚úÖ Columna 'Totalizador Bba Envasado': Convertida (object -> float64)\n",
      "  ‚úÖ Columna 'Totalizador Bba P51': Convertida (object -> float64)\n",
      "\n",
      "Conversi√≥n completada. Se convirtieron 7 columnas.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Script de Diagn√≥stico (el que ya ten√≠as) ---\n",
    "columnas_sospechosas = [\n",
    "    'Nivel Silo Bagazo Norte (1)', \n",
    "    'KW Trafo 8', \n",
    "    'Totalizador Bba P1', \n",
    "    'Totalizador Bba P2', \n",
    "    'Totalizador Bba P4', \n",
    "    'Totalizador Bba Envasado', \n",
    "    'Totalizador Bba P51'\n",
    "]\n",
    "\n",
    "print(\"--- Revisando valores √∫nicos de columnas 'texto' sospechosas ---\")\n",
    "\n",
    "for col in columnas_sospechosas:\n",
    "    if col in obj.df.columns:\n",
    "        # Mostramos los primeros 20 valores √∫nicos\n",
    "        print(f\"\\nValores en '{col}':\")\n",
    "        # Asegurarnos de manejar el error si hay menos de 20 √∫nicos\n",
    "        try:\n",
    "            print(obj.df[col].unique()[:20])\n",
    "        except IndexError:\n",
    "            print(obj.df[col].unique())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 2. Llamada al nuevo m√©todo (la \"instanciaci√≥n\") ---\n",
    "# Ahora que viste los valores, llamas al m√©todo de tu objeto 'obj'\n",
    "# para que realice la conversi√≥n en su dataframe interno (obj.df)\n",
    "\n",
    "obj.forzar_conversion_numerica(columnas_sospechosas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a78d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIA', 'HORA', 'EE Planta / Hl', 'EE Elaboracion / Hl', 'EE Bodega / Hl', 'EE Cocina / Hl', 'EE Envasado / Hl', 'EE Linea 2 / Hl', 'EE Linea 3 / Hl', 'EE Linea 4 / Hl', 'EE Servicios / Hl', 'EE Sala Maq / Hl', 'EE Frio / Hl', 'EE Aire / Hl', 'EE CO2 / Hl', 'EE Caldera / Hl', 'EE Eflu / Hl', 'EE Agua / Hl', 'EE Resto Serv / Hl', 'EE Resto Planta / Hl', 'Agua Planta / Hl', 'Agua Elab / Hl', 'Agua Bodega / Hl', 'Agua Cocina / Hl', 'Agua Envas / Hl', 'Agua Linea 2/Hl', 'Agua Linea 3/Hl', 'Agua Linea 4/Hl', 'Agua Linea 5/Hl', 'Agua Servicios/Hl', 'Agua Planta de Agua/Hl', 'Produccion Agua / Hl', 'ET Planta / Hl', 'ET Elab/Hl', 'ET Bodega/Hl', 'ET Cocina/Hl', 'ET Envasado/Hl', 'ET Linea 2/Hl', 'ET Linea 3/Hl', 'ET Linea 4/Hl', 'ET Linea 5/Hl', 'ET Servicios / Hl', 'Aire Planta / Hl', 'Aire Elaboracion / Hl', 'Aire Cocina / Hl', 'Aire Bodega / Hl', 'Aire Envasado / Hl', 'Aire L2 / Hl', 'Aire L3 / Hl', 'Aire L4 / Hl', 'Aire L5 / Hl', 'Aire Servicios / Hl', 'Aire Expulsion / Hl', 'CO 2 / Hl', 'CO 2 Filtro / Hl', 'CO 2 linea 3 / Hl', 'CO 2 Linea 4 / Hl', 'CO 2 linea 2 / Hl', 'Agua Lavadora L3 / Hl', 'Agua Paste L3 / Hl', 'Meta EE Planta', 'Meta Agua Planta', 'Meta ET Planta', 'Meta Aire Planta', 'Meta Agua Elab', 'Meta Agua Bodega', 'Meta Agua Cocina', 'Meta Agua Envas', 'Meta Agua Linea 2', 'Meta Agua Linea 3', 'Meta Agua Linea 4', 'Meta Agua Linea 5', 'Meta Agua Servicios', 'Meta Agua Planta de Agua', 'Meta Produccion Agua', 'Meta EE Elaboracion', 'Meta EE Bodega', 'Meta EE Cocina', 'Meta EE Envasado', 'Meta EE Linea 2', 'Meta EE Linea 3', 'Meta EE Linea 4', 'Meta EE Linea 5', 'Meta EE Servicios', 'Meta EE Sala Maq', 'Meta EE Frio', 'Meta EE Aire', 'Meta EE CO2', 'Meta EE Caldera', 'Meta EE Eflu', 'Meta EE Agua ', 'Meta EE Resto Serv', 'Meta EE Resto Planta', 'Meta ET Elab', 'Meta ET Bodega', 'Meta ET Cocina', 'Meta ET Envasado', 'Meta ET Linea 2', 'Meta ET Linea 3', 'Meta ET Linea 4', 'Meta ET Linea 5', 'Meta ET Servicios', 'Meta Aire Elaboracion', 'Meta Aire Cocina', 'Meta Aire Bodega', 'Meta Aire Envasado', 'Meta Aire L2', 'Meta Aire L3', 'Meta Aire L4', 'Meta Aire L5', 'Meta Aire Servicios', 'Hl de Mosto', 'Hl Cerveza Cocina', 'Hl Producido Bodega', 'Hl Cerveza Filtrada', 'Hl Cerveza Envasada', 'Hl Cerveza L2', 'Hl Cerveza L3', 'Hl Cerveza L4', 'Hl Cerveza L5', 'Cocimientos Diarios', 'Hl de Mosto Copia', 'HL Mosto Budweiser', 'HL Mosto Tecate', 'HL Mosto Local', 'HL Mosto Heineken', 'HL Mosto Negra', 'HL Mosto Fuerte', 'HL Mosto Indio', 'HL Mosto Palermo', 'HL Mosto Bieckert', 'HL Mosto Malta', 'HL Mosto Amstel', 'HL Mosto Sol', 'HL Mosto Frost', 'HL Mosto Miller', 'HL Mosto IPA', 'HL Mosto APA', 'HL Mosto Roja', 'HL Mosto Isenbeck', ' HL Mosto Golden', 'HL Mosto Amstel Lager', 'Hl Mosto Cautiva Roja', 'Hl Mosto Cautiva Torrontes', 'Hl Mosto Cautiva Blend', 'Hl Mosto Bieckert Urbana', 'Hl Mosto Bieckert BAPA', 'Hl Mosto Blue Moon', 'Hl Session IPA', 'Hl Reserva 8', 'HL Cerveza Sin Diliuir A90', 'HL Cerceza Sin Diluir A190', 'Nivel Silo Bagazo Norte (1)', 'Nivel Silo Bagazo Sur (2)', 'Nivel Tk Restos Lev', 'Id', 'Planta (Kw)', 'Elaboracion (Kw)', 'Bodega (Kw)', 'Cocina (Kw)', 'Envasado (Kw)', 'Linea 2 (Kw)', 'Linea 3 (Kw)', 'Linea 4 (Kw)', 'Servicios (Kw)', 'Sala Maq (Kw)', 'Aire (Kw)', 'Calderas (Kw)', 'Efluentes (Kw)', 'Frio (Kw)', 'Pta Agua / Eflu (Kw)', 'Prod Agua (Kw)', 'Resto Serv (Kw)', 'Restos Planta (Kw)', 'KW Gral Planta', 'KW Trafo 4', 'KW Trafo 5', 'KW Trafo 8', 'KW Trafo 11', 'KW Trafo 12', 'KW Linea 2', 'KW Mycom 1', 'KW Mycom 2', 'KW Mycom 3', 'KW Mycom 4', 'KW Mycom 5', 'KW Mycom 6', 'KW Mycom 7', 'KW Trafo 9', 'KW Trafo 10', 'KW Cocina', 'Kw Molino', 'KW Linea 3 y 4', 'KW Linea 3', 'KW Linea 4', 'KW Servicio L2', 'KW Planta de Agua', 'Kw llum/Serv L2', 'Kw Casona', 'KW Laboratorio', 'Kw Admininistracion', 'KW Pta Agua/Log', 'KW Bba Glicol Sala MAq', 'KW Comp Kaeser', 'KW Iluminacion L3', 'KW Iluminacion L4', 'KW Ilum Dep L3/L4', 'KW Cond 5. 6 y 9', 'KW Cond 7. 8 y 11', 'KW Cond 11. 12 y 13', 'KW Bba Glicol Bod', 'KW CO2', 'KW Secador Kaeser', 'KW Caldera 3', 'KW Caldera 4', 'KW Filtr Carbon', 'KW Atlas 3', 'KW Toma Agua', 'KW Enfr Agua Cocina', 'KW Enfluentes Coc', 'KW Enfluente Efl', 'KW Enfluentes Hidr', 'KW Obrador Contratistas', 'Kw Compresores Aire', 'Kw Linea Barriles', 'Agua Planta (Hl)', 'Agua Elaboracion (Hl)', 'Agua Bodega (Hl)', 'Agua Cocina (Hl)', 'Agua Dilucion (Hl)', 'Agua Envasado (Hl)', 'Agua Servicios (Hl)', 'Planta de agua (Hl)', 'Produccion (Hl)', 'Red L1 y L2', 'FC L1 y L2', 'FC Lavadora L2', 'Red L3', 'FC Lavadora L3', 'Red Paste L4', 'FC Barriles', 'Agua Paste L3', 'Agua Lavadora L3', 'Agua Cond Evaporativos', 'Agua Calderas', 'Agua Efluentes', 'Agua CO2', 'Red Paste L1', 'FC Linea 3', 'Rinser', 'Red Barriles', 'Red Bodega', 'FC CIP Bodega', 'FC Centec I', 'FC Cocina', 'FC Centec II', 'Agua Cond REC', 'Agua Red Servicios', 'Agua Cald', 'Agua Cist Industrial', 'Agua Filt FMaCist CE', 'Rep Agua Cist CE', 'Rep Agua Puerto', 'Agua Helada Cocina', 'Entrada Osmosis', 'Rechazo Osmosis', 'Red Administracion', 'Filtro de Carbon', 'Assa', 'Red Paste L3', 'Glicol Paste L3', 'Red Lavadora L3', 'Diluci√≥n A90', 'Diluci√≥n A190', 'Red  Bodega Interno', 'Agua Planta CO2', 'Retorno Planta CO2', 'Agua Limpieza y Bbba Vacio L2', 'Agua Enjuagadora L2', 'Agua Lavadora L2', 'Temp Tq Intermedio', 'Conversion Kg/Mj', 'Gas Planta (Mj)', 'Vapor Elaboracion (Kg)', 'Vapor Cocina (Kg)', 'Vapor Envasado (Kg)', 'Vapor Servicio (Kg)', 'ET Elaboracion (Mj)', 'ET Envasado (Mj)', 'ET Servicios (Mj)', 'Tot_Vapor_L3_L4', 'VAPOR DE LINEA 1 Y 2 KG', 'VAPOR DE LINEA 4 KG', 'Vapor_L5 (KG)', 'Tot_Vapor_CIP_Bodega', 'Vapor L3', 'Tot Vap Paste L3 / Hora', 'Tot Vap Lav L3 / Hora', 'Medicion Gas Planta (M3)', 'M3_Tot_Gas', 'Tot_Vapor_Caldera 3', 'CAUDAL DE GAS CALDERA 1 M3', 'CAUDAL DE BIO-GAS M3', 'VAPOR DE CALDERA 1 KG', 'VAPOR DE CALDERA 3 KG', 'VAPOR DE CALDERA 4 KG', 'VAPOR COCINA 1 KG', 'VAPOR COCINA 2 KG', 'TOT GAS ENTRADA PRINCIPAL M3', 'TOT AGUA DESAIREADA', 'Vapor Olla 1 (Ton)', '(Vapor Olla 2 (Ton)', 'Fuel Oil Tk1 (Kg)', 'Fuel Oil Tk2 (Kg)', 'Vapor Paste L3', 'Aire Producido (M3)', 'Aire Planta (M3)', 'Aire Elaboracion (m3)', 'Aire Envasado (M3)', 'Aire Servicios (M3)', 'Tot Aire Expulsion', 'Totalizador_Aire_Bodega', 'Totalizador_Aire_Cocina', 'Totalizador_Aire_L2', 'Totalizador_Aire_L3', 'Totaliador_Aire_L4', 'Totalizador_Aire_L5', 'Totalizador Sistema Kaeser', 'Tot Aire Servicio Logistica', 'Totalizador_L2_Barriles', 'Totalizador_L3', 'Totaliador_Latas', 'Totalizador_Bodega', 'Totalizador_Tq_Pulmon', 'Totalizador_Filtracion', 'Totalizador Bba P1', 'Totalizador Bba P2', 'Totalizador Bba P4', 'Totalizador Bba Envasado', 'Totalizador Bba P51', 'Totalizador Bba Gas', 'Tot A40/240/50/60/Centec/Filtro', 'Tot  A130/330/430', 'Tot  Trasiego', 'Tot A10/20', 'Hl Linea 2', 'Hl Reserva 7', 'Vapor _Vapor_L5 (KG)', 'Tot Fermantacion_Cocina', 'Tot Reposo Inferior', 'Tot  Reposo Superior', 'Tot_L3_L4_CO2_Unificado']\n",
      "(658797, 352)\n"
     ]
    }
   ],
   "source": [
    "print(list(obj.df.columns))\n",
    "print(obj.df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd8134",
   "metadata": {},
   "source": [
    "# Feature enginering de preprocesamiento\n",
    "Agregar nuevas variables que consideremos necesarias:\n",
    "- temperaturas diarias en la zona\n",
    "- variables temporales (estacion, dia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a6516b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openmeteo-requests in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: niquests>=3.15.2 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from openmeteo-requests) (3.15.2)\n",
      "Requirement already satisfied: openmeteo-sdk>=1.22.0 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from openmeteo-requests) (1.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from niquests>=3.15.2->openmeteo-requests) (3.4.4)\n",
      "Requirement already satisfied: urllib3-future<3,>=2.13.903 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from niquests>=3.15.2->openmeteo-requests) (2.14.905)\n",
      "Requirement already satisfied: wassima<3,>=1.0.1 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from niquests>=3.15.2->openmeteo-requests) (2.0.2)\n",
      "Requirement already satisfied: flatbuffers==25.9.23 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from openmeteo-sdk>=1.22.0->openmeteo-requests) (25.9.23)\n",
      "Requirement already satisfied: h11<1.0.0,>=0.11.0 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (0.16.0)\n",
      "Requirement already satisfied: jh2<6.0.0,>=5.0.3 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (5.0.10)\n",
      "Requirement already satisfied: qh3<2.0.0,>=1.5.4 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (1.5.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests-cache in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: retry-requests in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: attrs>=21.2 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests-cache) (25.4.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests-cache) (25.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests-cache) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.22 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests-cache) (2.32.5)\n",
      "Requirement already satisfied: url-normalize>=1.4 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests-cache) (2.2.1)\n",
      "Requirement already satisfied: urllib3>=1.25.5 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests-cache) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from cattrs>=22.2->requests-cache) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests>=2.22->requests-cache) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests>=2.22->requests-cache) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mateocrack\\desktop\\tp_final\\mostoelmostro\\.venv\\lib\\site-packages (from requests>=2.22->requests-cache) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openmeteo-requests\n",
    "%pip install requests-cache retry-requests numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64bfadbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>13.594001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:00:00+00:00</td>\n",
       "      <td>10.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:00:00+00:00</td>\n",
       "      <td>9.344001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:00:00+00:00</td>\n",
       "      <td>7.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:00:00+00:00</td>\n",
       "      <td>7.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023-12-31 19:00:00+00:00</td>\n",
       "      <td>12.594001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023-12-31 20:00:00+00:00</td>\n",
       "      <td>13.344001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023-12-31 21:00:00+00:00</td>\n",
       "      <td>13.344001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023-12-31 22:00:00+00:00</td>\n",
       "      <td>13.294001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2023-12-31 23:00:00+00:00</td>\n",
       "      <td>13.494000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  temperature_2m\n",
       "0     2020-01-01 00:00:00+00:00       13.594001\n",
       "1     2020-01-01 01:00:00+00:00       10.694000\n",
       "2     2020-01-01 02:00:00+00:00        9.344001\n",
       "3     2020-01-01 03:00:00+00:00        7.894000\n",
       "4     2020-01-01 04:00:00+00:00        7.694000\n",
       "...                         ...             ...\n",
       "35059 2023-12-31 19:00:00+00:00       12.594001\n",
       "35060 2023-12-31 20:00:00+00:00       13.344001\n",
       "35061 2023-12-31 21:00:00+00:00       13.344001\n",
       "35062 2023-12-31 22:00:00+00:00       13.294001\n",
       "35063 2023-12-31 23:00:00+00:00       13.494000\n",
       "\n",
       "[35064 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# El cambio clave est√° en esta URL\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "    \"latitude\": 32.5672,\n",
    "    \"longitude\": -116.6251,\n",
    "    \"start_date\": \"2020-01-01\",\n",
    "    \"end_date\": \"2023-12-31\",\n",
    "    \"hourly\": \"temperature_2m\"\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "    start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "    end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "    freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "    inclusive=\"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "\n",
    "\n",
    "hourly_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0951962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columna 'ESTACION' agregada correctamente.\n"
     ]
    }
   ],
   "source": [
    "obj.redondear_horarios()\n",
    "obj.add_estacion()\n",
    "obj.add_temp_y_dia(hourly_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2887471",
   "metadata": {},
   "source": [
    "# Construyendo el dataset final\n",
    "Nuestro datasets tiene datos temporales pero por tupla (fecha hora) hay multiples observaciones. Queremos promediar estas para ver si son series temporales pero queremos verificar que las variables categoricas no sean distintas entre los instantes (fecha hora). Entonces Vamos a verificar esto mismo con la salvedad de que la diferencia sean entre nan y otro valor.\n",
    "\n",
    "La unica columna que genera una diferencia es Ultimo Dato del Dia. La obviamos y juntamos los datos usando las medias de las observaciones (no promedios para no ser sensibles a atipicos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cd07964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verificando consistencia de categor√≠as binarias antes de agrupar ---\n",
      "--- Iniciando verificaci√≥n de consistencia en tuplas (DIA, HORA) duplicadas ---\n",
      "\n",
      "Revisados 29701 grupos ('DIA', 'HORA') √∫nicos.\n",
      "Se encontraron 29700 grupos con filas duplicadas.\n",
      "‚úÖ ¬°√âxito! Todas las tuplas (DIA, HORA) duplicadas son consistentes en las columnas binarias.\n",
      "   (Se puede proceder con la agrupaci√≥n de forma segura)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# --- EJECUCI√ìN DEL SCRIPT DE VERIFICACI√ìN DE CONSISTENCIA ---\n",
    "# ======================================================\n",
    "\n",
    "# Asumimos que 'obj' es una instancia de tu clase 'make_dataset'\n",
    "# y que 'cols_binarias_numericas' es la lista de columnas que ya definiste.\n",
    "\n",
    "print(\"\\n--- Verificando consistencia de categor√≠as binarias antes de agrupar ---\")\n",
    "\n",
    "# 3. Ejecutar el m√©todo\n",
    "# (Llamamos al m√©todo desde el objeto 'obj')\n",
    "hay_problemas, grupos_problematicos = obj.verificar_consistencia_binaria(cols_binarias_numericas)\n",
    "\n",
    "# 4. Mostrar los resultados si los hay\n",
    "if hay_problemas:\n",
    "    print(\"\\n--- Detalle de Inconsistencias ---\")\n",
    "    \n",
    "    # Mostramos los primeros 5 grupos problem√°ticos como ejemplo\n",
    "    count = 0\n",
    "    for (dia, hora), columnas in grupos_problematicos.items():\n",
    "        if count < 5:\n",
    "            print(f\"En ({dia}, {hora}), las siguientes columnas var√≠an: {columnas}\")\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        print(f\"... y {len(grupos_problematicos) - 5} m√°s.\")\n",
    "else:\n",
    "    print(\"   (Se puede proceder con la agrupaci√≥n de forma segura)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c70eca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando agrupaci√≥n por (DIA) usando la media (Promedio Diario) ---\n",
      "Agrupando 351 columnas por (DIA)...\n",
      "\n",
      "‚úÖ Agrupaci√≥n Diaria completada.\n",
      "   Filas reducidas de 658797 a 1190.\n",
      "\n",
      "Dimensiones finales del obj.df: (1190, 352)\n",
      "         DIA  EE Planta / Hl  EE Elaboracion / Hl  EE Bodega / Hl  \\\n",
      "0 2020-07-01      198.550355            14.944947       21.898397   \n",
      "1 2020-07-02        7.332630             0.648068        0.725053   \n",
      "2 2020-07-03        8.198708             0.756267        0.776495   \n",
      "3 2020-07-04        4.550079             0.417806        0.355416   \n",
      "4 2020-07-05        5.627349             0.491721        0.441206   \n",
      "\n",
      "   EE Cocina / Hl  EE Envasado / Hl  EE Linea 2 / Hl  EE Linea 3 / Hl  \\\n",
      "0        0.000000          1.905891         0.583151         0.000000   \n",
      "1        0.522019          2.414053         2.805374         1.328570   \n",
      "2        0.225265          1.869990         3.627103         1.706932   \n",
      "3        0.243756          1.305393         3.400092         1.079618   \n",
      "4        0.244048          1.386874         4.631722         1.776986   \n",
      "\n",
      "   EE Linea 4 / Hl  EE Servicios / Hl  ...  Tot  Trasiego  Tot A10/20  \\\n",
      "0         0.000000         173.274186  ...            NaN         NaN   \n",
      "1         1.279133           5.041896  ...            NaN         NaN   \n",
      "2         1.239513           5.344107  ...            NaN         NaN   \n",
      "3         1.234136           2.621910  ...            NaN         NaN   \n",
      "4         1.225286           3.471051  ...            NaN         NaN   \n",
      "\n",
      "   Hl Linea 2  Hl Reserva 7  Vapor _Vapor_L5 (KG)  Tot Fermantacion_Cocina  \\\n",
      "0         NaN           0.0              549.4908                 283.3356   \n",
      "1         NaN           0.0              145.9012                 333.6028   \n",
      "2         NaN           0.0              151.9752                 199.3560   \n",
      "3         NaN           0.0             1624.1192                 226.9436   \n",
      "4         NaN           0.0             2467.7536                 209.7964   \n",
      "\n",
      "   Tot Reposo Inferior  Tot  Reposo Superior  Tot_L3_L4_CO2_Unificado  \\\n",
      "0            3093.1960                 0.012                      NaN   \n",
      "1            2771.0868                 0.508                      NaN   \n",
      "2            2417.1620                 0.312                      NaN   \n",
      "3            2239.8384                 0.748                      NaN   \n",
      "4            2836.4704                 0.056                      NaN   \n",
      "\n",
      "   ESTACION  \n",
      "0    Verano  \n",
      "1    Verano  \n",
      "2    Verano  \n",
      "3    Verano  \n",
      "4    Verano  \n",
      "\n",
      "[5 rows x 352 columns]\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# --- EJECUCI√ìN DEL SCRIPT DE AGRUPACI√ìN ---\n",
    "# ======================================================\n",
    "# Asumimos que 'obj' es una instancia existente de tu clase 'make_dataset'\n",
    "\n",
    "# 1. Definir las columnas que NO se deben promediar\n",
    "\n",
    "lista_texto=[]\n",
    "\n",
    "\n",
    "# 3. Llamar al m√©todo de la clase\n",
    "# Esto modificar√° el DataFrame 'obj.df' internamente\n",
    "obj.agrupar_por_media(cols_binarias_numericas, lista_texto)\n",
    "\n",
    "# 4. Verificar el resultado\n",
    "print(f\"\\nDimensiones finales del obj.df: {obj.df.shape}\")\n",
    "print(obj.df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8deb0f",
   "metadata": {},
   "source": [
    "Ya construido este dataset final, lo almacenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b46e0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta 'dataset_FE' ya existe.\n",
      "‚úÖ DataFrame guardado exitosamente en: dataset_FE\\dataset_limpio.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_folder = 'dataset_FE'\n",
    "output_filename = 'dataset_limpio.csv' \n",
    "\n",
    "# Esto crea la ruta correcta: 'dataset_FE/dataset_limpio.csv'\n",
    "full_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "# --- 2. Crear la carpeta si no existe ---\n",
    "try:\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Carpeta creada en: {output_folder}\")\n",
    "    else:\n",
    "        print(f\"La carpeta '{output_folder}' ya existe.\")\n",
    "\n",
    "    # --- 3. Guardar el DataFrame ---\n",
    "    \n",
    "    # Usamos index=False para evitar que se guarde el √≠ndice de pandas\n",
    "    # como una columna extra 'Unnamed: 0' en el CSV.\n",
    "    # Usamos sep=';' y decimal='.' para buena compatibilidad con Excel.\n",
    "    \n",
    "    obj.df.to_csv(full_path, index=False, sep=';', decimal='.') \n",
    "    \n",
    "    print(f\"‚úÖ DataFrame guardado exitosamente en: {full_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al guardar el archivo: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
