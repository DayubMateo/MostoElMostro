{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2bef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from columnas_seleccionadas import COLUMNAS_SELECCIONADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d368d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_1748\\1861678415.py:1: DtypeWarning: Columns (124,171,196,354,355,356,357,358,365) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv('Dataset_csv_unificado_completo/Totalizadores_TODO.csv')\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('Dataset_csv_unificado_completo/Totalizadores_TODO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a7fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe[COLUMNAS_SELECCIONADAS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "758f062f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(662416, 80)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8452c1b",
   "metadata": {},
   "source": [
    "# Construcci√≥n del dataset\n",
    "El dataset cuenta con multiples registros sobre diferentes features relacionadas con una fabrica de cerveza que producen diferentes tipos. El dataset cuenta con (multiples) registros diarios sobre el estado de estas diferentes features, pero tambi√©n con contenido basura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cfaedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset():\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"Retorna la forma ACTUAL del dataframe interno.\"\"\"\n",
    "        return self.df.shape\n",
    "\n",
    "    def columnas_problematicas(self):\n",
    "        \"\"\"\n",
    "        Identifica, elimina y reporta columnas problem√°ticas:\n",
    "        1. Columnas completamente vac√≠as (todos sus valores son nulos).\n",
    "        2. Columnas que contienen 'Unnamed' en su nombre (comunes en CSVs).\n",
    "        3. Columnas cuyo nombre es un n√∫mero (ej. \"1\", \"2.5\").\n",
    "        4. Columnas cuyo nombre es un espacio en blanco (' ').\n",
    "        5. Columnas de metadatos espec√≠ficas (ej. 'Ultimo Dato del Dia').\n",
    "        \n",
    "        Retorna:\n",
    "            (list): Una lista con los nombres de todas las columnas eliminadas.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"--------- Buscando columnas problem√°ticas ---------\")\n",
    "        \n",
    "        # --- L√≥gica 1: Detectar columnas TOTALMENTE NULAS ---\n",
    "        columnas_totalmente_nulas = self.df.columns[self.df.isnull().all()].tolist()\n",
    "        \n",
    "        if columnas_totalmente_nulas:\n",
    "            print(f\"üßπ Detectadas {len(columnas_totalmente_nulas)} columnas totalmente nulas:\")\n",
    "            print(f\"   -> {columnas_totalmente_nulas}\")\n",
    "        else:\n",
    "            print(\"üëç No se encontraron columnas totalmente nulas.\")\n",
    "            \n",
    "        # --- L√≥gica 2: Detectar columnas 'Unnamed' ---\n",
    "        # Usamos str(col).lower() para ser robustos y case-insensitive\n",
    "        columnas_unnamed = [col for col in self.df.columns if 'unnamed' in str(col).lower()]\n",
    "        \n",
    "        if columnas_unnamed:\n",
    "            print(f\"üßπ Detectadas {len(columnas_unnamed)} columnas 'Unnamed':\")\n",
    "            print(f\"   -> {columnas_unnamed}\")\n",
    "        else:\n",
    "            print(\"üëç No se encontraron columnas 'Unnamed'.\")\n",
    "\n",
    "        # --- L√≥gica 3: Detectar columnas con nombre num√©rico ---\n",
    "        columnas_nombre_numerico = []\n",
    "        for col in self.df.columns:\n",
    "            try:\n",
    "                # Convertir a str por si el nombre ya es un int o float\n",
    "                float(str(col))\n",
    "                # Si esto funciona, el nombre de la columna es un n√∫mero\n",
    "                columnas_nombre_numerico.append(col)\n",
    "            except ValueError:\n",
    "                # Si falla, el nombre no es un n√∫mero (ej. \"Planta (Kw)\"), \n",
    "                # lo cual es correcto.\n",
    "                pass\n",
    "        \n",
    "        if columnas_nombre_numerico:\n",
    "            print(f\"üßπ Detectadas {len(columnas_nombre_numerico)} columnas con nombre num√©rico:\")\n",
    "            print(f\"   -> {columnas_nombre_numerico}\")\n",
    "        else:\n",
    "            print(\"üëç No se encontraron columnas con nombre num√©rico.\")\n",
    "\n",
    "        # --- L√≥gica 4: Detectar columnas con nombre de espacio ---\n",
    "        columnas_espacio_en_blanco = [col for col in self.df.columns if str(col) == ' ']\n",
    "        \n",
    "        if columnas_espacio_en_blanco:\n",
    "            print(f\"üßπ Detectadas {len(columnas_espacio_en_blanco)} columnas con nombre de espacio (' '):\")\n",
    "            print(f\"   -> {columnas_espacio_en_blanco}\")\n",
    "        else:\n",
    "            print(\"üëç No se encontraron columnas con nombre de espacio.\")\n",
    "\n",
    "        # --- L√≥gica 5: Detectar columnas de metadatos (NUEVO) ---\n",
    "        columna_metadato_especifica = 'Ultimo Dato del Dia'\n",
    "        columnas_metadatos = []\n",
    "        if columna_metadato_especifica in self.df.columns:\n",
    "            columnas_metadatos.append(columna_metadato_especifica)\n",
    "            print(f\"üßπ Detectada columna de metadatos: ['{columna_metadato_especifica}']\")\n",
    "        else:\n",
    "            print(\"üëç No se encontr√≥ la columna de metadatos ('Ultimo Dato del Dia').\")\n",
    "\n",
    "\n",
    "        # --- Combinar y Eliminar ---\n",
    "        # Usamos un 'set' para combinar las listas y evitar duplicados\n",
    "        columnas_a_eliminar = list(set(\n",
    "            columnas_totalmente_nulas + \n",
    "            columnas_unnamed + \n",
    "            columnas_nombre_numerico +\n",
    "            columnas_espacio_en_blanco +\n",
    "            columnas_metadatos  # <-- A√±adimos la nueva lista\n",
    "        ))\n",
    "        \n",
    "        if columnas_a_eliminar:\n",
    "            print(f\"\\nüóëÔ∏è Eliminando un total de {len(columnas_a_eliminar)} columnas...\")\n",
    "            self.df.drop(columns=columnas_a_eliminar, inplace=True)\n",
    "            print(f\"üìä Limpieza de columnas finalizada.\")\n",
    "        else:\n",
    "            print(\"\\nüëç No hay columnas problem√°ticas para eliminar.\")\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "        # Retornamos la lista de lo que se elimin√≥\n",
    "        return columnas_a_eliminar\n",
    "\n",
    "    # -----------------------\n",
    "    # Limpieza de FILAS\n",
    "    # -----------------------\n",
    "    def filas_problematicas(self, columnas_clave):\n",
    "        \"\"\"\n",
    "        Elimina y reporta filas donde columnas esenciales (como 'DIA' o 'HORA')\n",
    "        tienen valores nulos.\n",
    "        \n",
    "        Par√°metros:\n",
    "            columnas_clave (list): Lista de columnas que no pueden tener nulos.\n",
    "        \n",
    "        Retorna:\n",
    "            (int): El n√∫mero total de filas eliminadas.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"--------- Buscando filas problem√°ticas en {columnas_clave} ---------\")\n",
    "        filas_eliminadas_total = 0\n",
    "        \n",
    "        for col in columnas_clave:\n",
    "            if col in self.df.columns:\n",
    "                filas_antes = len(self.df)\n",
    "                self.df.dropna(subset=[col], inplace=True)\n",
    "                filas_despues = len(self.df)\n",
    "                filas_eliminadas_ronda = filas_antes - filas_despues\n",
    "                \n",
    "                if filas_eliminadas_ronda > 0:\n",
    "                    print(f\"‚úÖ Eliminadas {filas_eliminadas_ronda} filas con '{col}' vac√≠o\")\n",
    "                    filas_eliminadas_total += filas_eliminadas_ronda\n",
    "            \n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Aviso: La columna clave '{col}' no existe. No se limpiaron filas.\")\n",
    "\n",
    "        if filas_eliminadas_total == 0:\n",
    "            print(\"üëç No se encontraron filas problem√°ticas para eliminar.\")\n",
    "        else:\n",
    "            print(f\"üìä Limpieza de filas finalizada. Total eliminadas: {filas_eliminadas_total}.\")\n",
    "            \n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "        return filas_eliminadas_total\n",
    "        \n",
    "\n",
    "    # -----------------------\n",
    "    # Valores negativos\n",
    "    # -----------------------\n",
    "    def valores_negativos_por_columna(self, columnas_permiten_negativos: list):\n",
    "        \"\"\"\n",
    "        Identifica todas las columnas num√©ricas que tienen valores negativos\n",
    "        y los reemplaza por np.nan.\n",
    "        \n",
    "        Excluye del reemplazo a las columnas especificadas en la lista\n",
    "        'columnas_permiten_negativos' (ej. 'Temp Tq Intermedio').\n",
    "        \n",
    "        Par√°metros:\n",
    "            columnas_permiten_negativos (list): Lista de columnas que S√ç \n",
    "                                                pueden ser negativas.\n",
    "        \n",
    "        Retorna:\n",
    "            (int): El n√∫mero total de celdas reemplazadas.\n",
    "        \"\"\"\n",
    "        print(f\"--------- Tratando valores negativos ---------\")\n",
    "        \n",
    "        # 1. Identificar todas las columnas num√©ricas del dataframe\n",
    "        columnas_numericas = self.df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        # 2. Convertir la lista de excepciones a un 'set' para b√∫squedas m√°s r√°pidas\n",
    "        set_excepciones = set(columnas_permiten_negativos)\n",
    "\n",
    "        total_reemplazos = 0\n",
    "        columnas_afectadas = 0\n",
    "        \n",
    "        print(f\"‚ÑπÔ∏è Excepciones (columnas que se ignorar√°n): {list(set_excepciones)}\")\n",
    "        \n",
    "        # 3. Iterar sobre todas las columnas num√©ricas\n",
    "        for col in columnas_numericas:\n",
    "            \n",
    "            # 4. Si la columna est√° en las excepciones, la saltamos\n",
    "            if col in set_excepciones:\n",
    "                continue\n",
    "                \n",
    "            # 5. Si no es una excepci√≥n, buscamos negativos\n",
    "            \n",
    "            # Creamos una m√°scara (un True/False) para los valores < 0\n",
    "            # Usamos .loc para asegurarnos de que no haya errores de tipo\n",
    "            try:\n",
    "                mascara_negativos = self.df[col] < 0\n",
    "            except TypeError:\n",
    "                # Esto puede pasar si la columna es num√©rica pero tiene\n",
    "                # objetos mixtos. Mejor saltarla.\n",
    "                print(f\"‚ö†Ô∏è - Omitiendo columna '{col}' (posible tipo de dato mixto).\")\n",
    "                continue\n",
    "\n",
    "            # Contamos cu√°ntos negativos hay\n",
    "            negativos_count = mascara_negativos.sum()\n",
    "            \n",
    "            if negativos_count > 0:\n",
    "                # 6. Reemplazamos esos valores por np.nan\n",
    "                # Usamos .loc para modificar self.df de forma segura y directa\n",
    "                self.df.loc[mascara_negativos, col] = np.nan\n",
    "                \n",
    "                print(f\"  - Columna '{col}': {negativos_count} valores negativos convertidos a NaN.\")\n",
    "                total_reemplazos += negativos_count\n",
    "                columnas_afectadas += 1\n",
    "\n",
    "        if total_reemplazos == 0:\n",
    "            print(\"üëç No se encontraron valores negativos no deseados.\")\n",
    "        else:\n",
    "             print(f\"üìä Tratamiento finalizado. {total_reemplazos} celdas reemplazadas en {columnas_afectadas} columnas.\")\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "        # Retornamos el conteo por si es √∫til\n",
    "        return total_reemplazos\n",
    "\n",
    "    # -----------------------\n",
    "    # Redondeo horarios\n",
    "    # -----------------------\n",
    "    def redondear_horarios(self):\n",
    "        col = \"HORA\"\n",
    "        if col not in self.df.columns:\n",
    "            return self.df\n",
    "\n",
    "        # Convertir a datetime\n",
    "        self.df[col] = pd.to_datetime(self.df[col], format=\"%H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "        def redondear(hora):\n",
    "            if pd.isna(hora):\n",
    "                return None\n",
    "\n",
    "            # √öltima hora del d√≠a\n",
    "            if hora.hour == 23 and hora.minute == 59 and hora.second !=59:\n",
    "                return \"23:59:59\" #aca cambie, en teoria no deberia afectar: \n",
    "\n",
    "            # Redondear a la hora m√°s cercana\n",
    "            if hora.minute >= 30:\n",
    "                nueva_hora = (hora + pd.Timedelta(hours=1)).replace(minute=0, second=0)\n",
    "            else:\n",
    "                nueva_hora = hora.replace(minute=0, second=0)\n",
    "\n",
    "            # Si se pasa de d√≠a, dejar 23:59:59\n",
    "            if nueva_hora.hour == 0 and hora.hour == 23:\n",
    "                return \"23:59:59\"\n",
    "\n",
    "            return nueva_hora.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        self.df[col] = self.df[col].apply(redondear)\n",
    "        return self.df\n",
    "\n",
    "    def add_estacion(self):\n",
    "      \"\"\"\n",
    "      Agrega una columna 'ESTACION' al dataframe con la estaci√≥n del a√±o\n",
    "      (Primavera, Verano, Oto√±o, Invierno) seg√∫n la fecha en M√©xico (hemisferio norte).\n",
    "      Requiere que self.df tenga la columna 'DIA' en formato datetime.\n",
    "      \"\"\"\n",
    "      if \"DIA\" not in self.df.columns:\n",
    "          print(\"‚ùå No existe la columna 'DIA' en el dataframe.\")\n",
    "          return\n",
    "\n",
    "      # Asegurar formato datetime\n",
    "      self.df[\"DIA\"] = pd.to_datetime(self.df[\"DIA\"], errors=\"coerce\")\n",
    "\n",
    "      def obtener_estacion(fecha):\n",
    "          if pd.isna(fecha):\n",
    "              return None\n",
    "\n",
    "          a√±o = fecha.year\n",
    "          # Fechas de cambio de estaci√≥n (en hemisferio norte)\n",
    "          primavera = (pd.Timestamp(year=a√±o, month=3, day=21), pd.Timestamp(year=a√±o, month=6, day=20))\n",
    "          verano = (pd.Timestamp(year=a√±o, month=6, day=21), pd.Timestamp(year=a√±o, month=9, day=22))\n",
    "          otonio = (pd.Timestamp(year=a√±o, month=9, day=23), pd.Timestamp(year=a√±o, month=12, day=20))\n",
    "          # Invierno cruza el a√±o\n",
    "          invierno_1 = (pd.Timestamp(year=a√±o, month=12, day=21), pd.Timestamp(year=a√±o + 1, month=3, day=20))\n",
    "\n",
    "          if primavera[0] <= fecha <= primavera[1]:\n",
    "              return \"Primavera\"\n",
    "          elif verano[0] <= fecha <= verano[1]:\n",
    "              return \"Verano\"\n",
    "          elif otonio[0] <= fecha <= otonio[1]:\n",
    "              return \"Oto√±o\"\n",
    "          elif (fecha >= invierno_1[0]) or (fecha <= invierno_1[1]):\n",
    "              return \"Invierno\"\n",
    "          else:\n",
    "              return None\n",
    "\n",
    "      self.df[\"ESTACION\"] = self.df[\"DIA\"].apply(obtener_estacion)\n",
    "      print(\"‚úÖ Columna 'ESTACION' agregada correctamente.\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Agregar temperatura y d√≠a de la semana\n",
    "    # -----------------------\n",
    "    def add_temp_y_dia(self, temp_df):\n",
    "    # Columnas fecha\n",
    "      if 'DIA' not in self.df.columns or 'fecha' not in temp_df.columns:\n",
    "          return\n",
    "\n",
    "      self.df[\"DIA\"] = pd.to_datetime(self.df[\"DIA\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "      temp_df[\"fecha\"] = pd.to_datetime(temp_df[\"fecha\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "      # Columna combinada fecha-hora\n",
    "      self.df[\"fecha_hora\"] = pd.to_datetime(\n",
    "          self.df[\"DIA\"].astype(str) + \" \" + self.df[\"HORA\"].astype(str),\n",
    "          format=\"%Y-%m-%d %H:%M:%S\",\n",
    "          errors=\"coerce\"\n",
    "      )\n",
    "      temp_df[\"fecha_hora\"] = pd.to_datetime(\n",
    "          temp_df[\"fecha\"].astype(str) + \" \" + temp_df[\"hora\"].astype(str),\n",
    "          format=\"%Y-%m-%d %H:%M:%S\",\n",
    "          errors=\"coerce\"\n",
    "      )\n",
    "\n",
    "      # Caso especial 23:59:59 ‚Üí d√≠a siguiente 00:00:00\n",
    "      mask_2359 = self.df[\"HORA\"] == \"23:59:59\"\n",
    "      self.df.loc[mask_2359, \"fecha_hora\"] = self.df.loc[mask_2359, \"fecha_hora\"] + pd.Timedelta(days=1)\n",
    "      self.df.loc[mask_2359, \"fecha_hora\"] = self.df.loc[mask_2359, \"fecha_hora\"].dt.floor(\"D\")\n",
    "\n",
    "      # Merge para traer temperatura horaria\n",
    "      self.df = pd.merge(\n",
    "          self.df,\n",
    "          temp_df[[\"fecha_hora\", \"temperature_2m\"]],\n",
    "          on=\"fecha_hora\",\n",
    "          how=\"left\"\n",
    "      )\n",
    "\n",
    "      # Limpiar columna auxiliar\n",
    "      self.df.drop(columns=[\"fecha_hora\"], inplace=True)\n",
    "\n",
    "      # Agregar d√≠a de la semana\n",
    "      if \"DIA\" in self.df.columns:\n",
    "          self.df[\"DIA_SEMANA\"] = self.df[\"DIA\"].dt.day_name()\n",
    "\n",
    "      # --- NUEVO: agregar temperatura promedio por d√≠a ---\n",
    "      temp_promedio = temp_df.groupby(\"fecha\")[\"temperature_2m\"].mean().reset_index()\n",
    "      temp_promedio.rename(columns={\"temperature_2m\": \"temperature_promedio_dia\"}, inplace=True)\n",
    "\n",
    "      self.df = pd.merge(\n",
    "          self.df,\n",
    "          temp_promedio,\n",
    "          left_on=\"DIA\",\n",
    "          right_on=\"fecha\",\n",
    "          how=\"left\"\n",
    "      )\n",
    "      # Limpiar columna auxiliar\n",
    "      self.df.drop(columns=[\"fecha\"], inplace=True)\n",
    "\n",
    "\n",
    "    # -----------------------\n",
    "    # Promedio de EE Frio / Hl por hora\n",
    "    # -----------------------\n",
    "    def promedio_frio_por_hora(self):\n",
    "        if \"HORA\" not in self.df.columns or \"EE Frio / Hl\" not in self.df.columns:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        self.df[\"HORA\"] = pd.to_datetime(self.df[\"HORA\"], format=\"%H:%M:%S\", errors=\"coerce\").dt.time\n",
    "        resumen = (\n",
    "            self.df.groupby(\"HORA\", as_index=False)[\"EE Frio / Hl\"]\n",
    "            .mean()\n",
    "            .rename(columns={\"EE Frio / Hl\": \"frio_promedio\"})\n",
    "            .sort_values(\"HORA\")\n",
    "        )\n",
    "        return resumen\n",
    "    \n",
    "    def verificar_duplicado(self, col1, col2):\n",
    "        \"\"\"\n",
    "        Verifica si dos columnas del DataFrame interno (self.df) son duplicadas.\n",
    "        \n",
    "        Casos que comprueba:\n",
    "        1. Duplicado Perfecto (id√©nticas, incluyendo NaNs).\n",
    "        2. Duplicado con NaNs (id√©nticas en todos los valores no-nulos).\n",
    "        3. No duplicadas.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n--- Verificando: '{col1}' vs '{col2}' ---\")\n",
    "        \n",
    "        # --- 1. Chequear existencia ---\n",
    "        columnas_faltantes = []\n",
    "        if col1 not in self.df.columns:\n",
    "            columnas_faltantes.append(col1)\n",
    "        if col2 not in self.df.columns:\n",
    "            columnas_faltantes.append(col2)\n",
    "            \n",
    "        if columnas_faltantes:\n",
    "            print(f\"  ‚ùå Error: Las siguientes columnas no se encontraron: {columnas_faltantes}.\")\n",
    "            print(\"     Por favor, revisa que los nombres sean exactos (may√∫sculas, espacios, etc.)\")\n",
    "            return\n",
    "\n",
    "        # --- 2. Test 1: Duplicado Perfecto (con .equals()) ---\n",
    "        if self.df[col1].equals(self.df[col2]):\n",
    "            print(\"  ‚úÖ Resultado: ID√âNTICAS (Perfect match).\")\n",
    "            print(f\"     (Puedes eliminar una, por ejemplo '{col2}')\")\n",
    "            return\n",
    "\n",
    "        # --- 3. Test 2: Duplicado con diferencia de NaNs ---\n",
    "        mask_non_null = self.df[col1].notna() & self.df[col2].notna()\n",
    "        \n",
    "        if mask_non_null.sum() == 0:\n",
    "            print(\"  ‚ö†Ô∏è Resultado: No se pueden comparar (nunca tienen valores al mismo tiempo).\")\n",
    "            return\n",
    "            \n",
    "        diferencias = (self.df.loc[mask_non_null, col1] != self.df.loc[mask_non_null, col2])\n",
    "        num_diferencias = diferencias.sum()\n",
    "\n",
    "        if num_diferencias == 0:\n",
    "            print(\"  ‚ö†Ô∏è Resultado: DUPLICADAS (Difieren solo en la posici√≥n de los NaNs).\")\n",
    "            print(\"     (Son id√©nticas en todas las filas donde ambas tienen datos).\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Resultado: NO SON DUPLICADAS.\")\n",
    "            print(f\"     (Tienen {num_diferencias} valores diferentes donde ambas tienen datos).\")\n",
    "\n",
    "        nan_1 = self.df[col1].isna().sum()\n",
    "        nan_2 = self.df[col2].isna().sum()\n",
    "        print(f\"     Contexto: '{col1}' tiene {nan_1} NaNs. '{col2}' tiene {nan_2} NaNs.\")\n",
    "\n",
    "\n",
    "    def verificar_relevo(self, col1, col2, assume_sorted_index=True):\n",
    "        \"\"\"\n",
    "        Verifica si una columna 'toma el relevo' de otra en self.df.\n",
    "        \n",
    "        Retorna True si se cumplen dos condiciones:\n",
    "        1. Ambas columnas tienen *algunos* datos (no est√°n 100% vac√≠as).\n",
    "        2. NUNCA tienen datos en la misma fila (no hay superposici√≥n).\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n--- Verificando relevo: '{col1}' vs '{col2}' ---\")\n",
    "        \n",
    "        # --- 1. Chequear existencia ---\n",
    "        if col1 not in self.df.columns or col2 not in self.df.columns:\n",
    "            print(f\"  ‚ùå Error: Una o ambas columnas no se encontraron.\")\n",
    "            return False\n",
    "\n",
    "        # --- 2. Obtener m√°scaras de datos v√°lidos (no-NaN) ---\n",
    "        validos_col1 = self.df[col1].notna()\n",
    "        validos_col2 = self.df[col2].notna()\n",
    "\n",
    "        # --- 3. Condici√≥n 1: ¬øAmbas columnas tienen *algunos* datos? ---\n",
    "        ambas_tienen_datos = validos_col1.any() and validos_col2.any()\n",
    "        \n",
    "        if not ambas_tienen_datos:\n",
    "            print(\"  ‚ùå FALSO: Una o ambas columnas est√°n 100% vac√≠as (NaN).\")\n",
    "            return False\n",
    "\n",
    "        # --- 4. Condici√≥n 2: ¬øHay superposici√≥n? (Debe ser 0) ---\n",
    "        superposicion = (validos_col1 & validos_col2).sum()\n",
    "        \n",
    "        if superposicion > 0:\n",
    "            print(f\"  ‚ùå FALSO: Las columnas se superponen en {superposicion} filas.\")\n",
    "            return False\n",
    "\n",
    "        # Si llegamos aqu√≠, ambas condiciones se cumplen.\n",
    "        print(\"  ‚úÖ VERDADERO: Las columnas tienen datos pero nunca se superponen.\")\n",
    "        print(\"     (Esto confirma el patr√≥n de 'relevo' de datos).\")\n",
    "        \n",
    "        # --- 5. (Opcional) Proveer m√°s contexto si el √≠ndice est√° ordenado ---\n",
    "        if assume_sorted_index:\n",
    "            try:\n",
    "                fin_col1 = self.df.index[validos_col1].max()\n",
    "                inicio_col2 = self.df.index[validos_col2].min()\n",
    "                fin_col2 = self.df.index[validos_col2].max()\n",
    "                inicio_col1 = self.df.index[validos_col1].min()\n",
    "\n",
    "                if fin_col1 < inicio_col2:\n",
    "                    print(f\"     (Patr√≥n limpio: '{col1}' termina en {fin_col1}, luego '{col2}' empieza en {inicio_col2})\")\n",
    "                elif fin_col2 < inicio_col1:\n",
    "                    print(f\"     (Patr√≥n limpio: '{col2}' termina en {fin_col2}, luego '{col1}' empieza en {inicio_col1})\")\n",
    "                else:\n",
    "                    print(\"     (Los datos est√°n intercalados, no en un bloque 'antes' y 'despu√©s')\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"     (No se pudo verificar el orden cronol√≥gico del relevo: {e})\")\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    # -----------------------\n",
    "    # Verificaci√≥n de Consistencia\n",
    "    # -----------------------\n",
    "\n",
    "    def verificar_consistencia_binaria(self, columnas_binarias: list):\n",
    "        \"\"\"\n",
    "        Verifica si hay inconsistencias en columnas binarias para tuplas (DIA, HORA) duplicadas.\n",
    "\n",
    "        Una inconsistencia ocurre si para el mismo (DIA, HORA), una columna binaria\n",
    "        tiene valores diferentes (ej. 0 y 1), ignorando los NaN.\n",
    "        \n",
    "        Args:\n",
    "            columnas_binarias (list): La lista de columnas binarias a verificar.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (bool, dict)\n",
    "                - bool: True si se encontraron inconsistencias, False si no.\n",
    "                - dict: Un diccionario donde las claves son las tuplas (DIA, HORA)\n",
    "                        problem√°ticas y los valores son las columnas con\n",
    "                        inconsistencias.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"--- Iniciando verificaci√≥n de consistencia en tuplas (DIA, HORA) duplicadas ---\")\n",
    "        \n",
    "        # 1. Agrupar por la tupla clave\n",
    "        # Usamos el DataFrame interno 'self.df'\n",
    "        grupos = self.df.groupby(['DIA', 'HORA'])\n",
    "        \n",
    "        problematic_groups = {}\n",
    "        hay_inconsistencia = False\n",
    "        \n",
    "        grupos_con_duplicados = 0\n",
    "        \n",
    "        # 2. Iterar sobre cada grupo (DIA, HORA)\n",
    "        for (dia, hora), group in grupos:\n",
    "            \n",
    "            # Solo nos interesan los grupos con duplicados\n",
    "            if len(group) > 1:\n",
    "                grupos_con_duplicados += 1\n",
    "                inconsistencias_en_col = []\n",
    "                \n",
    "                # 3. Revisar cada columna binaria dentro de ese grupo\n",
    "                for col in columnas_binarias:\n",
    "                    if col not in group.columns:\n",
    "                        continue # Seguridad por si la columna no existe\n",
    "                    \n",
    "                    # 4. Obtener los valores √∫nicos NO-NAN\n",
    "                    # Esta es la l√≥gica clave: ignoramos los NaN\n",
    "                    valores_unicos = group[col].dropna().unique()\n",
    "                    \n",
    "                    # 5. Si hay m√°s de 1 valor √∫nico (ej. [0.0, 1.0]), es una inconsistencia\n",
    "                    if len(valores_unicos) > 1:\n",
    "                        hay_inconsistencia = True\n",
    "                        inconsistencias_en_col.append(col)\n",
    "                \n",
    "                # 6. Si encontramos columnas inconsistentes, guardamos el reporte\n",
    "                if inconsistencias_en_col:\n",
    "                    problematic_groups[(dia, hora)] = inconsistencias_en_col\n",
    "\n",
    "        print(f\"\\nRevisados {len(grupos)} grupos ('DIA', 'HORA') √∫nicos.\")\n",
    "        print(f\"Se encontraron {grupos_con_duplicados} grupos con filas duplicadas.\")\n",
    "        \n",
    "        if hay_inconsistencia:\n",
    "            print(f\"‚ùå ¬°Problema! Se encontraron {len(problematic_groups)} tuplas (DIA, HORA) con datos binarios inconsistentes.\")\n",
    "        else:\n",
    "            print(\"‚úÖ ¬°√âxito! Todas las tuplas (DIA, HORA) duplicadas son consistentes en las columnas binarias.\")\n",
    "            \n",
    "        return hay_inconsistencia, problematic_groups\n",
    "    \n",
    "    def verificar_suma_jerarquica(self, hipotesis_dict):\n",
    "        \"\"\"\n",
    "        Verifica si columnas \"Totales\" son la suma de sus \"partes\".\n",
    "        \n",
    "        Recibe un diccionario donde cada clave es la columna \"Total\" y\n",
    "        cada valor es una lista de las columnas \"Parte\".\n",
    "        \n",
    "        Args:\n",
    "            hipotesis_dict (dict): Diccionario de hip√≥tesis. \n",
    "                                   Ej: {'Total': ['Parte1', 'Parte2']}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"--- Iniciando verificaci√≥n de {len(hipotesis_dict)} sumas jer√°rquicas ---\")\n",
    "        \n",
    "        for col_total, lista_partes in hipotesis_dict.items():\n",
    "            print(f\"\\n--- Verificando: '{col_total}' vs Suma de {len(lista_partes)} partes ---\")\n",
    "            \n",
    "            # --- 1. Verificar que todas las columnas existan ---\n",
    "            columnas_a_chequear = [col_total] + lista_partes\n",
    "            columnas_faltantes = [col for col in columnas_a_chequear if col not in self.df.columns]\n",
    "            \n",
    "            if columnas_faltantes:\n",
    "                print(f\"  ‚ùå Error: Faltan columnas: {columnas_faltantes}. Saltando.\")\n",
    "                continue\n",
    "                \n",
    "            # --- 2. Sumamos las partes ---\n",
    "            # fillna(0) es crucial por si una de las partes tiene NaN\n",
    "            suma_calculada = self.df[lista_partes].fillna(0).sum(axis=1)\n",
    "            \n",
    "            # --- 3. Comparamos el total reportado vs. la suma calculada ---\n",
    "            # Usamos np.allclose para comparar n√∫meros flotantes, lo que\n",
    "            # permite peque√±os errores de precisi√≥n (tolerancia).\n",
    "            son_iguales = np.allclose(\n",
    "                self.df[col_total].fillna(0),  # Rellenamos NaNs en el total\n",
    "                suma_calculada, \n",
    "                rtol=1e-03, # Tolerancia relativa (0.1%)\n",
    "                atol=1e-05  # Tolerancia absoluta\n",
    "            )\n",
    "\n",
    "            if son_iguales:\n",
    "                print(f\"  ‚úÖ Resultado: REDUNDANCIA CONFIRMADA.\")\n",
    "                print(f\"     ('{col_total}' es la suma de sus partes).\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Resultado: NO SON REDUNDANTES.\")\n",
    "                print(f\"     ('{col_total}' NO es la suma simple de sus partes).\")\n",
    "                \n",
    "                # Mostramos un resumen de la diferencia\n",
    "                try:\n",
    "                    diferencia = (self.df[col_total] - suma_calculada).abs()\n",
    "                    print(f\"     Diferencia promedio: {diferencia.mean():.4f}\")\n",
    "                    print(f\"     Diferencia m√°xima:   {diferencia.max():.4f}\")\n",
    "                except TypeError:\n",
    "                    print(\"     (No se pudo calcular la diferencia, posible error de tipos).\")\n",
    "\n",
    "    def verificar_exclusividad_mutua(self, hipotesis_grupos):\n",
    "        \"\"\"\n",
    "        Verifica si grupos de columnas binarias (dummies) son mutuamente excluyentes.\n",
    "\n",
    "        La prueba comprueba que para cualquier fila, la suma de las columnas\n",
    "        del grupo sea como m√°ximo 1 (es decir, solo una puede estar \"activa\").\n",
    "        \n",
    "        Args:\n",
    "            hipotesis_grupos (dict): Diccionario de hip√≥tesis.\n",
    "                                   Ej: {'Grupo Mostos': ['Mosto1', 'Mosto2']}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"--- Iniciando verificaci√≥n de {len(hipotesis_grupos)} grupos mutuamente excluyentes ---\")\n",
    "\n",
    "        for nombre_grupo, lista_columnas in hipotesis_grupos.items():\n",
    "            print(f\"\\n--- Verificando Grupo: '{nombre_grupo}' ({len(lista_columnas)} columnas) ---\")\n",
    "\n",
    "            # --- 1. Verificar que todas las columnas existan ---\n",
    "            columnas_faltantes = [col for col in lista_columnas if col not in self.df.columns]\n",
    "            \n",
    "            if columnas_faltantes:\n",
    "                print(f\"  ‚ùå Error: Faltan columnas: {columnas_faltantes}. Saltando.\")\n",
    "                continue\n",
    "            \n",
    "            # --- 2. Calcular la suma por fila (asumiendo NaN=0) ---\n",
    "            try:\n",
    "                # fillna(0) es clave para tratar los NaNs como \"inactivo\"\n",
    "                suma_por_fila = self.df[lista_columnas].fillna(0).sum(axis=1)\n",
    "            except TypeError:\n",
    "                print(f\"  ‚ùå Error: No se pudo sumar. ¬øAlguna columna no es num√©rica? Saltando.\")\n",
    "                # Esto puede pasar si las columnas est√°n mal identificadas (ej. 'Totalizador Bba P1' \n",
    "                # que sali√≥ como 'object' pero se quiere tratar como binaria)\n",
    "                continue\n",
    "\n",
    "            # --- 3. Analizar el resultado ---\n",
    "            max_suma = suma_por_fila.max()\n",
    "            \n",
    "            if max_suma <= 1:\n",
    "                print(f\"  ‚úÖ Resultado: SON MUTUAMENTE EXCLUYENTES.\")\n",
    "                print(f\"     (Nunca hay m√°s de una columna 'activa' (1) al mismo tiempo).\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå Resultado: NO SON MUTUAMENTE EXCLUYENTES.\")\n",
    "                filas_violadas = (suma_por_fila > 1).sum()\n",
    "                print(f\"     (Se encontraron {filas_violadas} filas donde la suma es > 1).\")\n",
    "                print(f\"     (La suma m√°xima encontrada en una fila fue: {max_suma}).\")\n",
    "\n",
    "    def forzar_conversion_numerica(self, columnas_a_convertir: list):\n",
    "        \"\"\"\n",
    "        Fuerza la conversi√≥n de una lista de columnas a tipo num√©rico.\n",
    "        \n",
    "        Utiliza pd.to_numeric con errors='coerce', que convertir√° \n",
    "        autom√°ticamente cualquier valor no num√©rico (ej. \"Error\") en NaN.\n",
    "        \n",
    "        Args:\n",
    "            columnas_a_convertir (list): Lista de nombres de columnas.\n",
    "            \n",
    "        Returns:\n",
    "            int: El n√∫mero de columnas que cambiaron su tipo de dato.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"--- Forzando conversi√≥n a num√©rico en {len(columnas_a_convertir)} columnas ---\")\n",
    "        \n",
    "        columnas_convertidas = 0\n",
    "        for col in columnas_a_convertir:\n",
    "            if col in self.df.columns:\n",
    "                tipo_anterior = self.df[col].dtype\n",
    "                \n",
    "                # Aplicamos la conversi√≥n robusta\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "                \n",
    "                tipo_nuevo = self.df[col].dtype\n",
    "                \n",
    "                if tipo_anterior != tipo_nuevo:\n",
    "                    print(f\"  ‚úÖ Columna '{col}': Convertida ({tipo_anterior} -> {tipo_nuevo})\")\n",
    "                    columnas_convertidas += 1\n",
    "                else:\n",
    "                    print(f\"  ‚ÑπÔ∏è Columna '{col}': Ya era num√©rica ({tipo_nuevo}).\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"  ‚ùå Error: No se encontr√≥ la columna '{col}'.\")\n",
    "\n",
    "        print(f\"\\nConversi√≥n completada. Se convirtieron {columnas_convertidas} columnas.\")\n",
    "        return columnas_convertidas\n",
    "    \n",
    "\n",
    "    # -----------------------\n",
    "    # Agrupaci√≥n y Consolidaci√≥n\n",
    "    # -----------------------\n",
    "\n",
    "    def agrupar_por_media(self, columnas_binarias: list, columnas_texto: list = None):\n",
    "        \"\"\"\n",
    "        [MODIFICADO] Agrupa el DataFrame interno (self.df) por (DIA) para \n",
    "        consolidar todas las filas en un PROMEDIO DIARIO.\n",
    "        \n",
    "        ¬°ADVERTENCIA: Esto elimina la resoluci√≥n horaria!\n",
    "        \n",
    "        - Usa 'mean' (promedio) para columnas num√©ricas.\n",
    "        - Usa 'max' para columnas binarias/dummies (si hubo un 1 en el d√≠a, queda 1).\n",
    "        - Usa 'first' para columnas de texto.\n",
    "        \n",
    "        El DataFrame self.df se actualiza internamente.\n",
    "        \n",
    "        Args:\n",
    "            columnas_binarias (list): Lista de nombres de las columnas binarias.\n",
    "            columnas_texto (list, optional): Lista de otras columnas de texto \n",
    "                                              (aparte de DIA) que deban mantenerse.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"--- Iniciando agrupaci√≥n por (DIA) usando la media (Promedio Diario) ---\")\n",
    "        \n",
    "        # 1. Definir las columnas clave (MODIFICADO)\n",
    "        columnas_clave = ['DIA'] # Agrupamos solo por D√çA\n",
    "        \n",
    "        # 2. Crear el diccionario de agregaci√≥n\n",
    "        agg_dict = {}\n",
    "        \n",
    "        # Convertir listas a 'sets' para b√∫squedas r√°pidas\n",
    "        set_binarias = set(columnas_binarias)\n",
    "        set_texto = set(columnas_texto) if columnas_texto else set()\n",
    "        \n",
    "        filas_antes = self.df.shape[0]\n",
    "\n",
    "        # 3. Iterar sobre todas las columnas para construir el diccionario\n",
    "        for col in self.df.columns:\n",
    "            # Ignorar las columnas clave\n",
    "            if col in columnas_clave:\n",
    "                continue\n",
    "            \n",
    "            # Ignorar HORA expl√≠citamente, ya que la estamos perdiendo\n",
    "            if col == 'HORA':\n",
    "                continue\n",
    "                \n",
    "            # Si es una columna binaria...\n",
    "            if col in set_binarias:\n",
    "                agg_dict[col] = 'max'\n",
    "            \n",
    "            # Si es una columna de texto...\n",
    "            elif col in set_texto:\n",
    "                agg_dict[col] = 'first'\n",
    "            \n",
    "            # Si es una columna num√©rica (no-binaria y no-clave)...\n",
    "            elif pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                agg_dict[col] = 'mean'\n",
    "            \n",
    "            # Si es cualquier otra cosa (texto no especificado)...\n",
    "            else:\n",
    "                agg_dict[col] = 'first'\n",
    "                \n",
    "        # 4. Ejecutar la agregaci√≥n\n",
    "        if not agg_dict:\n",
    "            print(\"No hay columnas para agrupar. Abortando.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Agrupando {len(agg_dict)} columnas por (DIA)...\")\n",
    "        \n",
    "        # Agrupamos y reseteamos el √≠ndice\n",
    "        df_agrupado = self.df.groupby(columnas_clave).agg(agg_dict).reset_index()\n",
    "        \n",
    "        # 5. Actualizar el DataFrame interno\n",
    "        self.df = df_agrupado\n",
    "        filas_despues = self.df.shape[0]\n",
    "        \n",
    "        print(\"\\n‚úÖ Agrupaci√≥n Diaria completada.\")\n",
    "        print(f\"   Filas reducidas de {filas_antes} a {filas_despues}.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87dcbd2",
   "metadata": {},
   "source": [
    "Vamos a eliminar columnas problematicas. Para ello consideraremos:\n",
    "- Columnas con todos sus valores nulls o bien alguna alternativa en string (\"none\", \"null\") etc\n",
    "- COlumnas probleamticas (nombres indefinidos como unnamed, .numero, ' ')\n",
    "\n",
    "Mismo analisis para las filas, donde consideraremos:\n",
    "- Filas donde datos claves como dia/hora son nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ce8a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Buscando columnas problem√°ticas ---------\n",
      "üëç No se encontraron columnas totalmente nulas.\n",
      "üëç No se encontraron columnas 'Unnamed'.\n",
      "üëç No se encontraron columnas con nombre num√©rico.\n",
      "üëç No se encontraron columnas con nombre de espacio.\n",
      "üëç No se encontr√≥ la columna de metadatos ('Ultimo Dato del Dia').\n",
      "\n",
      "üëç No hay columnas problem√°ticas para eliminar.\n",
      "--------------------------------------------------\n",
      "--------- Buscando filas problem√°ticas en ['DIA', 'HORA'] ---------\n",
      "‚úÖ Eliminadas 3618 filas con 'DIA' vac√≠o\n",
      "‚úÖ Eliminadas 1 filas con 'HORA' vac√≠o\n",
      "üìä Limpieza de filas finalizada. Total eliminadas: 3619.\n",
      "--------------------------------------------------\n",
      "(662416, 80)\n",
      "(658797, 80)\n"
     ]
    }
   ],
   "source": [
    "obj = make_dataset(dataframe)\n",
    "\n",
    "columnas_eliminadas = obj.columnas_problematicas()\n",
    "filas_eliminadas = obj.filas_problematicas(['DIA', 'HORA'])\n",
    "\n",
    "print(dataframe.shape)\n",
    "print(obj.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de5d18",
   "metadata": {},
   "source": [
    "**Valores negativos**\n",
    "\n",
    "Filas con valores negativos: Hay muchas filas con valores negativos. Para ello nos ponemos a inspeccionar si alguna de estas es l√≥gica o probable que alberguen valores negativos y no se tratan de errores.\n",
    "    - Variables de temperatura en centigrados pueden ser negativas\n",
    "    - las variables totalizadoras NO pueden ser negativas\n",
    "    - columnas asociadas a tasas de volumen (/hl) no deberian ser negativas\n",
    "    - Columnas totalizadoras no tienen que ser negativas\n",
    "    - Columnas de potencia en Kw no deber√≠an ser negativas.\n",
    "    - Tot Vap Paset L3/hora y vapor de caldera deben ser positivos\n",
    "    - Retorno PLanta C=2. Es posible que sea negativo\n",
    "    - Red paste L3 y Agua lavadora L3. flujos probablemente positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be26979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Tratando valores negativos ---------\n",
      "‚ÑπÔ∏è Excepciones (columnas que se ignorar√°n): ['Retorno Planta CO2', 'Temp Tq Intermedio']\n",
      "  - Columna 'EE Planta / Hl': 28 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Elaboracion / Hl': 93 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Bodega / Hl': 295 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Servicios / Hl': 4 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Aire / Hl': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'EE Agua / Hl': 193 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Planta / Hl': 36 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Planta de Agua/Hl': 656 valores negativos convertidos a NaN.\n",
      "  - Columna 'Produccion Agua / Hl': 38 valores negativos convertidos a NaN.\n",
      "  - Columna 'CO 2 / Hl': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Planta (Kw)': 31 valores negativos convertidos a NaN.\n",
      "  - Columna 'Elaboracion (Kw)': 177 valores negativos convertidos a NaN.\n",
      "  - Columna 'Bodega (Kw)': 376 valores negativos convertidos a NaN.\n",
      "  - Columna 'Servicios (Kw)': 5 valores negativos convertidos a NaN.\n",
      "  - Columna 'Aire (Kw)': 2 valores negativos convertidos a NaN.\n",
      "  - Columna 'Prod Agua (Kw)': 195 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Planta (Hl)': 38 valores negativos convertidos a NaN.\n",
      "  - Columna 'Agua Servicios (Hl)': 694 valores negativos convertidos a NaN.\n",
      "  - Columna 'Planta de agua (Hl)': 31 valores negativos convertidos a NaN.\n",
      "  - Columna 'Aire Servicios (M3)': 786 valores negativos convertidos a NaN.\n",
      "üìä Tratamiento finalizado. 3682 celdas reemplazadas en 20 columnas.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columnas_excepcion = [\n",
    "    'Temp Tq Intermedio',\n",
    "    'Retorno Planta CO2'\n",
    "]\n",
    "total_reemplazos = obj.valores_negativos_por_columna(columnas_excepcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26572d",
   "metadata": {},
   "source": [
    "**Columnas duplicadas**\n",
    "Vamos a inspeccionar si hay algunas columnas repetidas. \n",
    "Las hipotesis que planteamos son las siguientes:\n",
    "- hl de mosto y hl de mosto copia. Si resultaron ser iguales\n",
    "- tot l3 la4 y planta de c02 vs tot l3. l4 y planta de co2: \n",
    "- planta(kw) y KW gral planta: No son iguales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2443203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "--- 1. Verificando Hip√≥tesis de Duplicados ---\n",
      "======================================================\n",
      "\n",
      "--- Verificando: 'Hl de Mosto' vs 'Hl de Mosto Copia' ---\n",
      "  ‚ùå Error: Las siguientes columnas no se encontraron: ['Hl de Mosto Copia'].\n",
      "     Por favor, revisa que los nombres sean exactos (may√∫sculas, espacios, etc.)\n",
      "\n",
      "--- Verificando: 'Tot L3, L4 y Planta de CO2' vs 'Tot L3. L4 y Planta de CO2' ---\n",
      "  ‚ùå Error: Las siguientes columnas no se encontraron: ['Tot L3, L4 y Planta de CO2'].\n",
      "     Por favor, revisa que los nombres sean exactos (may√∫sculas, espacios, etc.)\n",
      "\n",
      "--- Verificando: 'Planta (Kw)' vs 'KW Gral Planta' ---\n",
      "  ‚ùå Error: Las siguientes columnas no se encontraron: ['KW Gral Planta'].\n",
      "     Por favor, revisa que los nombres sean exactos (may√∫sculas, espacios, etc.)\n",
      "\n",
      "\n",
      "======================================================\n",
      "--- 2. Verificando Hip√≥tesis de Relevo de Datos ---\n",
      "======================================================\n",
      "\n",
      "--- Verificando relevo: 'Tot L3, L4 y Planta de CO2' vs 'Tot L3. L4 y Planta de CO2' ---\n",
      "  ‚ùå Error: Una o ambas columnas no se encontraron.\n",
      "\n",
      "--- No se detect√≥ relevo para 'Tot L3, L4 y Planta de CO2', no se unificar√°n. ---\n",
      "\n",
      "======================================================\n",
      "--- Verificaci√≥n y unificaci√≥n completadas ---\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2. Lista de hip√≥tesis a probar\n",
    "hipotesis_duplicados = [\n",
    "    ('Hl de Mosto', 'Hl de Mosto Copia'),\n",
    "    ('Tot L3, L4 y Planta de CO2', 'Tot L3. L4 y Planta de CO2'),\n",
    "    ('Planta (Kw)', 'KW Gral Planta')\n",
    "]\n",
    "\n",
    "# --- Secci√≥n 1: Verificaci√≥n de Duplicados ---\n",
    "print(\"======================================================\")\n",
    "print(\"--- 1. Verificando Hip√≥tesis de Duplicados ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "for col1, col2 in hipotesis_duplicados:\n",
    "    # Llamamos al M√âTODO de la clase\n",
    "    obj.verificar_duplicado(col1, col2)\n",
    "\n",
    "# --- Secci√≥n 2: Verificaci√≥n de Relevo (Handoff) ---\n",
    "print(\"\\n\\n======================================================\")\n",
    "print(\"--- 2. Verificando Hip√≥tesis de Relevo de Datos ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "col_relevo_1 = 'Tot L3, L4 y Planta de CO2'\n",
    "col_relevo_2 = 'Tot L3. L4 y Planta de CO2'\n",
    "\n",
    "# Llamamos al M√âTODO de la clase y guardamos el resultado\n",
    "hubo_relevo = obj.verificar_relevo(\n",
    "    col_relevo_1, \n",
    "    col_relevo_2, \n",
    "    assume_sorted_index=True\n",
    ")\n",
    "\n",
    "# --- Secci√≥n 3: Unificaci√≥n basada en el resultado de Relevo ---\n",
    "if hubo_relevo:\n",
    "    print(\"\\n--- Unificando columnas con relevo... ---\")\n",
    "    \n",
    "    col_unificada = 'Tot_L3_L4_CO2_Unificado' # Nombre nuevo y limpio\n",
    "\n",
    "    # 2. Verificar que existan antes de unificar (aunque 'verificar_relevo' ya lo hizo)\n",
    "    if col_relevo_1 in obj.df.columns and col_relevo_2 in obj.df.columns:\n",
    "        \n",
    "        # 3. Crear la columna unificada\n",
    "        # Rellena los NaN de la columna 1 con los valores de la columna 2\n",
    "        obj.df[col_unificada] = obj.df[col_relevo_1].fillna(obj.df[col_relevo_2])\n",
    "\n",
    "        # 4. Eliminar las columnas originales\n",
    "        obj.df.drop(columns=[col_relevo_1, col_relevo_2], inplace=True)\n",
    "        \n",
    "        print(f\"‚úÖ Columnas unificadas exitosamente en '{col_unificada}'.\")\n",
    "        print(f\"   Columnas originales eliminadas.\")\n",
    "        \n",
    "    else:\n",
    "        # Esto no deber√≠a pasar si hubo_relevo es True, pero es una buena pr√°ctica\n",
    "        print(f\"‚ùå Error: No se encontraron las columnas '{col_relevo_1}' o '{col_relevo_2}' para unificar.\")\n",
    "else:\n",
    "     print(f\"\\n--- No se detect√≥ relevo para '{col_relevo_1}', no se unificar√°n. ---\")\n",
    "\n",
    "\n",
    "print(\"\\n======================================================\")\n",
    "print(\"--- Verificaci√≥n y unificaci√≥n completadas ---\")\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aaebeb",
   "metadata": {},
   "source": [
    "**columnas compuestas**\n",
    "Hay columnas que puedens ser la suma de dos columnas: \n",
    "A su vez analizaremos si hay columnas que son suma de columnas. Las hipotesis:\n",
    "- M3_tot_gas vs TOT GAS\n",
    "- KW linea 3 y 4 quisas sean la suma de KW linea 3 + kw linea4\n",
    "- Hl mosto = hl mosto + hl budweiser + hlmoste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45853a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================\n",
      "--- 3. Verificando Hip√≥tesis de Sumas Jer√°rquicas ---\n",
      "======================================================\n",
      "--- Iniciando verificaci√≥n de 3 sumas jer√°rquicas ---\n",
      "\n",
      "--- Verificando: 'KW Linea 3 y 4' vs Suma de 2 partes ---\n",
      "  ‚ùå Error: Faltan columnas: ['KW Linea 3 y 4', 'KW Linea 3', 'KW Linea 4']. Saltando.\n",
      "\n",
      "--- Verificando: 'Planta (Kw)' vs Suma de 6 partes ---\n",
      "  ‚ùå Error: Faltan columnas: ['Restos Planta (Kw)']. Saltando.\n",
      "\n",
      "--- Verificando: 'Servicios (Kw)' vs Suma de 8 partes ---\n",
      "  ‚ùå Error: Faltan columnas: ['Sala Maq (Kw)', 'Pta Agua / Eflu (Kw)', 'Resto Serv (Kw)']. Saltando.\n",
      "\n",
      "======================================================\n",
      "--- Verificaci√≥n completada ---\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# --- EJECUCI√ìN DEL SCRIPT DE VERIFICACI√ìN ---\n",
    "# ======================================================\n",
    "\n",
    "# --- Secci√≥n 3: Verificaci√≥n de Sumas Jer√°rquicas (NUEVO) ---\n",
    "print(\"\\n\\n======================================================\")\n",
    "print(\"--- 3. Verificando Hip√≥tesis de Sumas Jer√°rquicas ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "# Definimos el diccionario de hip√≥tesis\n",
    "hipotesis_sumas = {\n",
    "    \n",
    "    'KW Linea 3 y 4': [\n",
    "        'KW Linea 3', \n",
    "        'KW Linea 4'\n",
    "    ],\n",
    "    \n",
    "    'Planta (Kw)': [\n",
    "        'Elaboracion (Kw)', \n",
    "        'Bodega (Kw)', \n",
    "        'Cocina (Kw)', \n",
    "        'Envasado (Kw)', \n",
    "        'Servicios (Kw)', \n",
    "        'Restos Planta (Kw)'\n",
    "    ],\n",
    "    \n",
    "    'Servicios (Kw)': [\n",
    "         'Sala Maq (Kw)', \n",
    "         'Aire (Kw)', \n",
    "         'Calderas (Kw)', \n",
    "         'Efluentes (Kw)', \n",
    "         'Frio (Kw)', \n",
    "         'Pta Agua / Eflu (Kw)', \n",
    "         'Prod Agua (Kw)', \n",
    "         'Resto Serv (Kw)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Llamamos al nuevo m√©todo de la clase\n",
    "obj.verificar_suma_jerarquica(hipotesis_sumas)\n",
    "\n",
    "print(\"\\n======================================================\")\n",
    "print(\"--- Verificaci√≥n completada ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "#como no verificaron ser sumas, damos por hecho que no son redundantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c13cd",
   "metadata": {},
   "source": [
    "**Columnas categoricas**\n",
    "Vamos a inspeccionar las columnas categoricas. \n",
    "- Buscamos las columnas que son del tipo 'Objetct' o 'Category'\n",
    "- Que solamente tienen 1 y 0 de valores posibles. Esperamos encontrar aca la categoria de cerveza.\n",
    "\n",
    "dentro de las columnas objetos:\n",
    "- muchas son numericas, las cambiamos\n",
    "\n",
    "Dentro de las columnas binarias:\n",
    "- tenemos dummies como mosto, combustible, vapor\n",
    "- banderas de estados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d327b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando b√∫squeda eficiente de columnas binarias..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B√∫squeda de binarias completada.\n",
      "\n",
      "Columnas categ√≥ricas o binarias (0/1):\n",
      " Categorias texto: ['DIA', 'HORA']\n",
      " Binarias num√©ricas: []\n",
      "\n",
      "\n",
      "======================================================\n",
      "--- 4. Verificando Hip√≥tesis de Exclusividad Mutua (Dummies) ---\n",
      "======================================================\n",
      "--- Iniciando verificaci√≥n de 4 grupos mutuamente excluyentes ---\n",
      "\n",
      "--- Verificando Grupo: 'Grupo Mostos' (10 columnas) ---\n",
      "  ‚ùå Error: Faltan columnas: ['HL Mosto Budweiser', 'HL Mosto Local', 'HL Mosto Fuerte', 'HL Mosto Indio', 'HL Mosto Palermo', 'HL Mosto Bieckert', 'HL Mosto Malta', 'HL Mosto Frost', 'Hl Session IPA', 'Hl Reserva 8']. Saltando.\n",
      "\n",
      "--- Verificando Grupo: 'Grupo Fuel Oil Tanks' (2 columnas) ---\n",
      "  ‚ùå Error: Faltan columnas: ['Fuel Oil Tk1 (Kg)', 'Fuel Oil Tk2 (Kg)']. Saltando.\n",
      "\n",
      "--- Verificando Grupo: 'Grupo Calderas (Vapor)' (2 columnas) ---\n",
      "  ‚ùå Error: Faltan columnas: ['Tot_Vapor_Caldera 3', 'VAPOR DE CALDERA 1 KG']. Saltando.\n",
      "\n",
      "--- Verificando Grupo: 'Grupo Reservas' (2 columnas) ---\n",
      "  ‚ùå Error: Faltan columnas: ['Hl Reserva 7', 'Hl Reserva 8']. Saltando.\n",
      "\n",
      "======================================================\n",
      "--- Verificaci√≥n completada ---\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Columnas categ√≥ricas (texto, objeto) ---\n",
    "cols_categoricas_texto = obj.df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# --- 2. Columnas binarias (num√©ricas 0/1) ---\n",
    "print(\"Iniciando b√∫squeda eficiente de columnas binarias...\")\n",
    "cols_binarias_numericas = []\n",
    "\n",
    "# Bucle eficiente (LA SOLUCI√ìN):\n",
    "# Iteramos sobre todos los nombres de columnas (una lista ligera)\n",
    "for col in obj.df.columns:\n",
    "    \n",
    "    # Verificamos el tipo de la columna SIN copiarla\n",
    "    if pd.api.types.is_numeric_dtype(obj.df[col]):\n",
    "        \n",
    "        # Si es num√©rica, aplicamos tu l√≥gica original\n",
    "        # .unique() es r√°pido en columnas individuales\n",
    "        try:\n",
    "            valores_unicos = obj.df[col].dropna().unique()\n",
    "            \n",
    "            if set(valores_unicos).issubset({0, 1}):\n",
    "                cols_binarias_numericas.append(col)\n",
    "        except Exception as e:\n",
    "            print(f\"  - Advertencia: No se pudo procesar la columna '{col}'. Error: {e}\")\n",
    "\n",
    "print(\"B√∫squeda de binarias completada.\")\n",
    "\n",
    "# --- 3. Combinar ambas listas ---\n",
    "columnas_finales = list(set(cols_categoricas_texto + cols_binarias_numericas))\n",
    "\n",
    "print(\"\\nColumnas categ√≥ricas o binarias (0/1):\")\n",
    "print(f\" Categorias texto: {cols_categoricas_texto}\")\n",
    "print(f\" Binarias num√©ricas: {cols_binarias_numericas}\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# --- EJECUCI√ìN DEL SCRIPT DE VERIFICACI√ìN ---\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n\\n======================================================\")\n",
    "print(\"--- 4. Verificando Hip√≥tesis de Exclusividad Mutua (Dummies) ---\")\n",
    "print(\"======================================================\")\n",
    "\n",
    "# Definimos el diccionario de hip√≥tesis\n",
    "hipotesis_dummies = {\n",
    "    \n",
    "    'Grupo Mostos': [\n",
    "        'HL Mosto Budweiser', 'HL Mosto Local', 'HL Mosto Fuerte', \n",
    "        'HL Mosto Indio', 'HL Mosto Palermo', 'HL Mosto Bieckert', \n",
    "        'HL Mosto Malta', 'HL Mosto Frost', 'Hl Session IPA', 'Hl Reserva 8'\n",
    "    ],\n",
    "    \n",
    "    'Grupo Fuel Oil Tanks': [\n",
    "        'Fuel Oil Tk1 (Kg)', \n",
    "        'Fuel Oil Tk2 (Kg)'\n",
    "    ],\n",
    "    \n",
    "    'Grupo Calderas (Vapor)': [\n",
    "         'Tot_Vapor_Caldera 3',\n",
    "         'VAPOR DE CALDERA 1 KG'\n",
    "    ], \n",
    "    'Grupo Reservas': [\n",
    "        'Hl Reserva 7', \n",
    "        'Hl Reserva 8'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Llamamos al nuevo m√©todo de la clase\n",
    "# (Aseg√∫rate de que las columnas en hipotesis_dummies existan \n",
    "# y se hayan identificado correctamente como binarias)\n",
    "obj.verificar_exclusividad_mutua(hipotesis_dummies)\n",
    "\n",
    "print(\"\\n======================================================\")\n",
    "print(\"--- Verificaci√≥n completada ---\")\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c262d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Revisando valores √∫nicos de columnas 'texto' sospechosas ---\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Forzando conversi√≥n a num√©rico en 7 columnas ---\n",
      "  ‚ùå Error: No se encontr√≥ la columna 'Nivel Silo Bagazo Norte (1)'.\n",
      "  ‚ùå Error: No se encontr√≥ la columna 'KW Trafo 8'.\n",
      "  ‚ùå Error: No se encontr√≥ la columna 'Totalizador Bba P1'.\n",
      "  ‚ùå Error: No se encontr√≥ la columna 'Totalizador Bba P2'.\n",
      "  ‚ùå Error: No se encontr√≥ la columna 'Totalizador Bba P4'.\n",
      "  ‚ùå Error: No se encontr√≥ la columna 'Totalizador Bba Envasado'.\n",
      "  ‚ùå Error: No se encontr√≥ la columna 'Totalizador Bba P51'.\n",
      "\n",
      "Conversi√≥n completada. Se convirtieron 0 columnas.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Script de Diagn√≥stico (el que ya ten√≠as) ---\n",
    "columnas_sospechosas = [\n",
    "    'Nivel Silo Bagazo Norte (1)', \n",
    "    'KW Trafo 8', \n",
    "    'Totalizador Bba P1', \n",
    "    'Totalizador Bba P2', \n",
    "    'Totalizador Bba P4', \n",
    "    'Totalizador Bba Envasado', \n",
    "    'Totalizador Bba P51'\n",
    "]\n",
    "\n",
    "print(\"--- Revisando valores √∫nicos de columnas 'texto' sospechosas ---\")\n",
    "\n",
    "for col in columnas_sospechosas:\n",
    "    if col in obj.df.columns:\n",
    "        # Mostramos los primeros 20 valores √∫nicos\n",
    "        print(f\"\\nValores en '{col}':\")\n",
    "        # Asegurarnos de manejar el error si hay menos de 20 √∫nicos\n",
    "        try:\n",
    "            print(obj.df[col].unique()[:20])\n",
    "        except IndexError:\n",
    "            print(obj.df[col].unique())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 2. Llamada al nuevo m√©todo (la \"instanciaci√≥n\") ---\n",
    "# Ahora que viste los valores, llamas al m√©todo de tu objeto 'obj'\n",
    "# para que realice la conversi√≥n en su dataframe interno (obj.df)\n",
    "\n",
    "obj.forzar_conversion_numerica(columnas_sospechosas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a78d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIA', 'HORA', 'EE Planta / Hl', 'EE Elaboracion / Hl', 'EE Bodega / Hl', 'EE Cocina / Hl', 'EE Envasado / Hl', 'EE Servicios / Hl', 'EE Frio / Hl', 'EE Aire / Hl', 'EE CO2 / Hl', 'EE Agua / Hl', 'Agua Planta / Hl', 'Agua Elab / Hl', 'Agua Bodega / Hl', 'Agua Cocina / Hl', 'Agua Envas / Hl', 'Agua Planta de Agua/Hl', 'Produccion Agua / Hl', 'ET Planta / Hl', 'ET Elab/Hl', 'ET Bodega/Hl', 'ET Cocina/Hl', 'ET Envasado/Hl', 'Aire Planta / Hl', 'Aire Elaboracion / Hl', 'Aire Cocina / Hl', 'Aire Bodega / Hl', 'Aire Envasado / Hl', 'CO 2 / Hl', 'Hl de Mosto', 'Cocimientos Diarios', 'Planta (Kw)', 'Elaboracion (Kw)', 'Bodega (Kw)', 'Cocina (Kw)', 'Envasado (Kw)', 'Servicios (Kw)', 'Aire (Kw)', 'Calderas (Kw)', 'Efluentes (Kw)', 'Frio (Kw)', 'Prod Agua (Kw)', 'KW CO2', 'KW Enfluentes Coc', 'KW Enfluente Efl', 'KW Enfluentes Hidr', 'Kw Compresores Aire', 'Agua Planta (Hl)', 'Agua Elaboracion (Hl)', 'Agua Bodega (Hl)', 'Agua Cocina (Hl)', 'Agua Dilucion (Hl)', 'Agua Envasado (Hl)', 'Agua Servicios (Hl)', 'Planta de agua (Hl)', 'Produccion (Hl)', 'Agua CO2', 'Agua Efluentes', 'Agua Planta CO2', 'Temp Tq Intermedio', 'Conversion Kg/Mj', 'Gas Planta (Mj)', 'Vapor Elaboracion (Kg)', 'Vapor Cocina (Kg)', 'Vapor Envasado (Kg)', 'Vapor Servicio (Kg)', 'ET Elaboracion (Mj)', 'ET Envasado (Mj)', 'ET Servicios (Mj)', 'Medicion Gas Planta (M3)', 'Aire Producido (M3)', 'Aire Planta (M3)', 'Aire Elaboracion (m3)', 'Aire Envasado (M3)', 'Aire Servicios (M3)', 'Tot L3. L4 y Planta de CO2', 'Tot A40/240/50/60/Centec/Filtro', 'Tot  A130/330/430', 'Tot  Trasiego']\n",
      "(658797, 80)\n"
     ]
    }
   ],
   "source": [
    "print(list(obj.df.columns))\n",
    "print(obj.df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd8134",
   "metadata": {},
   "source": [
    "# Feature enginering de preprocesamiento\n",
    "Agregar nuevas variables que consideremos necesarias:\n",
    "- temperaturas diarias en la zona\n",
    "- variables temporales (estacion, dia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a6516b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openmeteo-requests\n",
    "%pip install requests-cache retry-requests numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64bfadbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00+00:00</td>\n",
       "      <td>13.594001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:00:00+00:00</td>\n",
       "      <td>10.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:00:00+00:00</td>\n",
       "      <td>9.344001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:00:00+00:00</td>\n",
       "      <td>7.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:00:00+00:00</td>\n",
       "      <td>7.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2023-12-31 19:00:00+00:00</td>\n",
       "      <td>12.594001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2023-12-31 20:00:00+00:00</td>\n",
       "      <td>13.344001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2023-12-31 21:00:00+00:00</td>\n",
       "      <td>13.344001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2023-12-31 22:00:00+00:00</td>\n",
       "      <td>13.294001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2023-12-31 23:00:00+00:00</td>\n",
       "      <td>13.494000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  temperature_2m\n",
       "0     2020-01-01 00:00:00+00:00       13.594001\n",
       "1     2020-01-01 01:00:00+00:00       10.694000\n",
       "2     2020-01-01 02:00:00+00:00        9.344001\n",
       "3     2020-01-01 03:00:00+00:00        7.894000\n",
       "4     2020-01-01 04:00:00+00:00        7.694000\n",
       "...                         ...             ...\n",
       "35059 2023-12-31 19:00:00+00:00       12.594001\n",
       "35060 2023-12-31 20:00:00+00:00       13.344001\n",
       "35061 2023-12-31 21:00:00+00:00       13.344001\n",
       "35062 2023-12-31 22:00:00+00:00       13.294001\n",
       "35063 2023-12-31 23:00:00+00:00       13.494000\n",
       "\n",
       "[35064 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# El cambio clave est√° en esta URL\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "    \"latitude\": 32.5672,\n",
    "    \"longitude\": -116.6251,\n",
    "    \"start_date\": \"2020-01-01\",\n",
    "    \"end_date\": \"2023-12-31\",\n",
    "    \"hourly\": \"temperature_2m\"\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "    start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "    end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "    freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "    inclusive=\"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "\n",
    "\n",
    "hourly_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0951962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columna 'ESTACION' agregada correctamente.\n"
     ]
    }
   ],
   "source": [
    "obj.redondear_horarios()\n",
    "obj.add_estacion()\n",
    "obj.add_temp_y_dia(hourly_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2887471",
   "metadata": {},
   "source": [
    "# Construyendo el dataset final\n",
    "Nuestro datasets tiene datos temporales pero por tupla (fecha hora) hay multiples observaciones. Queremos promediar estas para ver si son series temporales pero queremos verificar que las variables categoricas no sean distintas entre los instantes (fecha hora). Entonces Vamos a verificar esto mismo con la salvedad de que la diferencia sean entre nan y otro valor.\n",
    "\n",
    "La unica columna que genera una diferencia es Ultimo Dato del Dia. La obviamos y juntamos los datos usando las medias de las observaciones (no promedios para no ser sensibles a atipicos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cd07964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verificando consistencia de categor√≠as binarias antes de agrupar ---\n",
      "--- Iniciando verificaci√≥n de consistencia en tuplas (DIA, HORA) duplicadas ---\n",
      "\n",
      "Revisados 29701 grupos ('DIA', 'HORA') √∫nicos.\n",
      "Se encontraron 29700 grupos con filas duplicadas.\n",
      "‚úÖ ¬°√âxito! Todas las tuplas (DIA, HORA) duplicadas son consistentes en las columnas binarias.\n",
      "   (Se puede proceder con la agrupaci√≥n de forma segura)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# --- EJECUCI√ìN DEL SCRIPT DE VERIFICACI√ìN DE CONSISTENCIA ---\n",
    "# ======================================================\n",
    "\n",
    "# Asumimos que 'obj' es una instancia de tu clase 'make_dataset'\n",
    "# y que 'cols_binarias_numericas' es la lista de columnas que ya definiste.\n",
    "\n",
    "print(\"\\n--- Verificando consistencia de categor√≠as binarias antes de agrupar ---\")\n",
    "\n",
    "# 3. Ejecutar el m√©todo\n",
    "# (Llamamos al m√©todo desde el objeto 'obj')\n",
    "hay_problemas, grupos_problematicos = obj.verificar_consistencia_binaria(cols_binarias_numericas)\n",
    "\n",
    "# 4. Mostrar los resultados si los hay\n",
    "if hay_problemas:\n",
    "    print(\"\\n--- Detalle de Inconsistencias ---\")\n",
    "    \n",
    "    # Mostramos los primeros 5 grupos problem√°ticos como ejemplo\n",
    "    count = 0\n",
    "    for (dia, hora), columnas in grupos_problematicos.items():\n",
    "        if count < 5:\n",
    "            print(f\"En ({dia}, {hora}), las siguientes columnas var√≠an: {columnas}\")\n",
    "        count += 1\n",
    "    if count >= 5:\n",
    "        print(f\"... y {len(grupos_problematicos) - 5} m√°s.\")\n",
    "else:\n",
    "    print(\"   (Se puede proceder con la agrupaci√≥n de forma segura)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c70eca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando agrupaci√≥n por (DIA) usando la media (Promedio Diario) ---\n",
      "Agrupando 79 columnas por (DIA)...\n",
      "\n",
      "‚úÖ Agrupaci√≥n Diaria completada.\n",
      "   Filas reducidas de 658797 a 1190.\n",
      "\n",
      "Dimensiones finales del obj.df: (1190, 80)\n",
      "         DIA  EE Planta / Hl  EE Elaboracion / Hl  EE Bodega / Hl  \\\n",
      "0 2020-07-01      198.550355            14.944947       21.898397   \n",
      "1 2020-07-02        7.332630             0.648068        0.725053   \n",
      "2 2020-07-03        8.198708             0.756267        0.776495   \n",
      "3 2020-07-04        4.550079             0.417806        0.355416   \n",
      "4 2020-07-05        5.627349             0.491721        0.441206   \n",
      "\n",
      "   EE Cocina / Hl  EE Envasado / Hl  EE Servicios / Hl  EE Frio / Hl  \\\n",
      "0        0.000000          1.905891         173.274186    104.618915   \n",
      "1        0.522019          2.414053           5.041896      3.106550   \n",
      "2        0.225265          1.869990           5.344107      3.559398   \n",
      "3        0.243756          1.305393           2.621910      1.545884   \n",
      "4        0.244048          1.386874           3.471051      2.179957   \n",
      "\n",
      "   EE Aire / Hl  EE CO2 / Hl  ...  Aire Producido (M3)  Aire Planta (M3)  \\\n",
      "0     31.220775    12.091938  ...             15935.52        12383.2384   \n",
      "1      1.004567     0.391853  ...             25539.52        22052.7272   \n",
      "2      1.103764     0.133667  ...             31479.12        22713.4784   \n",
      "3      0.657071     0.034204  ...             34717.56        26562.2908   \n",
      "4      0.757975     0.086481  ...             33215.52        24532.4500   \n",
      "\n",
      "   Aire Elaboracion (m3)  Aire Envasado (M3)  Aire Servicios (M3)  \\\n",
      "0             11792.8248            590.4136            3552.2816   \n",
      "1             15669.0588           6911.4764            2958.9848   \n",
      "2             17912.6108           6328.4836            7238.0256   \n",
      "3             19504.5308           9907.2320            5305.7972   \n",
      "4             17504.5100           9568.9640            6142.0460   \n",
      "\n",
      "   Tot L3. L4 y Planta de CO2  Tot A40/240/50/60/Centec/Filtro  \\\n",
      "0                         NaN                              NaN   \n",
      "1                         NaN                              NaN   \n",
      "2                         NaN                              NaN   \n",
      "3                         NaN                              NaN   \n",
      "4                         NaN                              NaN   \n",
      "\n",
      "   Tot  A130/330/430  Tot  Trasiego  ESTACION  \n",
      "0                NaN            NaN    Verano  \n",
      "1                NaN            NaN    Verano  \n",
      "2                NaN            NaN    Verano  \n",
      "3                NaN            NaN    Verano  \n",
      "4                NaN            NaN    Verano  \n",
      "\n",
      "[5 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# --- EJECUCI√ìN DEL SCRIPT DE AGRUPACI√ìN ---\n",
    "# ======================================================\n",
    "# Asumimos que 'obj' es una instancia existente de tu clase 'make_dataset'\n",
    "\n",
    "# 1. Definir las columnas que NO se deben promediar\n",
    "\n",
    "lista_texto=[]\n",
    "\n",
    "\n",
    "# 3. Llamar al m√©todo de la clase\n",
    "# Esto modificar√° el DataFrame 'obj.df' internamente\n",
    "obj.agrupar_por_media(cols_binarias_numericas, lista_texto)\n",
    "\n",
    "# 4. Verificar el resultado\n",
    "print(f\"\\nDimensiones finales del obj.df: {obj.df.shape}\")\n",
    "print(obj.df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8deb0f",
   "metadata": {},
   "source": [
    "Ya construido este dataset final, lo almacenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b46e0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta creada en: dataset_FE\n",
      "‚úÖ DataFrame guardado exitosamente en: dataset_FE\\dataset_limpio.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_folder = 'dataset_FE'\n",
    "output_filename = 'dataset_limpio.csv' \n",
    "\n",
    "# Esto crea la ruta correcta: 'dataset_FE/dataset_limpio.csv'\n",
    "full_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "# --- 2. Crear la carpeta si no existe ---\n",
    "try:\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Carpeta creada en: {output_folder}\")\n",
    "    else:\n",
    "        print(f\"La carpeta '{output_folder}' ya existe.\")\n",
    "\n",
    "    # --- 3. Guardar el DataFrame ---\n",
    "    \n",
    "    # Usamos index=False para evitar que se guarde el √≠ndice de pandas\n",
    "    # como una columna extra 'Unnamed: 0' en el CSV.\n",
    "    # Usamos sep=';' y decimal='.' para buena compatibilidad con Excel.\n",
    "    \n",
    "    obj.df.to_csv(full_path, index=False, sep=';', decimal='.') \n",
    "    \n",
    "    print(f\"‚úÖ DataFrame guardado exitosamente en: {full_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al guardar el archivo: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
