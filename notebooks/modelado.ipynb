{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4294b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc2c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame cargado exitosamente desde: ../data/processed\\dataset_final.csv\n",
      "Dimensiones: (1184, 304)\n",
      "\n",
      "Primeras 5 filas:\n",
      "          DIA  EE Planta / Hl  EE Elaboracion / Hl  EE Bodega / Hl  \\\n",
      "0  2020-07-01      642.727209            47.145349       69.023256   \n",
      "1  2020-07-02        7.767254             0.769609        0.798838   \n",
      "2  2020-07-03        8.801205             0.862593        0.835762   \n",
      "3  2020-07-04        5.175639             0.439225        0.371077   \n",
      "4  2020-07-05        7.924665             0.802365        0.717787   \n",
      "\n",
      "   EE Cocina / Hl  EE Envasado / Hl  EE Linea 2 / Hl  EE Linea 3 / Hl  \\\n",
      "0        0.000000         13.813953        14.578784         0.000000   \n",
      "1        0.319229          2.358593         4.158962         1.506838   \n",
      "2        0.260924          1.985462        39.076667         1.448962   \n",
      "3        0.258048          1.442114         4.348182         1.355238   \n",
      "4        0.301592          1.664726         5.125920         2.704348   \n",
      "\n",
      "   EE Linea 4 / Hl  EE Servicios / Hl  ...  Tot A40/240/50/60/Centec/Filtro  \\\n",
      "0         0.000000         554.604651  ...                              NaN   \n",
      "1         1.521823           5.429388  ...                              NaN   \n",
      "2         1.500923           5.703346  ...                              NaN   \n",
      "3         1.536507           3.058399  ...                              NaN   \n",
      "4         1.471990           5.094301  ...                              NaN   \n",
      "\n",
      "   Tot  A130/330/430  Anio  Mes  Dia  Dia_semana  Temperatura_amb  \\\n",
      "0                NaN  2020    7    1   Miercoles        18.919000   \n",
      "1                NaN  2020    7    2      Jueves        21.044000   \n",
      "2                NaN  2020    7    3     Viernes        24.950249   \n",
      "3                NaN  2020    7    4      Sabado        25.531500   \n",
      "4                NaN  2020    7    5     Domingo        27.466917   \n",
      "\n",
      "   Tarifa_electrica  estacion  Frio (Kw) tomorrow  \n",
      "0      1.183769e+08    Verano             23954.0  \n",
      "1      2.372603e+08    Verano             28268.0  \n",
      "2      2.801249e+08    Verano             24246.0  \n",
      "3      2.854641e+08    Verano             29885.0  \n",
      "4      2.947832e+08    Verano             24449.0  \n",
      "\n",
      "[5 rows x 304 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = '../data/processed'\n",
    "filename = 'dataset_final.csv'\n",
    "file_path = os.path.join(folder, filename)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=',',\n",
    "        decimal='.'\n",
    "    )\n",
    "    df = df.sort_values(by='DIA', ignore_index=True)\n",
    "    print(f\"✅ DataFrame cargado exitosamente desde: {file_path}\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: No se encontró el archivo en la ruta: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c72d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dataset: (1184, 304)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1️⃣ Cargar dataset definitivo\n",
    "ruta_csv = \"../data/processed/X_test_preproc.csv\"\n",
    "X_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/X_train_preproc.csv\"\n",
    "X_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_test.csv\"\n",
    "y_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_train.csv\"\n",
    "y_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "\n",
    "\n",
    "print(\"Shape dataset:\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f71077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_float(\"max_features\", 0.1, 1.0, log=True)\n",
    "\n",
    "    # Modelo\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f44e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True) # L1\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True) # L2\n",
    "\n",
    "    # Modelo\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a1e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 20, 150)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True) # L1\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True) # L2\n",
    "\n",
    "    # Modelo\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        num_leaves=num_leaves,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6375cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_elasticnet_mae(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0) \n",
    "\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f499",
   "metadata": {},
   "source": [
    "# 1. Crear los estudios (todos buscan minimizar el MAE)\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_lgbm_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# 2. Ejecutar las optimizaciones\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=50)\n",
    "\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando LightGBM (MAE)...\")\n",
    "study_lgbm_mae.optimize(objective_lgbm_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=40)\n",
    "\n",
    "# 3. Ver los mejores parámetros\n",
    "print(f\"Mejor RF (MAE): {study_rf_mae.best_value} con params {study_rf_mae.best_params}\")\n",
    "print(f\"Mejor XGB (MAE): {study_xgb_mae.best_value} con params {study_xgb_mae.best_params}\")\n",
    "print(f\"Mejor LGBM (MAE): {study_lgbm_mae.best_value} con params {study_lgbm_mae.best_params}\")\n",
    "print(f\"Mejor ElasticNet (MAE): {study_elasticnet_mae.best_value} con params {study_elasticnet_mae.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e82ff312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_experiment(model_name: str, \n",
    "                   model_object: object, \n",
    "                   study: optuna.study.Study, \n",
    "                   X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con los mejores parámetros de un estudio de Optuna,\n",
    "    calcula métricas y las guarda en un CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Definir la ruta del log ---\n",
    "    log_dir = \"results\"\n",
    "    log_file = os.path.join(log_dir, \"experiment_logs.csv\")\n",
    "    os.makedirs(log_dir, exist_ok=True) # Crea la carpeta si no existe\n",
    "\n",
    "    # --- 2. Obtener mejores parámetros y valor del estudio ---\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value # El MAE de CV que encontró Optuna\n",
    "    \n",
    "    # --- 3. Instanciar y entrenar el modelo final ---\n",
    "    # Manejo especial para modelos lineales que necesitan Pipeline\n",
    "    if \"pipeline\" in model_name.lower():\n",
    "        # El pipeline ya está definido en la función objective, \n",
    "        # aquí solo re-creamos el mejor.\n",
    "        if model_name == \"Pipeline_ElasticNet\":\n",
    "             # Re-crea el pipeline con los mejores params\n",
    "             model =ElasticNet(\n",
    "                    alpha=best_params.get('alpha'), \n",
    "                    l1_ratio=best_params.get('l1_ratio'),\n",
    "                    random_state=42, \n",
    "                    max_iter=2000\n",
    "                )\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # Para modelos no-pipeline (RF, XGB, LGBM)\n",
    "        model = model_object(**best_params, random_state=42, n_jobs=-1)\n",
    "        # Ajustes para modelos que no les gusta 'n_jobs' o necesitan 'verbose'\n",
    "        if isinstance(model, (xgb.XGBRegressor, lgb.LGBMRegressor)):\n",
    "            model.set_params(verbosity=0) if isinstance(model, xgb.XGBRegressor) else None\n",
    "            model.set_params(verbose=-1) if isinstance(model, lgb.LGBMRegressor) else None\n",
    "\n",
    "    # Entrenar el modelo final con todos los datos de train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- 4. Calcular Métricas (Train y Test) ---\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_name': model_name,\n",
    "        'optuna_cv_mae': best_value, # Métrica optimizada\n",
    "        'mae_train': mean_absolute_error(y_train, y_pred_train),\n",
    "        'mae_test': mean_absolute_error(y_test, y_pred_test),\n",
    "        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'r2_train': r2_score(y_train, y_pred_train),\n",
    "        'r2_test': r2_score(y_test, y_pred_test),\n",
    "        'hyperparameters': json.dumps(best_params) # Guardar params como string JSON\n",
    "    }\n",
    "    \n",
    "    # --- 5. Escribir en el CSV ---\n",
    "    file_exists = os.path.isfile(log_file)\n",
    "    \n",
    "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=metrics.keys())\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader() # Escribir cabecera solo si el archivo es nuevo\n",
    "            \n",
    "        writer.writerow(metrics)\n",
    "        \n",
    "    print(f\"Resultados de '{model_name}' guardados en {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 14:08:16,022] A new study created in memory with name: no-name-d8384e7c-437c-4df9-8ca0-3d8a4cb36f44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizando Random Forest (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 14:08:26,220] Trial 0 finished with value: 9132.613083411647 and parameters: {'n_estimators': 450, 'max_depth': 31, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 0.24636527742907252}. Best is trial 0 with value: 9132.613083411647.\n",
      "[I 2025-11-15 14:08:31,246] Trial 1 finished with value: 10588.828433003382 and parameters: {'n_estimators': 150, 'max_depth': 13, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 0.16449629918650788}. Best is trial 0 with value: 9132.613083411647.\n",
      "[I 2025-11-15 14:08:33,713] Trial 2 finished with value: 9468.753411903132 and parameters: {'n_estimators': 250, 'max_depth': 9, 'min_samples_split': 12, 'min_samples_leaf': 7, 'max_features': 0.26613661575955216}. Best is trial 0 with value: 9132.613083411647.\n",
      "[I 2025-11-15 14:08:45,080] Trial 3 finished with value: 9567.22105795698 and parameters: {'n_estimators': 800, 'max_depth': 38, 'min_samples_split': 16, 'min_samples_leaf': 10, 'max_features': 0.48636033347909013}. Best is trial 0 with value: 9132.613083411647.\n",
      "[I 2025-11-15 14:09:06,355] Trial 4 finished with value: 10227.679684465937 and parameters: {'n_estimators': 1000, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 0.8995133588730944}. Best is trial 0 with value: 9132.613083411647.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "[I 2025-11-15 14:09:08,077] A new study created in memory with name: no-name-2e1c3e51-6f3a-444e-88f3-fb1cb32079c0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'RandomForest' guardados en results\\experiment_logs.csv\n",
      "Optimizando ElasticNet (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 14:09:08,360] Trial 0 finished with value: 15111.201374012448 and parameters: {'alpha': 0.2634886355954175, 'l1_ratio': 0.3029687571186219}. Best is trial 0 with value: 15111.201374012448.\n",
      "[I 2025-11-15 14:09:08,621] Trial 1 finished with value: 14657.6732133451 and parameters: {'alpha': 0.22297663436474036, 'l1_ratio': 0.06139370902019958}. Best is trial 1 with value: 14657.6732133451.\n",
      "[I 2025-11-15 14:09:08,841] Trial 2 finished with value: 28490.652469442204 and parameters: {'alpha': 0.001934597825688664, 'l1_ratio': 0.7755079057073868}. Best is trial 1 with value: 14657.6732133451.\n",
      "[I 2025-11-15 14:09:09,079] Trial 3 finished with value: 23187.69058010725 and parameters: {'alpha': 0.02464041082788516, 'l1_ratio': 0.5214786898245859}. Best is trial 1 with value: 14657.6732133451.\n",
      "[I 2025-11-15 14:09:09,371] Trial 4 finished with value: 23911.072788696325 and parameters: {'alpha': 0.01605227391965591, 'l1_ratio': 0.5497746836917935}. Best is trial 1 with value: 14657.6732133451.\n",
      "[I 2025-11-15 14:09:09,504] Trial 5 finished with value: 10923.490038532438 and parameters: {'alpha': 2.083044052627999, 'l1_ratio': 0.5680514722033234}. Best is trial 5 with value: 10923.490038532438.\n",
      "[I 2025-11-15 14:09:09,748] Trial 6 finished with value: 23873.729196485 and parameters: {'alpha': 0.025330431415320157, 'l1_ratio': 0.7077522484172226}. Best is trial 5 with value: 10923.490038532438.\n",
      "[I 2025-11-15 14:09:10,004] Trial 7 finished with value: 27978.812619307817 and parameters: {'alpha': 0.0016908536185769871, 'l1_ratio': 0.6544883239073586}. Best is trial 5 with value: 10923.490038532438.\n",
      "[I 2025-11-15 14:09:10,294] Trial 8 finished with value: 25207.32156228256 and parameters: {'alpha': 0.003698436178601936, 'l1_ratio': 0.16156731278165293}. Best is trial 5 with value: 10923.490038532438.\n",
      "[I 2025-11-15 14:09:10,373] Trial 9 finished with value: 9363.906866710851 and parameters: {'alpha': 9.414982946257572, 'l1_ratio': 0.5633279839247181}. Best is trial 9 with value: 9363.906866710851.\n",
      "[I 2025-11-15 14:09:10,568] Trial 10 finished with value: 12242.72459958416 and parameters: {'alpha': 9.031065090763336, 'l1_ratio': 0.9483259069976957}. Best is trial 9 with value: 9363.906866710851.\n",
      "[I 2025-11-15 14:09:10,641] Trial 11 finished with value: 9264.886324916759 and parameters: {'alpha': 7.613930640630067, 'l1_ratio': 0.3787356907026803}. Best is trial 11 with value: 9264.886324916759.\n",
      "[I 2025-11-15 14:09:10,683] Trial 12 finished with value: 9147.94102647521 and parameters: {'alpha': 9.000928923348551, 'l1_ratio': 0.38027320025763617}. Best is trial 12 with value: 9147.94102647521.\n",
      "[I 2025-11-15 14:09:10,864] Trial 13 finished with value: 11210.102693882485 and parameters: {'alpha': 1.1388760489978103, 'l1_ratio': 0.33234214708460896}. Best is trial 12 with value: 9147.94102647521.\n",
      "[I 2025-11-15 14:09:10,951] Trial 14 finished with value: 10428.272863737937 and parameters: {'alpha': 1.9436800007448656, 'l1_ratio': 0.34534634057242564}. Best is trial 12 with value: 9147.94102647521.\n",
      "[I 2025-11-15 14:09:10,971] A new study created in memory with name: no-name-e2a2735f-2327-4d3e-a0d1-ae88a81dbd0b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'Pipeline_ElasticNet' guardados en results\\experiment_logs.csv\n",
      "Optimizando XGBoost (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 14:09:41,896] Trial 0 finished with value: 12220.52314453125 and parameters: {'n_estimators': 900, 'learning_rate': 0.1904493667018738, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 3.898470237330194e-06, 'reg_lambda': 0.13988062844855642}. Best is trial 0 with value: 12220.52314453125.\n",
      "[I 2025-11-15 14:11:21,957] Trial 1 finished with value: 11612.04189453125 and parameters: {'n_estimators': 900, 'learning_rate': 0.04906289733080212, 'max_depth': 10, 'subsample': 0.9, 'colsample_bytree': 1.0, 'reg_alpha': 0.00014699320990922487, 'reg_lambda': 5.1661014543360604e-08}. Best is trial 1 with value: 11612.04189453125.\n",
      "[I 2025-11-15 14:11:27,838] Trial 2 finished with value: 11607.59765625 and parameters: {'n_estimators': 600, 'learning_rate': 0.015195406702123685, 'max_depth': 4, 'subsample': 0.6, 'colsample_bytree': 0.8, 'reg_alpha': 0.001463912294169605, 'reg_lambda': 0.000573071664279915}. Best is trial 2 with value: 11607.59765625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'XGBoost' guardados en results\\experiment_logs.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Ejecutar el estudio (ejemplo con Random Forest) ---\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_rf_mae' está definida)\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=15) \n",
    "\n",
    "# --- 2. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"RandomForest\",\n",
    "    model_object=RandomForestRegressor, # Pasa la clase del modelo\n",
    "    study=study_rf_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 5. Ejecutar el estudio (ejemplo con ElasticNet) ---\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_elasticnet_mae' está definida)\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=30)\n",
    "\n",
    "# --- 6. Registrar el resultado ---\n",
    "# Nota: 'model_object' es None porque se maneja dentro del 'if'\n",
    "log_experiment(\n",
    "    model_name=\"Pipeline_ElasticNet\",\n",
    "    model_object=None, \n",
    "    study=study_elasticnet_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 3. Ejecutar el estudio (ejemplo con XGBoost) ---\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_xgb_mae' está definida)\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=5)\n",
    "\n",
    "# --- 4. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"XGBoost\",\n",
    "    model_object=xgb.XGBRegressor, # Pasa la clase del modelo\n",
    "    study=study_xgb_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# Repetir para LightGBM..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MostoElMostro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
