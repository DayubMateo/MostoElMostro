{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder = '../data/processed'\n",
    "filename = 'dataset_final.csv'\n",
    "file_path = os.path.join(folder, filename)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=',',\n",
    "        decimal='.'\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ DataFrame cargado exitosamente desde: {file_path}\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: No se encontró el archivo en la ruta: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1️⃣ Cargar dataset definitivo\n",
    "ruta_csv = \"../data/processed/X_test_preproc.csv\"\n",
    "X_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/X_train_preproc.csv\"\n",
    "X_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_test.csv\"\n",
    "y_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_train.csv\"\n",
    "y_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "\n",
    "\n",
    "print(\"Shape dataset:\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f71077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_float(\"max_features\", 0.1, 1.0, log=True)\n",
    "\n",
    "    # Modelo\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f44e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True) # L1\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True) # L2\n",
    "\n",
    "    # Modelo\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 20, 150)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True) # L1\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True) # L2\n",
    "\n",
    "    # Modelo\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        num_leaves=num_leaves,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_elasticnet_mae(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0) \n",
    "\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f499",
   "metadata": {},
   "source": [
    "# 1. Crear los estudios (todos buscan minimizar el MAE)\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_lgbm_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# 2. Ejecutar las optimizaciones\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=50)\n",
    "\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando LightGBM (MAE)...\")\n",
    "study_lgbm_mae.optimize(objective_lgbm_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=40)\n",
    "\n",
    "# 3. Ver los mejores parámetros\n",
    "print(f\"Mejor RF (MAE): {study_rf_mae.best_value} con params {study_rf_mae.best_params}\")\n",
    "print(f\"Mejor XGB (MAE): {study_xgb_mae.best_value} con params {study_xgb_mae.best_params}\")\n",
    "print(f\"Mejor LGBM (MAE): {study_lgbm_mae.best_value} con params {study_lgbm_mae.best_params}\")\n",
    "print(f\"Mejor ElasticNet (MAE): {study_elasticnet_mae.best_value} con params {study_elasticnet_mae.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ff312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_experiment(model_name: str, \n",
    "                   model_object: object, \n",
    "                   study: optuna.study.Study, \n",
    "                   X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con los mejores parámetros de un estudio de Optuna,\n",
    "    calcula métricas y las guarda en un CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Definir la ruta del log ---\n",
    "    log_dir = \"results\"\n",
    "    log_file = os.path.join(log_dir, \"experiment_logs.csv\")\n",
    "    os.makedirs(log_dir, exist_ok=True) # Crea la carpeta si no existe\n",
    "\n",
    "    # --- 2. Obtener mejores parámetros y valor del estudio ---\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value # El MAE de CV que encontró Optuna\n",
    "    \n",
    "    # --- 3. Instanciar y entrenar el modelo final ---\n",
    "    # Manejo especial para modelos lineales que necesitan Pipeline\n",
    "    if \"pipeline\" in model_name.lower():\n",
    "        # El pipeline ya está definido en la función objective, \n",
    "        # aquí solo re-creamos el mejor.\n",
    "        if model_name == \"Pipeline_ElasticNet\":\n",
    "             # Re-crea el pipeline con los mejores params\n",
    "             model =ElasticNet(\n",
    "                    alpha=best_params.get('alpha'), \n",
    "                    l1_ratio=best_params.get('l1_ratio'),\n",
    "                    random_state=42, \n",
    "                    max_iter=2000\n",
    "                )\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # Para modelos no-pipeline (RF, XGB, LGBM)\n",
    "        model = model_object(**best_params, random_state=42, n_jobs=-1)\n",
    "        # Ajustes para modelos que no les gusta 'n_jobs' o necesitan 'verbose'\n",
    "        if isinstance(model, (xgb.XGBRegressor, lgb.LGBMRegressor)):\n",
    "            model.set_params(verbosity=0) if isinstance(model, xgb.XGBRegressor) else None\n",
    "            model.set_params(verbose=-1) if isinstance(model, lgb.LGBMRegressor) else None\n",
    "\n",
    "    # Entrenar el modelo final con todos los datos de train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- 4. Calcular Métricas (Train y Test) ---\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_name': model_name,\n",
    "        'optuna_cv_mae': best_value, # Métrica optimizada\n",
    "        'mae_train': mean_absolute_error(y_train, y_pred_train),\n",
    "        'mae_test': mean_absolute_error(y_test, y_pred_test),\n",
    "        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'r2_train': r2_score(y_train, y_pred_train),\n",
    "        'r2_test': r2_score(y_test, y_pred_test),\n",
    "        'hyperparameters': json.dumps(best_params) # Guardar params como string JSON\n",
    "    }\n",
    "    \n",
    "    # --- 5. Escribir en el CSV ---\n",
    "    file_exists = os.path.isfile(log_file)\n",
    "    \n",
    "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=metrics.keys())\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader() # Escribir cabecera solo si el archivo es nuevo\n",
    "            \n",
    "        writer.writerow(metrics)\n",
    "        \n",
    "    print(f\"Resultados de '{model_name}' guardados en {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Ejecutar el estudio (ejemplo con Random Forest) ---\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_rf_mae' está definida)\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=50) \n",
    "\n",
    "# --- 2. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"RandomForest\",\n",
    "    model_object=RandomForestRegressor, # Pasa la clase del modelo\n",
    "    study=study_rf_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 3. Ejecutar el estudio (ejemplo con XGBoost) ---\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_xgb_mae' está definida)\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=50)\n",
    "\n",
    "# --- 4. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"XGBoost\",\n",
    "    model_object=xgb.XGBRegressor, # Pasa la clase del modelo\n",
    "    study=study_xgb_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 5. Ejecutar el estudio (ejemplo con ElasticNet) ---\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_elasticnet_mae' está definida)\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=40)\n",
    "\n",
    "# --- 6. Registrar el resultado ---\n",
    "# Nota: 'model_object' es None porque se maneja dentro del 'if'\n",
    "log_experiment(\n",
    "    model_name=\"Pipeline_ElasticNet\",\n",
    "    model_object=None, \n",
    "    study=study_elasticnet_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# Repetir para LightGBM..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MostoElMostro (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
