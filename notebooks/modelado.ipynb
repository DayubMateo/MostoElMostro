{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4294b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc2c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame cargado exitosamente desde: ../data/processed\\dataset_final.csv\n",
      "Dimensiones: (1182, 431)\n",
      "\n",
      "Primeras 5 filas:\n",
      "          DIA  EE Planta / Hl  EE Elaboracion / Hl  EE Bodega / Hl  \\\n",
      "0  2020-07-01      642.727209            47.145349       69.023256   \n",
      "1  2020-07-02        7.767254             0.769609        0.798838   \n",
      "2  2020-07-03        8.801205             0.862593        0.835762   \n",
      "3  2020-07-04        5.175639             0.439225        0.371077   \n",
      "4  2020-07-05        7.924665             0.802365        0.717787   \n",
      "\n",
      "   EE Cocina / Hl  EE Envasado / Hl  EE Linea 2 / Hl  EE Linea 3 / Hl  \\\n",
      "0        0.000000         13.813953        14.578784         0.000000   \n",
      "1        0.319229          2.358593         4.158962         1.506838   \n",
      "2        0.260924          1.985462        39.076667         1.448962   \n",
      "3        0.258048          1.442114         4.348182         1.355238   \n",
      "4        0.301592          1.664726         5.125920         2.704348   \n",
      "\n",
      "   EE Linea 4 / Hl  EE Servicios / Hl  ...  diferencia_ET Servicios / Hl  \\\n",
      "0         0.000000         554.604651  ...                           NaN   \n",
      "1         1.521823           5.429388  ...                           NaN   \n",
      "2         1.500923           5.703346  ...                           NaN   \n",
      "3         1.536507           3.058399  ...                           NaN   \n",
      "4         1.471990           5.094301  ...                           NaN   \n",
      "\n",
      "   diferencia_Aire Elaboracion / Hl  diferencia_Aire Cocina / Hl  \\\n",
      "0                               NaN                          NaN   \n",
      "1                               NaN                          NaN   \n",
      "2                               NaN                          NaN   \n",
      "3                               NaN                          NaN   \n",
      "4                               NaN                          NaN   \n",
      "\n",
      "   diferencia_Aire Bodega / Hl  diferencia_Aire Envasado / Hl  \\\n",
      "0                          NaN                            NaN   \n",
      "1                          NaN                            NaN   \n",
      "2                          NaN                            NaN   \n",
      "3                          NaN                            NaN   \n",
      "4                          NaN                            NaN   \n",
      "\n",
      "   diferencia_Aire L2 / Hl  diferencia_Aire L3 / Hl  diferencia_Aire L4 / Hl  \\\n",
      "0                      NaN                      NaN                      NaN   \n",
      "1                      NaN                      NaN                      NaN   \n",
      "2                      NaN                      NaN                      NaN   \n",
      "3                      NaN                      NaN                      NaN   \n",
      "4                      NaN                      NaN                      NaN   \n",
      "\n",
      "   diferencia_Aire Servicios / Hl  Temperatura_amb  \n",
      "0                             NaN        18.919000  \n",
      "1                             NaN        21.044000  \n",
      "2                             NaN        24.950249  \n",
      "3                             NaN        25.531500  \n",
      "4                             NaN        27.466917  \n",
      "\n",
      "[5 rows x 431 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = '../data/processed'\n",
    "filename = 'dataset_final.csv'\n",
    "file_path = os.path.join(folder, filename)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=',',\n",
    "        decimal='.'\n",
    "    )\n",
    "    df = df.sort_values(by='DIA', ignore_index=True)\n",
    "    print(f\"✅ DataFrame cargado exitosamente desde: {file_path}\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: No se encontró el archivo en la ruta: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c72d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (827, 40) y_train: (827,)\n",
      "X_test: (355, 40) y_test: (355,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1️⃣ Cargar dataset definitivo\n",
    "ruta_csv = \"../data/processed/X_test_preproc.csv\"\n",
    "X_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/X_train_preproc.csv\"\n",
    "X_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_test.csv\"\n",
    "y_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_train.csv\"\n",
    "y_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "\n",
    "y_train = y_train.iloc[:, 0].values.ravel()\n",
    "y_test  = y_test.iloc[:, 0].values.ravel()\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f71077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf_mae(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_float(\"max_features\", 0.1, 1.0, log=True)\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        n_jobs=-1,\n",
    "#        random_state=42\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    fold_mae = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        fold_mae.append(mae)\n",
    "\n",
    "        # Reportar al pruner\n",
    "        trial.report(mae, fold_idx)\n",
    "\n",
    "        # Podar si es necesario\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f44e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_mae(trial):\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        subsample=trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1),\n",
    "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1),\n",
    "        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        n_jobs=-1,\n",
    "#        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    fold_mae = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        fold_mae.append(mae)\n",
    "\n",
    "        trial.report(mae, fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a1e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm_mae(trial):\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        num_leaves=trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        subsample=trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1),\n",
    "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1),\n",
    "        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        n_jobs=-1,\n",
    "    #    random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    fold_mae = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        fold_mae.append(mae)\n",
    "\n",
    "        trial.report(mae, fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6375cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_elasticnet_mae(trial):\n",
    "    model = ElasticNet(\n",
    "        alpha=trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "        l1_ratio=trial.suggest_float(\"l1_ratio\", 0.0, 1.0),\n",
    "#        random_state=42,\n",
    "        max_iter=2000\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    fold_mae = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        fold_mae.append(mae)\n",
    "\n",
    "        trial.report(mae, fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f499",
   "metadata": {},
   "source": [
    "# 1. Crear los estudios (todos buscan minimizar el MAE)\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_lgbm_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# 2. Ejecutar las optimizaciones\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=50)\n",
    "\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando LightGBM (MAE)...\")\n",
    "study_lgbm_mae.optimize(objective_lgbm_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=40)\n",
    "\n",
    "# 3. Ver los mejores parámetros\n",
    "print(f\"Mejor RF (MAE): {study_rf_mae.best_value} con params {study_rf_mae.best_params}\")\n",
    "print(f\"Mejor XGB (MAE): {study_xgb_mae.best_value} con params {study_xgb_mae.best_params}\")\n",
    "print(f\"Mejor LGBM (MAE): {study_lgbm_mae.best_value} con params {study_lgbm_mae.best_params}\")\n",
    "print(f\"Mejor ElasticNet (MAE): {study_elasticnet_mae.best_value} con params {study_elasticnet_mae.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82ff312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_experiment(model_name: str, \n",
    "                   model_object: object, \n",
    "                   study: optuna.study.Study, \n",
    "                   X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con los mejores parámetros de un estudio de Optuna,\n",
    "    calcula métricas y las guarda en un CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Definir la ruta del log ---\n",
    "    log_dir = \"results\"\n",
    "    log_file = os.path.join(log_dir, \"experiment_logs.csv\")\n",
    "    os.makedirs(log_dir, exist_ok=True) # Crea la carpeta si no existe\n",
    "\n",
    "    # --- 2. Obtener mejores parámetros y valor del estudio ---\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value # El MAE de CV que encontró Optuna\n",
    "    \n",
    "    # --- 3. Instanciar y entrenar el modelo final ---\n",
    "    # Manejo especial para modelos lineales que necesitan Pipeline\n",
    "    if \"pipeline\" in model_name.lower():\n",
    "        # El pipeline ya está definido en la función objective, \n",
    "        # aquí solo re-creamos el mejor.\n",
    "        if model_name == \"Pipeline_ElasticNet\":\n",
    "             # Re-crea el pipeline con los mejores params\n",
    "             model =ElasticNet(\n",
    "                    alpha=best_params.get('alpha'), \n",
    "                    l1_ratio=best_params.get('l1_ratio'),\n",
    "                    random_state=42, \n",
    "                    max_iter=2000\n",
    "                )\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # Para modelos no-pipeline (RF, XGB, LGBM)\n",
    "        model = model_object(**best_params, random_state=42, n_jobs=-1)\n",
    "        # Ajustes para modelos que no les gusta 'n_jobs' o necesitan 'verbose'\n",
    "        if isinstance(model, (xgb.XGBRegressor, lgb.LGBMRegressor)):\n",
    "            model.set_params(verbosity=0) if isinstance(model, xgb.XGBRegressor) else None\n",
    "            model.set_params(verbose=-1) if isinstance(model, lgb.LGBMRegressor) else None\n",
    "\n",
    "    # Entrenar el modelo final con todos los datos de train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- 4. Calcular Métricas (Train y Test) ---\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_name': model_name,\n",
    "        'optuna_cv_mae': best_value, # Métrica optimizada\n",
    "        'mae_train': mean_absolute_error(y_train, y_pred_train),\n",
    "        'mae_test': mean_absolute_error(y_test, y_pred_test),\n",
    "        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'r2_train': r2_score(y_train, y_pred_train),\n",
    "        'r2_test': r2_score(y_test, y_pred_test),\n",
    "        'hyperparameters': json.dumps(best_params) # Guardar params como string JSON\n",
    "    }\n",
    "    \n",
    "    # --- 5. Escribir en el CSV ---\n",
    "    file_exists = os.path.isfile(log_file)\n",
    "    \n",
    "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=metrics.keys())\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader() # Escribir cabecera solo si el archivo es nuevo\n",
    "            \n",
    "        writer.writerow(metrics)\n",
    "        \n",
    "    print(f\"Resultados de '{model_name}' guardados en {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32cb1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 22:16:54,092] A new study created in memory with name: no-name-7b0f1b15-46ee-4fc2-b53d-87adfac18198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizando Random Forest (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 22:17:07,726] Trial 0 finished with value: 8609.681754570061 and parameters: {'n_estimators': 950, 'max_depth': 31, 'min_samples_split': 15, 'min_samples_leaf': 4, 'max_features': 0.6429094230294136}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:17:11,338] Trial 1 finished with value: 10696.95554843017 and parameters: {'n_estimators': 300, 'max_depth': 14, 'min_samples_split': 13, 'min_samples_leaf': 1, 'max_features': 0.42637112195174404}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:17:18,676] Trial 2 finished with value: 9320.631833031874 and parameters: {'n_estimators': 900, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 0.28776440410031556}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:17:22,217] Trial 3 finished with value: 10233.663927924137 and parameters: {'n_estimators': 350, 'max_depth': 22, 'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 0.39480553404775537}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:17:27,623] Trial 4 finished with value: 12728.436277880843 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.9979893046231776}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:17:29,380] Trial 5 finished with value: 9996.089640168399 and parameters: {'n_estimators': 150, 'max_depth': 28, 'min_samples_split': 6, 'min_samples_leaf': 7, 'max_features': 0.3518567698892163}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:17:46,406] Trial 6 finished with value: 9961.572968398512 and parameters: {'n_estimators': 950, 'max_depth': 30, 'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 0.8415916306261549}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:17:51,722] Trial 7 finished with value: 9136.28122296804 and parameters: {'n_estimators': 550, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 0.3981144468767584}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:17:58,153] Trial 8 finished with value: 9438.313427104269 and parameters: {'n_estimators': 800, 'max_depth': 31, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_features': 0.13021414455966826}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:18:03,370] Trial 9 finished with value: 9606.003233471349 and parameters: {'n_estimators': 600, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 0.10059796523097674}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:18:12,208] Trial 10 finished with value: 9106.541882319065 and parameters: {'n_estimators': 700, 'max_depth': 47, 'min_samples_split': 19, 'min_samples_leaf': 4, 'max_features': 0.6659045644308563}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:18:20,853] Trial 11 finished with value: 9937.980234007437 and parameters: {'n_estimators': 700, 'max_depth': 48, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': 0.621371734572726}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:18:32,949] Trial 12 finished with value: 8696.705461898102 and parameters: {'n_estimators': 1000, 'max_depth': 43, 'min_samples_split': 17, 'min_samples_leaf': 4, 'max_features': 0.6299429234728369}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:18:42,008] Trial 13 finished with value: 9544.072449378626 and parameters: {'n_estimators': 1000, 'max_depth': 39, 'min_samples_split': 15, 'min_samples_leaf': 4, 'max_features': 0.22281945965032815}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:18:53,332] Trial 14 finished with value: 9351.003675465587 and parameters: {'n_estimators': 850, 'max_depth': 39, 'min_samples_split': 13, 'min_samples_leaf': 3, 'max_features': 0.5653727162398371}. Best is trial 0 with value: 8609.681754570061.\n",
      "[I 2025-11-15 22:18:56,898] A new study created in memory with name: no-name-718d27c1-7c19-4407-99b1-7f754c497bf6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'RandomForest' guardados en results\\experiment_logs.csv\n",
      "Optimizando ElasticNet (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.607e+11, tolerance: 7.756e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.562e+09, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.371e+10, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.120e+10, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 22:18:57,254] Trial 0 finished with value: 22457.387359924916 and parameters: {'alpha': 0.0028659184397149423, 'l1_ratio': 0.5917452304116181}. Best is trial 0 with value: 22457.387359924916.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.304e+11, tolerance: 7.756e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.701e+10, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.220e+12, tolerance: 7.753e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.330e+08, tolerance: 4.314e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.366e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 22:18:57,599] Trial 1 finished with value: 22500.952313759328 and parameters: {'alpha': 0.0037738364022517276, 'l1_ratio': 0.9302437809141573}. Best is trial 0 with value: 22457.387359924916.\n",
      "[I 2025-11-15 22:18:57,667] Trial 2 finished with value: 16243.815949426504 and parameters: {'alpha': 0.04537821261091706, 'l1_ratio': 0.440298997730876}. Best is trial 2 with value: 16243.815949426504.\n",
      "[I 2025-11-15 22:18:57,692] Trial 3 finished with value: 9245.173837109072 and parameters: {'alpha': 3.889655972289259, 'l1_ratio': 0.34105743845962844}. Best is trial 3 with value: 9245.173837109072.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.422e+09, tolerance: 4.244e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.927e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.208e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+12, tolerance: 7.758e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 22:18:58,006] Trial 4 finished with value: 22615.500329291055 and parameters: {'alpha': 0.0015261132690837975, 'l1_ratio': 0.7044064761526627}. Best is trial 3 with value: 9245.173837109072.\n",
      "[I 2025-11-15 22:18:58,040] Trial 5 finished with value: 8906.327410071652 and parameters: {'alpha': 8.060047755373581, 'l1_ratio': 0.4985655097624263}. Best is trial 5 with value: 8906.327410071652.\n",
      "[I 2025-11-15 22:18:58,082] Trial 6 finished with value: 12446.97363331055 and parameters: {'alpha': 0.8685557426522426, 'l1_ratio': 0.800240983102915}. Best is trial 5 with value: 8906.327410071652.\n",
      "[I 2025-11-15 22:18:58,112] Trial 7 finished with value: 8881.361346147653 and parameters: {'alpha': 7.956537074029717, 'l1_ratio': 0.31131612455648205}. Best is trial 7 with value: 8881.361346147653.\n",
      "[I 2025-11-15 22:18:58,164] Trial 8 finished with value: 12636.821545930547 and parameters: {'alpha': 0.20602574931538917, 'l1_ratio': 0.13488230866735496}. Best is trial 7 with value: 8881.361346147653.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.414e+11, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.696e+09, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.073e+06, tolerance: 4.442e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.596e+11, tolerance: 7.753e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.525e+12, tolerance: 7.756e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 22:18:58,607] Trial 9 finished with value: 22954.89021817036 and parameters: {'alpha': 0.027972370226665398, 'l1_ratio': 0.9943959259740901}. Best is trial 7 with value: 8881.361346147653.\n",
      "[I 2025-11-15 22:18:58,653] Trial 10 finished with value: 9973.978195682987 and parameters: {'alpha': 1.2139522627838952, 'l1_ratio': 0.0012024635665873196}. Best is trial 7 with value: 8881.361346147653.\n",
      "[I 2025-11-15 22:18:58,692] Trial 11 finished with value: 8753.734263156288 and parameters: {'alpha': 8.958207992452607, 'l1_ratio': 0.31248150642528644}. Best is trial 11 with value: 8753.734263156288.\n",
      "[I 2025-11-15 22:18:58,739] Trial 12 finished with value: 9887.87726664534 and parameters: {'alpha': 1.5837347683794156, 'l1_ratio': 0.2691181667676923}. Best is trial 11 with value: 8753.734263156288.\n",
      "[I 2025-11-15 22:18:58,777] Trial 13 finished with value: 8553.15937600608 and parameters: {'alpha': 9.225634476993521, 'l1_ratio': 0.24304329595794255}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:58,834] Trial 14 finished with value: 11825.80995108172 and parameters: {'alpha': 0.2945573591763569, 'l1_ratio': 0.15821992819211167}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:58,877] Trial 15 finished with value: 9311.631895634644 and parameters: {'alpha': 2.417522545797391, 'l1_ratio': 0.17847519156215452}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:58,926] Trial 16 finished with value: 10703.958469630179 and parameters: {'alpha': 0.4942155989100783, 'l1_ratio': 0.013415994112176977}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,000] Trial 17 finished with value: 14437.366656372109 and parameters: {'alpha': 0.10849600350887745, 'l1_ratio': 0.4401742451501502}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,050] Trial 18 finished with value: 9400.111813267325 and parameters: {'alpha': 5.124332742161386, 'l1_ratio': 0.5951371792696829}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,189] Trial 19 finished with value: 19335.013811528257 and parameters: {'alpha': 0.009693235038190695, 'l1_ratio': 0.2707131020888237}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,219] Trial 20 finished with value: 8604.597759590713 and parameters: {'alpha': 9.731892439475747, 'l1_ratio': 0.3623646313158625}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,256] Trial 21 finished with value: 8752.085259586045 and parameters: {'alpha': 9.05715026717888, 'l1_ratio': 0.38774070227741586}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,298] Trial 22 finished with value: 9542.28558558967 and parameters: {'alpha': 2.6873314721004147, 'l1_ratio': 0.39785396904766357}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,346] Trial 23 finished with value: 9521.701798338703 and parameters: {'alpha': 3.564835325273164, 'l1_ratio': 0.5704358930319902}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,392] Trial 24 finished with value: 10261.013640574009 and parameters: {'alpha': 0.7323046280867282, 'l1_ratio': 0.10817775736814061}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,444] Trial 25 finished with value: 8687.22154963786 and parameters: {'alpha': 9.711118699996272, 'l1_ratio': 0.2188701561201034}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,506] Trial 26 finished with value: 9494.343445110979 and parameters: {'alpha': 1.9344773732959095, 'l1_ratio': 0.22688450232033297}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,582] Trial 27 finished with value: 8825.11323719014 and parameters: {'alpha': 4.804252161696693, 'l1_ratio': 0.05880368781922912}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,686] Trial 28 finished with value: 10895.317713485585 and parameters: {'alpha': 0.5686441641293921, 'l1_ratio': 0.2296689586154528}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,754] Trial 29 finished with value: 10534.321206276782 and parameters: {'alpha': 1.5133292105128762, 'l1_ratio': 0.5029414236697487}. Best is trial 13 with value: 8553.15937600608.\n",
      "[I 2025-11-15 22:18:59,770] A new study created in memory with name: no-name-d9238576-ec2c-4440-a948-13dbbe7f9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'Pipeline_ElasticNet' guardados en results\\experiment_logs.csv\n",
      "Optimizando XGBoost (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 22:19:29,498] Trial 0 finished with value: 7711.7099886525275 and parameters: {'n_estimators': 1100, 'learning_rate': 0.013569881064365569, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 0.0028139825980812517, 'reg_lambda': 0.010863909346049673}. Best is trial 0 with value: 7711.7099886525275.\n",
      "[I 2025-11-15 22:19:41,727] Trial 1 finished with value: 6728.7522631232805 and parameters: {'n_estimators': 300, 'learning_rate': 0.16011962646696293, 'max_depth': 8, 'subsample': 0.9, 'colsample_bytree': 1.0, 'reg_alpha': 0.0006167519087054743, 'reg_lambda': 0.046358226488068074}. Best is trial 1 with value: 6728.7522631232805.\n",
      "[I 2025-11-15 22:20:02,498] Trial 2 finished with value: 7261.4000157388555 and parameters: {'n_estimators': 1900, 'learning_rate': 0.21473838894854733, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.9, 'reg_alpha': 1.68787552223528e-08, 'reg_lambda': 5.764662117550487e-06}. Best is trial 1 with value: 6728.7522631232805.\n",
      "[I 2025-11-15 22:20:14,589] Trial 3 finished with value: 6616.502438701302 and parameters: {'n_estimators': 200, 'learning_rate': 0.0860398964216472, 'max_depth': 10, 'subsample': 0.6, 'colsample_bytree': 1.0, 'reg_alpha': 1.3081383386733693e-08, 'reg_lambda': 0.13563783178024025}. Best is trial 3 with value: 6616.502438701302.\n",
      "[I 2025-11-15 22:20:38,050] Trial 4 finished with value: 8974.747844328307 and parameters: {'n_estimators': 2000, 'learning_rate': 0.11011816579034221, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.6, 'reg_alpha': 0.008033587698251899, 'reg_lambda': 0.0007194076255100253}. Best is trial 3 with value: 6616.502438701302.\n",
      "[I 2025-11-15 22:20:40,612] A new study created in memory with name: no-name-bc12742b-03f3-4290-8e53-ad06aee8c9af\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'XGBoost' guardados en results\\experiment_logs.csv\n",
      "Optimizando LightGBM (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 22:20:43,293] Trial 0 finished with value: 11992.553661762571 and parameters: {'n_estimators': 400, 'learning_rate': 0.01623149457250011, 'num_leaves': 49, 'subsample': 1.0, 'colsample_bytree': 0.6, 'reg_alpha': 0.004661959308924353, 'reg_lambda': 1.378288527860829e-08}. Best is trial 0 with value: 11992.553661762571.\n",
      "[I 2025-11-15 22:20:52,787] Trial 1 finished with value: 23877.47563864687 and parameters: {'n_estimators': 1200, 'learning_rate': 0.1383291892324728, 'num_leaves': 82, 'subsample': 0.8, 'colsample_bytree': 0.6, 'reg_alpha': 7.840656950449376e-05, 'reg_lambda': 0.0002121899179641522}. Best is trial 0 with value: 11992.553661762571.\n",
      "[I 2025-11-15 22:21:10,101] Trial 2 finished with value: 23460.504428012988 and parameters: {'n_estimators': 1500, 'learning_rate': 0.23676387294537973, 'num_leaves': 114, 'subsample': 0.8, 'colsample_bytree': 0.7, 'reg_alpha': 9.09427672617356e-06, 'reg_lambda': 2.6195795879727737e-08}. Best is trial 0 with value: 11992.553661762571.\n",
      "[I 2025-11-15 22:21:10,952] Trial 3 finished with value: 10199.975574561702 and parameters: {'n_estimators': 100, 'learning_rate': 0.03383974828181722, 'num_leaves': 79, 'subsample': 1.0, 'colsample_bytree': 0.6, 'reg_alpha': 0.00010803334714897675, 'reg_lambda': 0.005156501918679643}. Best is trial 3 with value: 10199.975574561702.\n",
      "[I 2025-11-15 22:21:25,496] Trial 4 finished with value: 12338.972719933077 and parameters: {'n_estimators': 1400, 'learning_rate': 0.022781704426643123, 'num_leaves': 146, 'subsample': 0.7, 'colsample_bytree': 0.9, 'reg_alpha': 3.6504393822077113e-07, 'reg_lambda': 3.132821626683783e-05}. Best is trial 3 with value: 10199.975574561702.\n",
      "[I 2025-11-15 22:21:47,822] Trial 5 finished with value: 24546.208721470557 and parameters: {'n_estimators': 2000, 'learning_rate': 0.09259696653954022, 'num_leaves': 30, 'subsample': 0.6, 'colsample_bytree': 0.7, 'reg_alpha': 2.326692845252324e-08, 'reg_lambda': 1.1828645996969582e-05}. Best is trial 3 with value: 10199.975574561702.\n",
      "[I 2025-11-15 22:21:58,713] Trial 6 finished with value: 22044.598274697288 and parameters: {'n_estimators': 600, 'learning_rate': 0.14978990786795762, 'num_leaves': 105, 'subsample': 0.6, 'colsample_bytree': 0.9, 'reg_alpha': 2.1489712035049104e-07, 'reg_lambda': 0.0002708290181071485}. Best is trial 3 with value: 10199.975574561702.\n",
      "[I 2025-11-15 22:22:14,787] Trial 7 finished with value: 26565.010195457136 and parameters: {'n_estimators': 1400, 'learning_rate': 0.08051532966059968, 'num_leaves': 130, 'subsample': 1.0, 'colsample_bytree': 0.6, 'reg_alpha': 1.4142759797700414e-08, 'reg_lambda': 0.007645434345479544}. Best is trial 3 with value: 10199.975574561702.\n",
      "[I 2025-11-15 22:22:38,934] Trial 8 finished with value: 20910.82198581722 and parameters: {'n_estimators': 1800, 'learning_rate': 0.1294254750903701, 'num_leaves': 87, 'subsample': 0.8, 'colsample_bytree': 0.9, 'reg_alpha': 0.00018588759083353252, 'reg_lambda': 0.008403232227138053}. Best is trial 3 with value: 10199.975574561702.\n",
      "[I 2025-11-15 22:22:49,812] Trial 9 finished with value: 14602.12595348761 and parameters: {'n_estimators': 1300, 'learning_rate': 0.012543774992163642, 'num_leaves': 120, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 3.488157977813247e-05, 'reg_lambda': 6.830266251053289e-06}. Best is trial 3 with value: 10199.975574561702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'LightGBM' guardados en results\\experiment_logs.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Ejecutar el estudio (ejemplo con Random Forest) ---\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "# (Aquí asumimos que 'objective_rf_mae' está definida)\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=15) \n",
    "\n",
    "# --- 2. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"RandomForest\",\n",
    "    model_object=RandomForestRegressor, # Pasa la clase del modelo\n",
    "    study=study_rf_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 5. Ejecutar el estudio (ejemplo con ElasticNet) ---\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "# (Aquí asumimos que 'objective_elasticnet_mae' está definida)\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=30)\n",
    "\n",
    "# --- 6. Registrar el resultado ---\n",
    "# Nota: 'model_object' es None porque se maneja dentro del 'if'\n",
    "log_experiment(\n",
    "    model_name=\"Pipeline_ElasticNet\",\n",
    "    model_object=None, \n",
    "    study=study_elasticnet_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 3. Ejecutar el estudio (ejemplo con XGBoost) ---\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "# (Aquí asumimos que 'objective_xgb_mae' está definida)\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=5)\n",
    "\n",
    "# --- 4. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"XGBoost\",\n",
    "    model_object=xgb.XGBRegressor, # Pasa la clase del modelo\n",
    "    study=study_xgb_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 5. Ejecutar el estudio (ejemplo con LGBM) ---\n",
    "print(\"Optimizando LightGBM (MAE)...\")\n",
    "study_lgbm_mae = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_lgbm_mae.optimize(objective_lgbm_mae, n_trials=10)\n",
    "\n",
    "# --- 6. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"LightGBM\",\n",
    "    model_object=lgb.LGBMRegressor,  # Pasamos la clase del modelo\n",
    "    study=study_lgbm_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MostoElMostro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
