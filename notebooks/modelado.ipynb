{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4294b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fc2c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame cargado exitosamente desde: ../data/processed\\dataset_final.csv\n",
      "Dimensiones: (1184, 318)\n",
      "\n",
      "Primeras 5 filas:\n",
      "          DIA  EE Planta / Hl  EE Elaboracion / Hl  EE Bodega / Hl  \\\n",
      "0  2020-07-01      642.727209            47.145349       69.023256   \n",
      "1  2020-07-02        7.767254             0.769609        0.798838   \n",
      "2  2020-07-03        8.801205             0.862593        0.835762   \n",
      "3  2020-07-04        5.175639             0.439225        0.371077   \n",
      "4  2020-07-05        7.924665             0.802365        0.717787   \n",
      "\n",
      "   EE Cocina / Hl  EE Envasado / Hl  EE Linea 2 / Hl  EE Linea 3 / Hl  \\\n",
      "0        0.000000         13.813953        14.578784         0.000000   \n",
      "1        0.319229          2.358593         4.158962         1.506838   \n",
      "2        0.260924          1.985462        39.076667         1.448962   \n",
      "3        0.258048          1.442114         4.348182         1.355238   \n",
      "4        0.301592          1.664726         5.125920         2.704348   \n",
      "\n",
      "   EE Linea 4 / Hl  EE Servicios / Hl  ...  Tot  A130/330/430  \\\n",
      "0         0.000000         554.604651  ...                0.0   \n",
      "1         1.521823           5.429388  ...                0.0   \n",
      "2         1.500923           5.703346  ...                0.0   \n",
      "3         1.536507           3.058399  ...                0.0   \n",
      "4         1.471990           5.094301  ...                0.0   \n",
      "\n",
      "   Tot L3, L4 y Planta de CO2  Anio  Mes  Dia  Dia_semana  Temperatura_amb  \\\n",
      "0                         0.0  2020    7    1   Miercoles        18.919000   \n",
      "1                         0.0  2020    7    2      Jueves        21.044000   \n",
      "2                         0.0  2020    7    3     Viernes        24.950249   \n",
      "3                         0.0  2020    7    4      Sabado        25.531500   \n",
      "4                         0.0  2020    7    5     Domingo        27.466917   \n",
      "\n",
      "   Tarifa_electrica  estacion  Frio (Kw) tomorrow  \n",
      "0      1.390135e+08    Verano             23954.0  \n",
      "1      2.830998e+08    Verano             28268.0  \n",
      "2      3.276745e+08    Verano             24246.0  \n",
      "3      3.342809e+08    Verano             29885.0  \n",
      "4      3.460976e+08    Verano             24449.0  \n",
      "\n",
      "[5 rows x 318 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = '../data/processed'\n",
    "filename = 'dataset_final.csv'\n",
    "file_path = os.path.join(folder, filename)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=',',\n",
    "        decimal='.'\n",
    "    )\n",
    "    df = df.sort_values(by='DIA', ignore_index=True)\n",
    "    print(f\"✅ DataFrame cargado exitosamente desde: {file_path}\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: No se encontró el archivo en la ruta: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c72d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dataset: (1184, 318)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1️⃣ Cargar dataset definitivo\n",
    "ruta_csv = \"../data/processed/X_test_preproc.csv\"\n",
    "X_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/X_train_preproc.csv\"\n",
    "X_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_test.csv\"\n",
    "y_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_train.csv\"\n",
    "y_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "\n",
    "\n",
    "print(\"Shape dataset:\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6f71077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_float(\"max_features\", 0.1, 1.0, log=True)\n",
    "\n",
    "    # Modelo\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86f44e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True) # L1\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True) # L2\n",
    "\n",
    "    # Modelo\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2a1e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 20, 150)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True) # L1\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True) # L2\n",
    "\n",
    "    # Modelo\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        num_leaves=num_leaves,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6375cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_elasticnet_mae(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0) \n",
    "\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f499",
   "metadata": {},
   "source": [
    "# 1. Crear los estudios (todos buscan minimizar el MAE)\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_lgbm_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# 2. Ejecutar las optimizaciones\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=50)\n",
    "\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando LightGBM (MAE)...\")\n",
    "study_lgbm_mae.optimize(objective_lgbm_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=40)\n",
    "\n",
    "# 3. Ver los mejores parámetros\n",
    "print(f\"Mejor RF (MAE): {study_rf_mae.best_value} con params {study_rf_mae.best_params}\")\n",
    "print(f\"Mejor XGB (MAE): {study_xgb_mae.best_value} con params {study_xgb_mae.best_params}\")\n",
    "print(f\"Mejor LGBM (MAE): {study_lgbm_mae.best_value} con params {study_lgbm_mae.best_params}\")\n",
    "print(f\"Mejor ElasticNet (MAE): {study_elasticnet_mae.best_value} con params {study_elasticnet_mae.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e82ff312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_experiment(model_name: str, \n",
    "                   model_object: object, \n",
    "                   study: optuna.study.Study, \n",
    "                   X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con los mejores parámetros de un estudio de Optuna,\n",
    "    calcula métricas y las guarda en un CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Definir la ruta del log ---\n",
    "    log_dir = \"results\"\n",
    "    log_file = os.path.join(log_dir, \"experiment_logs.csv\")\n",
    "    os.makedirs(log_dir, exist_ok=True) # Crea la carpeta si no existe\n",
    "\n",
    "    # --- 2. Obtener mejores parámetros y valor del estudio ---\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value # El MAE de CV que encontró Optuna\n",
    "    \n",
    "    # --- 3. Instanciar y entrenar el modelo final ---\n",
    "    # Manejo especial para modelos lineales que necesitan Pipeline\n",
    "    if \"pipeline\" in model_name.lower():\n",
    "        # El pipeline ya está definido en la función objective, \n",
    "        # aquí solo re-creamos el mejor.\n",
    "        if model_name == \"Pipeline_ElasticNet\":\n",
    "             # Re-crea el pipeline con los mejores params\n",
    "             model =ElasticNet(\n",
    "                    alpha=best_params.get('alpha'), \n",
    "                    l1_ratio=best_params.get('l1_ratio'),\n",
    "                    random_state=42, \n",
    "                    max_iter=2000\n",
    "                )\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # Para modelos no-pipeline (RF, XGB, LGBM)\n",
    "        model = model_object(**best_params, random_state=42, n_jobs=-1)\n",
    "        # Ajustes para modelos que no les gusta 'n_jobs' o necesitan 'verbose'\n",
    "        if isinstance(model, (xgb.XGBRegressor, lgb.LGBMRegressor)):\n",
    "            model.set_params(verbosity=0) if isinstance(model, xgb.XGBRegressor) else None\n",
    "            model.set_params(verbose=-1) if isinstance(model, lgb.LGBMRegressor) else None\n",
    "\n",
    "    # Entrenar el modelo final con todos los datos de train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- 4. Calcular Métricas (Train y Test) ---\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_name': model_name,\n",
    "        'optuna_cv_mae': best_value, # Métrica optimizada\n",
    "        'mae_train': mean_absolute_error(y_train, y_pred_train),\n",
    "        'mae_test': mean_absolute_error(y_test, y_pred_test),\n",
    "        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'r2_train': r2_score(y_train, y_pred_train),\n",
    "        'r2_test': r2_score(y_test, y_pred_test),\n",
    "        'hyperparameters': json.dumps(best_params) # Guardar params como string JSON\n",
    "    }\n",
    "    \n",
    "    # --- 5. Escribir en el CSV ---\n",
    "    file_exists = os.path.isfile(log_file)\n",
    "    \n",
    "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=metrics.keys())\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader() # Escribir cabecera solo si el archivo es nuevo\n",
    "            \n",
    "        writer.writerow(metrics)\n",
    "        \n",
    "    print(f\"Resultados de '{model_name}' guardados en {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32cb1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-14 00:10:42,016] A new study created in memory with name: no-name-f95cf453-0cb0-4476-a939-89b9cdf832b5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizando Random Forest (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-14 00:11:01,651] Trial 0 finished with value: 17428.642738449078 and parameters: {'n_estimators': 850, 'max_depth': 34, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_features': 0.4978273328021799}. Best is trial 0 with value: 17428.642738449078.\n",
      "[I 2025-11-14 00:11:07,817] Trial 1 finished with value: 16546.03450566535 and parameters: {'n_estimators': 400, 'max_depth': 17, 'min_samples_split': 20, 'min_samples_leaf': 7, 'max_features': 0.1251761747417061}. Best is trial 1 with value: 16546.03450566535.\n",
      "[I 2025-11-14 00:11:16,359] Trial 2 finished with value: 24388.831155100776 and parameters: {'n_estimators': 250, 'max_depth': 48, 'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 0.8053409431472845}. Best is trial 1 with value: 16546.03450566535.\n",
      "[I 2025-11-14 00:11:27,994] Trial 3 finished with value: 17519.25058739371 and parameters: {'n_estimators': 700, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': 0.5248464855114723}. Best is trial 1 with value: 16546.03450566535.\n",
      "[I 2025-11-14 00:11:39,819] Trial 4 finished with value: 21754.713156721464 and parameters: {'n_estimators': 600, 'max_depth': 49, 'min_samples_split': 13, 'min_samples_leaf': 3, 'max_features': 0.533207097696879}. Best is trial 1 with value: 16546.03450566535.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "[I 2025-11-14 00:11:40,780] A new study created in memory with name: no-name-c98794bb-61ce-4fef-9f02-566deef2c86b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'RandomForest' guardados en results\\experiment_logs.csv\n",
      "Optimizando ElasticNet (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-14 00:11:41,008] Trial 0 finished with value: 29673.26067130121 and parameters: {'alpha': 3.736575908012251, 'l1_ratio': 0.9299441026188173}. Best is trial 0 with value: 29673.26067130121.\n",
      "[I 2025-11-14 00:11:41,268] Trial 1 finished with value: 47934.16206861458 and parameters: {'alpha': 0.03414077287173811, 'l1_ratio': 0.5822533373079968}. Best is trial 0 with value: 29673.26067130121.\n",
      "[I 2025-11-14 00:11:41,341] Trial 2 finished with value: 21813.77747398184 and parameters: {'alpha': 4.797641287903446, 'l1_ratio': 0.746514929260232}. Best is trial 2 with value: 21813.77747398184.\n",
      "[I 2025-11-14 00:11:41,425] Trial 3 finished with value: 21970.334858358343 and parameters: {'alpha': 1.8797962454913668, 'l1_ratio': 0.3789419013726981}. Best is trial 2 with value: 21813.77747398184.\n",
      "[I 2025-11-14 00:11:41,671] Trial 4 finished with value: 31747.46713534498 and parameters: {'alpha': 0.6614100077299491, 'l1_ratio': 0.7131067584742958}. Best is trial 2 with value: 21813.77747398184.\n",
      "[I 2025-11-14 00:11:41,734] Trial 5 finished with value: 20683.860745814825 and parameters: {'alpha': 3.9280074888445236, 'l1_ratio': 0.5688088954093695}. Best is trial 5 with value: 20683.860745814825.\n",
      "[I 2025-11-14 00:11:42,162] Trial 6 finished with value: 38001.46428522689 and parameters: {'alpha': 0.3401458607266572, 'l1_ratio': 0.7828940967499685}. Best is trial 5 with value: 20683.860745814825.\n",
      "[I 2025-11-14 00:11:42,434] Trial 7 finished with value: 30276.393790313152 and parameters: {'alpha': 1.2151138354186115, 'l1_ratio': 0.8039303448115983}. Best is trial 5 with value: 20683.860745814825.\n",
      "[I 2025-11-14 00:11:42,660] Trial 8 finished with value: 60000.616443895036 and parameters: {'alpha': 0.002209108872649735, 'l1_ratio': 0.531040587282813}. Best is trial 5 with value: 20683.860745814825.\n",
      "[I 2025-11-14 00:11:42,912] Trial 9 finished with value: 33396.0529563592 and parameters: {'alpha': 2.598112054277206, 'l1_ratio': 0.94310240993616}. Best is trial 5 with value: 20683.860745814825.\n",
      "[I 2025-11-14 00:11:43,142] Trial 10 finished with value: 43994.99899100904 and parameters: {'alpha': 0.030083696935872678, 'l1_ratio': 0.05306835872003268}. Best is trial 5 with value: 20683.860745814825.\n",
      "[I 2025-11-14 00:11:43,184] Trial 11 finished with value: 18701.847698621128 and parameters: {'alpha': 5.645691756825165, 'l1_ratio': 0.32893224906888974}. Best is trial 11 with value: 18701.847698621128.\n",
      "[I 2025-11-14 00:11:43,225] Trial 12 finished with value: 17820.798210281853 and parameters: {'alpha': 8.730793633918292, 'l1_ratio': 0.3004141175696032}. Best is trial 12 with value: 17820.798210281853.\n",
      "[I 2025-11-14 00:11:43,283] Trial 13 finished with value: 17741.59016147718 and parameters: {'alpha': 8.595060571106133, 'l1_ratio': 0.256938202795587}. Best is trial 13 with value: 17741.59016147718.\n",
      "[I 2025-11-14 00:11:43,535] Trial 14 finished with value: 33378.87310907707 and parameters: {'alpha': 0.17219661792577617, 'l1_ratio': 0.13794021505854018}. Best is trial 13 with value: 17741.59016147718.\n",
      "[I 2025-11-14 00:11:43,559] A new study created in memory with name: no-name-2f6eea8e-eb1e-4175-be8d-83f405463568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'Pipeline_ElasticNet' guardados en results\\experiment_logs.csv\n",
      "Optimizando XGBoost (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-14 00:11:44,980] Trial 0 finished with value: 23428.6951171875 and parameters: {'n_estimators': 100, 'learning_rate': 0.011929171005084598, 'max_depth': 3, 'subsample': 1.0, 'colsample_bytree': 0.9, 'reg_alpha': 0.18139946218762099, 'reg_lambda': 4.025743214275714e-06}. Best is trial 0 with value: 23428.6951171875.\n",
      "[I 2025-11-14 00:11:57,250] Trial 1 finished with value: 27748.446484375 and parameters: {'n_estimators': 1600, 'learning_rate': 0.14404364698484182, 'max_depth': 3, 'subsample': 0.9, 'colsample_bytree': 1.0, 'reg_alpha': 0.0006420922378346894, 'reg_lambda': 0.07213551205306755}. Best is trial 0 with value: 23428.6951171875.\n",
      "[I 2025-11-14 00:12:32,350] Trial 2 finished with value: 30034.24609375 and parameters: {'n_estimators': 1100, 'learning_rate': 0.026685149691001345, 'max_depth': 6, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 0.1798618438534292, 'reg_lambda': 0.7311884877338454}. Best is trial 0 with value: 23428.6951171875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'XGBoost' guardados en results\\experiment_logs.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Ejecutar el estudio (ejemplo con Random Forest) ---\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_rf_mae' está definida)\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=5) \n",
    "\n",
    "# --- 2. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"RandomForest\",\n",
    "    model_object=RandomForestRegressor, # Pasa la clase del modelo\n",
    "    study=study_rf_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 5. Ejecutar el estudio (ejemplo con ElasticNet) ---\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_elasticnet_mae' está definida)\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=15)\n",
    "\n",
    "# --- 6. Registrar el resultado ---\n",
    "# Nota: 'model_object' es None porque se maneja dentro del 'if'\n",
    "log_experiment(\n",
    "    model_name=\"Pipeline_ElasticNet\",\n",
    "    model_object=None, \n",
    "    study=study_elasticnet_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 3. Ejecutar el estudio (ejemplo con XGBoost) ---\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_xgb_mae' está definida)\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=3)\n",
    "\n",
    "# --- 4. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"XGBoost\",\n",
    "    model_object=xgb.XGBRegressor, # Pasa la clase del modelo\n",
    "    study=study_xgb_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# Repetir para LightGBM..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MostoElMostro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
