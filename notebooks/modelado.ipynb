{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4294b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame cargado exitosamente desde: ../data/processed\\dataset_final.csv\n",
      "Dimensiones: (1183, 107)\n",
      "\n",
      "Primeras 5 filas:\n",
      "          DIA  Frio (Kw)  Hl de Mosto  Sala Maq (Kw)  Servicios (Kw)  \\\n",
      "0  2020-07-01    14325.0          0.0        17080.0         23848.0   \n",
      "1  2020-07-02    23954.0       2907.0        27216.0         38033.0   \n",
      "2  2020-07-03    28268.0       4829.0        31386.0         42565.5   \n",
      "3  2020-07-04    24246.0       7828.0        28070.0         39650.0   \n",
      "4  2020-07-05    29885.0       6406.0        33463.0         45385.0   \n",
      "\n",
      "   KW Gral Planta  Planta (Kw)  Agua Planta (Hl)  Planta de agua (Hl)  \\\n",
      "0         59058.0     27637.27           10280.0             11241.40   \n",
      "1        131184.0     54409.81           13970.0             22107.77   \n",
      "2        136078.0     65685.59           36300.0             46955.43   \n",
      "3        139714.0     67098.54           40120.0             51124.18   \n",
      "4        146862.0     70600.64           38940.0             49146.08   \n",
      "\n",
      "   KW Trafo 10  ...  Agua Calderas  Hl Cerveza L4  Anio  Mes  Dia  Dia_semana  \\\n",
      "0      6046.25  ...            0.0           24.0  2020    7    1   Miercoles   \n",
      "1     10108.13  ...            0.0        10402.0  2020    7    2      Jueves   \n",
      "2      9177.75  ...            0.0        10832.0  2020    7    3     Viernes   \n",
      "3      6717.25  ...            0.0        10628.0  2020    7    4      Sabado   \n",
      "4      9527.25  ...            0.0        10996.0  2020    7    5     Domingo   \n",
      "\n",
      "   Temperatura_amb  Tarifa_electrica  estacion  Frio (Kw) tomorrow  \n",
      "0        18.919000      1.320630e+08    Verano             23954.0  \n",
      "1        21.044000      2.684737e+08    Verano             28268.0  \n",
      "2        24.950249      3.148428e+08    Verano             24246.0  \n",
      "3        25.531500      3.150148e+08    Verano             29885.0  \n",
      "4        27.466917      3.248102e+08    Verano             24449.0  \n",
      "\n",
      "[5 rows x 107 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = '../data/processed'\n",
    "filename = 'dataset_final.csv'\n",
    "file_path = os.path.join(folder, filename)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=',',\n",
    "        decimal='.'\n",
    "    )\n",
    "    df = df.sort_values(by='DIA', ignore_index=True)\n",
    "    print(f\"✅ DataFrame cargado exitosamente desde: {file_path}\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: No se encontró el archivo en la ruta: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c72d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dataset: (1183, 107)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1️⃣ Cargar dataset definitivo\n",
    "ruta_csv = \"../data/processed/X_test_preproc.csv\"\n",
    "X_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/X_train_preproc.csv\"\n",
    "X_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_test.csv\"\n",
    "y_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_train.csv\"\n",
    "y_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "\n",
    "\n",
    "print(\"Shape dataset:\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f71077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_float(\"max_features\", 0.1, 1.0, log=True)\n",
    "\n",
    "    # Modelo\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f44e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True) # L1\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True) # L2\n",
    "\n",
    "    # Modelo\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a1e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm_mae(trial):\n",
    "    # Hiperparámetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000, step=100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 20, 150)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True) # L1\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True) # L2\n",
    "\n",
    "    # Modelo\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        num_leaves=num_leaves,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # CV y Métrica (MAE)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6375cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_elasticnet_mae(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0) \n",
    "\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f499",
   "metadata": {},
   "source": [
    "# 1. Crear los estudios (todos buscan minimizar el MAE)\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_lgbm_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# 2. Ejecutar las optimizaciones\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=50)\n",
    "\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando LightGBM (MAE)...\")\n",
    "study_lgbm_mae.optimize(objective_lgbm_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=40)\n",
    "\n",
    "# 3. Ver los mejores parámetros\n",
    "print(f\"Mejor RF (MAE): {study_rf_mae.best_value} con params {study_rf_mae.best_params}\")\n",
    "print(f\"Mejor XGB (MAE): {study_xgb_mae.best_value} con params {study_xgb_mae.best_params}\")\n",
    "print(f\"Mejor LGBM (MAE): {study_lgbm_mae.best_value} con params {study_lgbm_mae.best_params}\")\n",
    "print(f\"Mejor ElasticNet (MAE): {study_elasticnet_mae.best_value} con params {study_elasticnet_mae.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82ff312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_experiment(model_name: str, \n",
    "                   model_object: object, \n",
    "                   study: optuna.study.Study, \n",
    "                   X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con los mejores parámetros de un estudio de Optuna,\n",
    "    calcula métricas y las guarda en un CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Definir la ruta del log ---\n",
    "    log_dir = \"results\"\n",
    "    log_file = os.path.join(log_dir, \"experiment_logs.csv\")\n",
    "    os.makedirs(log_dir, exist_ok=True) # Crea la carpeta si no existe\n",
    "\n",
    "    # --- 2. Obtener mejores parámetros y valor del estudio ---\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value # El MAE de CV que encontró Optuna\n",
    "    \n",
    "    # --- 3. Instanciar y entrenar el modelo final ---\n",
    "    # Manejo especial para modelos lineales que necesitan Pipeline\n",
    "    if \"pipeline\" in model_name.lower():\n",
    "        # El pipeline ya está definido en la función objective, \n",
    "        # aquí solo re-creamos el mejor.\n",
    "        if model_name == \"Pipeline_ElasticNet\":\n",
    "             # Re-crea el pipeline con los mejores params\n",
    "             model =ElasticNet(\n",
    "                    alpha=best_params.get('alpha'), \n",
    "                    l1_ratio=best_params.get('l1_ratio'),\n",
    "                    random_state=42, \n",
    "                    max_iter=2000\n",
    "                )\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # Para modelos no-pipeline (RF, XGB, LGBM)\n",
    "        model = model_object(**best_params, random_state=42, n_jobs=-1)\n",
    "        # Ajustes para modelos que no les gusta 'n_jobs' o necesitan 'verbose'\n",
    "        if isinstance(model, (xgb.XGBRegressor, lgb.LGBMRegressor)):\n",
    "            model.set_params(verbosity=0) if isinstance(model, xgb.XGBRegressor) else None\n",
    "            model.set_params(verbose=-1) if isinstance(model, lgb.LGBMRegressor) else None\n",
    "\n",
    "    # Entrenar el modelo final con todos los datos de train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- 4. Calcular Métricas (Train y Test) ---\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_name': model_name,\n",
    "        'optuna_cv_mae': best_value, # Métrica optimizada\n",
    "        'mae_train': mean_absolute_error(y_train, y_pred_train),\n",
    "        'mae_test': mean_absolute_error(y_test, y_pred_test),\n",
    "        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'r2_train': r2_score(y_train, y_pred_train),\n",
    "        'r2_test': r2_score(y_test, y_pred_test),\n",
    "        'hyperparameters': json.dumps(best_params) # Guardar params como string JSON\n",
    "    }\n",
    "    \n",
    "    # --- 5. Escribir en el CSV ---\n",
    "    file_exists = os.path.isfile(log_file)\n",
    "    \n",
    "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=metrics.keys())\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader() # Escribir cabecera solo si el archivo es nuevo\n",
    "            \n",
    "        writer.writerow(metrics)\n",
    "        \n",
    "    print(f\"Resultados de '{model_name}' guardados en {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32cb1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 20:17:18,722] A new study created in memory with name: no-name-c402ec4b-3951-4ea9-bec9-98ed86a5b1d4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizando Random Forest (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 20:17:26,935] Trial 0 finished with value: 68009.3376617101 and parameters: {'n_estimators': 200, 'max_depth': 27, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.17512321837013087}. Best is trial 0 with value: 68009.3376617101.\n",
      "[I 2025-11-13 20:17:32,297] Trial 1 finished with value: 65756.35009985275 and parameters: {'n_estimators': 100, 'max_depth': 13, 'min_samples_split': 13, 'min_samples_leaf': 6, 'max_features': 0.8065810196908856}. Best is trial 1 with value: 65756.35009985275.\n",
      "[I 2025-11-13 20:17:36,414] Trial 2 finished with value: 64778.22396586195 and parameters: {'n_estimators': 950, 'max_depth': 35, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_features': 0.1954827314142993}. Best is trial 2 with value: 64778.22396586195.\n",
      "[I 2025-11-13 20:17:37,680] Trial 3 finished with value: 66063.9624720562 and parameters: {'n_estimators': 300, 'max_depth': 25, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': 0.1623177212316744}. Best is trial 2 with value: 64778.22396586195.\n",
      "[I 2025-11-13 20:17:39,416] Trial 4 finished with value: 65808.14891927554 and parameters: {'n_estimators': 250, 'max_depth': 40, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': 0.3819671761388975}. Best is trial 2 with value: 64778.22396586195.\n",
      "[I 2025-11-13 20:17:40,077] Trial 5 finished with value: 69643.2498423306 and parameters: {'n_estimators': 100, 'max_depth': 42, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.14645200599895866}. Best is trial 2 with value: 64778.22396586195.\n",
      "[I 2025-11-13 20:17:42,740] Trial 6 finished with value: 65878.61783361781 and parameters: {'n_estimators': 550, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 6, 'max_features': 0.45217870827085854}. Best is trial 2 with value: 64778.22396586195.\n",
      "[I 2025-11-13 20:17:44,889] Trial 7 finished with value: 64928.02509402059 and parameters: {'n_estimators': 550, 'max_depth': 40, 'min_samples_split': 19, 'min_samples_leaf': 10, 'max_features': 0.21011718417225078}. Best is trial 2 with value: 64778.22396586195.\n",
      "[I 2025-11-13 20:17:52,967] Trial 8 finished with value: 54462.85417425501 and parameters: {'n_estimators': 450, 'max_depth': 29, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 0.8853929221630836}. Best is trial 8 with value: 54462.85417425501.\n",
      "[I 2025-11-13 20:17:59,600] Trial 9 finished with value: 64276.78541124661 and parameters: {'n_estimators': 800, 'max_depth': 9, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 0.5565040556312177}. Best is trial 8 with value: 54462.85417425501.\n",
      "[I 2025-11-13 20:18:05,927] Trial 10 finished with value: 54356.92210672968 and parameters: {'n_estimators': 400, 'max_depth': 50, 'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 0.9570733548178165}. Best is trial 10 with value: 54356.92210672968.\n",
      "[I 2025-11-13 20:18:11,830] Trial 11 finished with value: 54356.92210672968 and parameters: {'n_estimators': 400, 'max_depth': 50, 'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 0.9560609502398018}. Best is trial 10 with value: 54356.92210672968.\n",
      "[I 2025-11-13 20:18:15,713] Trial 12 finished with value: 63010.804819376695 and parameters: {'n_estimators': 400, 'max_depth': 47, 'min_samples_split': 17, 'min_samples_leaf': 3, 'max_features': 0.6628126678654312}. Best is trial 10 with value: 54356.92210672968.\n",
      "[I 2025-11-13 20:18:25,808] Trial 13 finished with value: 58785.0667095974 and parameters: {'n_estimators': 700, 'max_depth': 50, 'min_samples_split': 17, 'min_samples_leaf': 2, 'max_features': 0.992342767718341}. Best is trial 10 with value: 54356.92210672968.\n",
      "[I 2025-11-13 20:18:28,152] Trial 14 finished with value: 65657.41647360798 and parameters: {'n_estimators': 400, 'max_depth': 50, 'min_samples_split': 17, 'min_samples_leaf': 4, 'max_features': 0.2783263378056867}. Best is trial 10 with value: 54356.92210672968.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "[I 2025-11-13 20:18:30,114] A new study created in memory with name: no-name-f0fef70f-3004-4b92-bf32-e9be4d3bcc30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'RandomForest' guardados en results\\experiment_logs.csv\n",
      "Optimizando XGBoost (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 20:18:32,063] Trial 0 finished with value: 58050.63359375 and parameters: {'n_estimators': 300, 'learning_rate': 0.05103829963709337, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 1.0, 'reg_alpha': 0.00030609974707518904, 'reg_lambda': 3.177659753100075e-08}. Best is trial 0 with value: 58050.63359375.\n",
      "[I 2025-11-13 20:19:13,288] Trial 1 finished with value: 62266.0078125 and parameters: {'n_estimators': 1500, 'learning_rate': 0.01599164344722994, 'max_depth': 9, 'subsample': 1.0, 'colsample_bytree': 0.9, 'reg_alpha': 0.00019729851604466834, 'reg_lambda': 0.00020280871858668857}. Best is trial 0 with value: 58050.63359375.\n",
      "[I 2025-11-13 20:19:21,204] Trial 2 finished with value: 65068.8265625 and parameters: {'n_estimators': 400, 'learning_rate': 0.07084518170313625, 'max_depth': 8, 'subsample': 1.0, 'colsample_bytree': 0.8, 'reg_alpha': 5.653851477776792e-07, 'reg_lambda': 2.859738900015738e-06}. Best is trial 0 with value: 58050.63359375.\n",
      "[I 2025-11-13 20:19:51,746] Trial 3 finished with value: 70233.528125 and parameters: {'n_estimators': 1200, 'learning_rate': 0.054460409671215786, 'max_depth': 10, 'subsample': 0.6, 'colsample_bytree': 1.0, 'reg_alpha': 1.6996808116728467e-07, 'reg_lambda': 1.2980285061863859e-07}. Best is trial 0 with value: 58050.63359375.\n",
      "[I 2025-11-13 20:20:13,987] Trial 4 finished with value: 77464.71484375 and parameters: {'n_estimators': 1300, 'learning_rate': 0.06652166831709443, 'max_depth': 10, 'subsample': 0.6, 'colsample_bytree': 0.7, 'reg_alpha': 0.9098414416252546, 'reg_lambda': 0.000282102487790392}. Best is trial 0 with value: 58050.63359375.\n",
      "[I 2025-11-13 20:20:26,374] Trial 5 finished with value: 67211.6546875 and parameters: {'n_estimators': 400, 'learning_rate': 0.07454551070465094, 'max_depth': 10, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.025539714536388147, 'reg_lambda': 4.94735786117239e-08}. Best is trial 0 with value: 58050.63359375.\n",
      "[I 2025-11-13 20:20:29,151] Trial 6 finished with value: 51819.95234375 and parameters: {'n_estimators': 1100, 'learning_rate': 0.05665105473670123, 'max_depth': 3, 'subsample': 1.0, 'colsample_bytree': 0.9, 'reg_alpha': 0.011120091914287951, 'reg_lambda': 4.023608551177787e-06}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:03,169] Trial 7 finished with value: 73677.803125 and parameters: {'n_estimators': 1900, 'learning_rate': 0.05548839943193962, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_alpha': 0.01771381231849194, 'reg_lambda': 0.03709767179598866}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:12,513] Trial 8 finished with value: 59024.6232421875 and parameters: {'n_estimators': 1500, 'learning_rate': 0.010580737828957576, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 0.00031718837477585964, 'reg_lambda': 0.07382061306211098}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:19,036] Trial 9 finished with value: 81096.34140625 and parameters: {'n_estimators': 1200, 'learning_rate': 0.2075211828406739, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.6881284998275052, 'reg_lambda': 5.63937300300629e-05}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:21,172] Trial 10 finished with value: 57420.941015625 and parameters: {'n_estimators': 700, 'learning_rate': 0.18273428797291766, 'max_depth': 3, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 2.7191206714701487e-06, 'reg_lambda': 2.4078115158958153e-06}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:23,016] Trial 11 finished with value: 60926.2140625 and parameters: {'n_estimators': 800, 'learning_rate': 0.25267121134323595, 'max_depth': 3, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 3.0807593270699943e-06, 'reg_lambda': 3.0122948625208725e-06}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:24,555] Trial 12 finished with value: 54652.70390625 and parameters: {'n_estimators': 700, 'learning_rate': 0.14056301476064093, 'max_depth': 3, 'subsample': 1.0, 'colsample_bytree': 0.9, 'reg_alpha': 1.7862283345934164e-08, 'reg_lambda': 4.862701723224767e-06}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:32,079] Trial 13 finished with value: 66193.178125 and parameters: {'n_estimators': 800, 'learning_rate': 0.12055877343109202, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 0.8, 'reg_alpha': 1.1292530880554911e-08, 'reg_lambda': 0.003344837930215207}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:32,654] Trial 14 finished with value: 60194.1314453125 and parameters: {'n_estimators': 100, 'learning_rate': 0.028818252290593236, 'max_depth': 4, 'subsample': 1.0, 'colsample_bytree': 0.9, 'reg_alpha': 0.012688815315424283, 'reg_lambda': 1.6187287774694173e-05}. Best is trial 6 with value: 51819.95234375.\n",
      "[I 2025-11-13 20:21:34,211] A new study created in memory with name: no-name-bfc7160f-da7f-491b-bcbe-68e494c78f94\n",
      "[I 2025-11-13 20:21:34,314] Trial 0 finished with value: 133404.43867425324 and parameters: {'alpha': 0.003239833092982467, 'l1_ratio': 0.576256865799053}. Best is trial 0 with value: 133404.43867425324.\n",
      "[I 2025-11-13 20:21:34,353] Trial 1 finished with value: 84261.24053316866 and parameters: {'alpha': 0.5028139297092314, 'l1_ratio': 0.39454094436127196}. Best is trial 1 with value: 84261.24053316866.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'XGBoost' guardados en results\\experiment_logs.csv\n",
      "Optimizando ElasticNet (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 20:21:34,411] Trial 2 finished with value: 119544.8404353179 and parameters: {'alpha': 0.02528668473588323, 'l1_ratio': 0.5521020610225856}. Best is trial 1 with value: 84261.24053316866.\n",
      "[I 2025-11-13 20:21:34,450] Trial 3 finished with value: 73606.31610029723 and parameters: {'alpha': 1.8047631903149544, 'l1_ratio': 0.3048255801205134}. Best is trial 3 with value: 73606.31610029723.\n",
      "[I 2025-11-13 20:21:34,533] Trial 4 finished with value: 129497.78403168284 and parameters: {'alpha': 0.0036966243574217445, 'l1_ratio': 0.05498352891046454}. Best is trial 3 with value: 73606.31610029723.\n",
      "[I 2025-11-13 20:21:34,572] Trial 5 finished with value: 80105.4476917838 and parameters: {'alpha': 2.840369735597952, 'l1_ratio': 0.8177312290159995}. Best is trial 3 with value: 73606.31610029723.\n",
      "[I 2025-11-13 20:21:34,620] Trial 6 finished with value: 94216.26086279757 and parameters: {'alpha': 0.1865121655013306, 'l1_ratio': 0.46399797919167296}. Best is trial 3 with value: 73606.31610029723.\n",
      "[I 2025-11-13 20:21:34,712] Trial 7 finished with value: 135671.2697299334 and parameters: {'alpha': 0.0017179726268315443, 'l1_ratio': 0.8184373191048969}. Best is trial 3 with value: 73606.31610029723.\n",
      "[I 2025-11-13 20:21:34,759] Trial 8 finished with value: 97608.4191657386 and parameters: {'alpha': 0.9836690365652503, 'l1_ratio': 0.9270786145572065}. Best is trial 3 with value: 73606.31610029723.\n",
      "[I 2025-11-13 20:21:34,817] Trial 9 finished with value: 115659.62808915605 and parameters: {'alpha': 0.021227248665538486, 'l1_ratio': 0.25027122916980726}. Best is trial 3 with value: 73606.31610029723.\n",
      "[I 2025-11-13 20:21:34,849] Trial 10 finished with value: 66146.48441095543 and parameters: {'alpha': 5.024524047924258, 'l1_ratio': 0.026241783022902032}. Best is trial 10 with value: 66146.48441095543.\n",
      "[I 2025-11-13 20:21:34,881] Trial 11 finished with value: 64830.53499258847 and parameters: {'alpha': 7.950768957229087, 'l1_ratio': 0.010263646011041128}. Best is trial 11 with value: 64830.53499258847.\n",
      "[I 2025-11-13 20:21:34,912] Trial 12 finished with value: 64547.425507273525 and parameters: {'alpha': 9.691395506466543, 'l1_ratio': 0.03354274088580766}. Best is trial 12 with value: 64547.425507273525.\n",
      "[I 2025-11-13 20:21:34,943] Trial 13 finished with value: 64759.50409650411 and parameters: {'alpha': 9.818894204058036, 'l1_ratio': 0.16493044586272748}. Best is trial 12 with value: 64547.425507273525.\n",
      "[I 2025-11-13 20:21:34,973] Trial 14 finished with value: 65229.13945565695 and parameters: {'alpha': 7.876311681881224, 'l1_ratio': 0.16392586282000798}. Best is trial 12 with value: 64547.425507273525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'Pipeline_ElasticNet' guardados en results\\experiment_logs.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Ejecutar el estudio (ejemplo con Random Forest) ---\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_rf_mae' está definida)\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=15) \n",
    "\n",
    "# --- 2. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"RandomForest\",\n",
    "    model_object=RandomForestRegressor, # Pasa la clase del modelo\n",
    "    study=study_rf_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 3. Ejecutar el estudio (ejemplo con XGBoost) ---\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_xgb_mae' está definida)\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=15)\n",
    "\n",
    "# --- 4. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"XGBoost\",\n",
    "    model_object=xgb.XGBRegressor, # Pasa la clase del modelo\n",
    "    study=study_xgb_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 5. Ejecutar el estudio (ejemplo con ElasticNet) ---\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "# (Aquí asumimos que 'objective_elasticnet_mae' está definida)\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=15)\n",
    "\n",
    "# --- 6. Registrar el resultado ---\n",
    "# Nota: 'model_object' es None porque se maneja dentro del 'if'\n",
    "log_experiment(\n",
    "    model_name=\"Pipeline_ElasticNet\",\n",
    "    model_object=None, \n",
    "    study=study_elasticnet_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# Repetir para LightGBM..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MostoElMostro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
