{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6fc2c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataFrame cargado exitosamente desde: ../data/processed\\dataset_final.csv\n",
      "Dimensiones: (1184, 394)\n",
      "\n",
      "Primeras 5 filas:\n",
      "          DIA  EE Planta / Hl  EE Elaboracion / Hl  EE Bodega / Hl  \\\n",
      "0  2020-07-01      642.727209            47.145349       69.023256   \n",
      "1  2020-07-02        7.767254             0.769609        0.798838   \n",
      "2  2020-07-03        8.801205             0.862593        0.835762   \n",
      "3  2020-07-04        5.175639             0.439225        0.371077   \n",
      "4  2020-07-05        7.924665             0.802365        0.717787   \n",
      "\n",
      "   EE Cocina / Hl  EE Envasado / Hl  EE Linea 2 / Hl  EE Linea 3 / Hl  \\\n",
      "0        0.000000         13.813953        14.578784         0.000000   \n",
      "1        0.319229          2.358593         4.158962         1.506838   \n",
      "2        0.260924          1.985462        39.076667         1.448962   \n",
      "3        0.258048          1.442114         4.348182         1.355238   \n",
      "4        0.301592          1.664726         5.125920         2.704348   \n",
      "\n",
      "   EE Linea 4 / Hl  EE Servicios / Hl  ...  Prod Agua (Kw)_lag1  \\\n",
      "0         0.000000         554.604651  ...                  NaN   \n",
      "1         1.521823           5.429388  ...                188.0   \n",
      "2         1.500923           5.703346  ...               -164.0   \n",
      "3         1.536507           3.058399  ...                943.0   \n",
      "4         1.471990           5.094301  ...               1011.0   \n",
      "\n",
      "   Prod Agua (Kw)_lag2  Prod Agua (Kw)_lag3  Resto Serv (Kw)_lag1  \\\n",
      "0                  NaN                  NaN                   NaN   \n",
      "1                  NaN                  NaN                2241.0   \n",
      "2                188.0                  NaN                3812.0   \n",
      "3               -164.0                188.0                3302.5   \n",
      "4                943.0               -164.0                3998.0   \n",
      "\n",
      "   Resto Serv (Kw)_lag2  Resto Serv (Kw)_lag3  Restos Planta (Kw)_lag1  \\\n",
      "0                   NaN                   NaN                      NaN   \n",
      "1                   NaN                   NaN                  1168.02   \n",
      "2                2241.0                   NaN                 -5536.32   \n",
      "3                3812.0                2241.0                  1864.34   \n",
      "4                3302.5                3812.0                  3058.29   \n",
      "\n",
      "   Restos Planta (Kw)_lag2  Restos Planta (Kw)_lag3  Frio (Kw) tomorrow  \n",
      "0                      NaN                      NaN             23954.0  \n",
      "1                      NaN                      NaN             28268.0  \n",
      "2                  1168.02                      NaN             24246.0  \n",
      "3                 -5536.32                  1168.02             29885.0  \n",
      "4                  1864.34                 -5536.32             24449.0  \n",
      "\n",
      "[5 rows x 394 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder = '../data/processed'\n",
    "filename = 'dataset_final.csv'\n",
    "file_path = os.path.join(folder, filename)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        sep=',',\n",
    "        decimal='.'\n",
    "    )\n",
    "    df = df.sort_values(by='DIA', ignore_index=True)\n",
    "    print(f\"✅ DataFrame cargado exitosamente desde: {file_path}\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras 5 filas:\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: No se encontró el archivo en la ruta: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0c72d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (828, 40) y_train: (828,)\n",
      "X_test: (356, 40) y_test: (356,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1️⃣ Cargar dataset definitivo\n",
    "ruta_csv = \"../data/processed/X_test_preproc.csv\"\n",
    "X_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/X_train_preproc.csv\"\n",
    "X_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_test.csv\"\n",
    "y_test = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "ruta_csv = \"../data/processed/y_train.csv\"\n",
    "y_train = pd.read_csv(ruta_csv, sep=',', decimal='.')\n",
    "\n",
    "y_train = y_train.iloc[:, 0].values.ravel()\n",
    "y_test  = y_test.iloc[:, 0].values.ravel()\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f71077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf_mae(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_float(\"max_features\", 0.1, 1.0, log=True)\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_mae = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        fold_mae.append(mae)\n",
    "\n",
    "        # Reportar al pruner\n",
    "        trial.report(mae, fold_idx)\n",
    "\n",
    "        # Podar si es necesario\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f44e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_mae(trial):\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        subsample=trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1),\n",
    "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1),\n",
    "        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_mae = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        fold_mae.append(mae)\n",
    "\n",
    "        trial.report(mae, fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm_mae(trial):\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 100, 2000, step=100),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        num_leaves=trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "        subsample=trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1),\n",
    "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1),\n",
    "        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_mae = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        fold_mae.append(mae)\n",
    "\n",
    "        trial.report(mae, fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_elasticnet_mae(trial):\n",
    "    model = ElasticNet(\n",
    "        alpha=trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "        l1_ratio=trial.suggest_float(\"l1_ratio\", 0.0, 1.0),\n",
    "        random_state=42,\n",
    "        max_iter=2000\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_mae = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        fold_mae.append(mae)\n",
    "\n",
    "        trial.report(mae, fold_idx)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f499",
   "metadata": {},
   "source": [
    "# 1. Crear los estudios (todos buscan minimizar el MAE)\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_lgbm_mae = optuna.create_study(direction=\"minimize\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# 2. Ejecutar las optimizaciones\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=50)\n",
    "\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando LightGBM (MAE)...\")\n",
    "study_lgbm_mae.optimize(objective_lgbm_mae, n_trials=75)\n",
    "\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=40)\n",
    "\n",
    "# 3. Ver los mejores parámetros\n",
    "print(f\"Mejor RF (MAE): {study_rf_mae.best_value} con params {study_rf_mae.best_params}\")\n",
    "print(f\"Mejor XGB (MAE): {study_xgb_mae.best_value} con params {study_xgb_mae.best_params}\")\n",
    "print(f\"Mejor LGBM (MAE): {study_lgbm_mae.best_value} con params {study_lgbm_mae.best_params}\")\n",
    "print(f\"Mejor ElasticNet (MAE): {study_elasticnet_mae.best_value} con params {study_elasticnet_mae.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e82ff312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_experiment(model_name: str, \n",
    "                   model_object: object, \n",
    "                   study: optuna.study.Study, \n",
    "                   X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con los mejores parámetros de un estudio de Optuna,\n",
    "    calcula métricas y las guarda en un CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Definir la ruta del log ---\n",
    "    log_dir = \"results\"\n",
    "    log_file = os.path.join(log_dir, \"experiment_logs.csv\")\n",
    "    os.makedirs(log_dir, exist_ok=True) # Crea la carpeta si no existe\n",
    "\n",
    "    # --- 2. Obtener mejores parámetros y valor del estudio ---\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value # El MAE de CV que encontró Optuna\n",
    "    \n",
    "    # --- 3. Instanciar y entrenar el modelo final ---\n",
    "    # Manejo especial para modelos lineales que necesitan Pipeline\n",
    "    if \"pipeline\" in model_name.lower():\n",
    "        # El pipeline ya está definido en la función objective, \n",
    "        # aquí solo re-creamos el mejor.\n",
    "        if model_name == \"Pipeline_ElasticNet\":\n",
    "             # Re-crea el pipeline con los mejores params\n",
    "             model =ElasticNet(\n",
    "                    alpha=best_params.get('alpha'), \n",
    "                    l1_ratio=best_params.get('l1_ratio'),\n",
    "                    random_state=42, \n",
    "                    max_iter=2000\n",
    "                )\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # Para modelos no-pipeline (RF, XGB, LGBM)\n",
    "        model = model_object(**best_params, random_state=42, n_jobs=-1)\n",
    "        # Ajustes para modelos que no les gusta 'n_jobs' o necesitan 'verbose'\n",
    "        if isinstance(model, (xgb.XGBRegressor, lgb.LGBMRegressor)):\n",
    "            model.set_params(verbosity=0) if isinstance(model, xgb.XGBRegressor) else None\n",
    "            model.set_params(verbose=-1) if isinstance(model, lgb.LGBMRegressor) else None\n",
    "\n",
    "    # Entrenar el modelo final con todos los datos de train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- 4. Calcular Métricas (Train y Test) ---\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_name': model_name,\n",
    "        'optuna_cv_mae': best_value, # Métrica optimizada\n",
    "        'mae_train': mean_absolute_error(y_train, y_pred_train),\n",
    "        'mae_test': mean_absolute_error(y_test, y_pred_test),\n",
    "        'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'r2_train': r2_score(y_train, y_pred_train),\n",
    "        'r2_test': r2_score(y_test, y_pred_test),\n",
    "        'hyperparameters': json.dumps(best_params) # Guardar params como string JSON\n",
    "    }\n",
    "    \n",
    "    # --- 5. Escribir en el CSV ---\n",
    "    file_exists = os.path.isfile(log_file)\n",
    "    \n",
    "    with open(log_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=metrics.keys())\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader() # Escribir cabecera solo si el archivo es nuevo\n",
    "            \n",
    "        writer.writerow(metrics)\n",
    "        \n",
    "    print(f\"Resultados de '{model_name}' guardados en {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "32cb1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 20:40:15,593] A new study created in memory with name: no-name-878ae403-80c0-41ad-acc1-dface604710d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizando Random Forest (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 20:40:21,821] Trial 0 finished with value: -10977.056876404415 and parameters: {'n_estimators': 250, 'max_depth': 42, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 0.839549239303193}. Best is trial 0 with value: -10977.056876404415.\n",
      "[I 2025-11-15 20:40:30,633] Trial 1 finished with value: -8970.241214176727 and parameters: {'n_estimators': 750, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 0.7068953429680763}. Best is trial 0 with value: -10977.056876404415.\n",
      "[I 2025-11-15 20:40:34,512] Trial 2 finished with value: -8946.058383797397 and parameters: {'n_estimators': 400, 'max_depth': 22, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 0.15143932764268447}. Best is trial 0 with value: -10977.056876404415.\n",
      "[I 2025-11-15 20:40:44,112] Trial 3 finished with value: -11092.34241235886 and parameters: {'n_estimators': 450, 'max_depth': 20, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 0.9971140444574218}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:40:46,653] Trial 4 finished with value: -9009.49537012506 and parameters: {'n_estimators': 250, 'max_depth': 39, 'min_samples_split': 17, 'min_samples_leaf': 8, 'max_features': 0.11360618613758011}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:40:48,300] Trial 5 finished with value: -9125.083279310216 and parameters: {'n_estimators': 150, 'max_depth': 41, 'min_samples_split': 17, 'min_samples_leaf': 8, 'max_features': 0.2049026498065913}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:40:49,391] Trial 6 finished with value: -9082.336207348208 and parameters: {'n_estimators': 100, 'max_depth': 26, 'min_samples_split': 11, 'min_samples_leaf': 8, 'max_features': 0.17600369730659074}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:40:57,303] Trial 7 finished with value: -9053.691900644373 and parameters: {'n_estimators': 450, 'max_depth': 32, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 0.8771107983859784}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:41:06,186] Trial 8 finished with value: -9034.331507711699 and parameters: {'n_estimators': 650, 'max_depth': 42, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 0.7518637694638229}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:41:14,845] Trial 9 finished with value: -9050.334742308693 and parameters: {'n_estimators': 800, 'max_depth': 30, 'min_samples_split': 17, 'min_samples_leaf': 8, 'max_features': 0.26155455369430713}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:41:26,007] Trial 10 finished with value: -9912.994924579103 and parameters: {'n_estimators': 900, 'max_depth': 13, 'min_samples_split': 11, 'min_samples_leaf': 1, 'max_features': 0.4669940021509166}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:41:31,113] Trial 11 finished with value: -10111.33070644751 and parameters: {'n_estimators': 350, 'max_depth': 16, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 0.4644647887377684}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:41:43,365] Trial 12 finished with value: -9098.360175855669 and parameters: {'n_estimators': 550, 'max_depth': 50, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 0.9927072019401206}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:41:46,726] Trial 13 finished with value: -9442.239929010186 and parameters: {'n_estimators': 250, 'max_depth': 49, 'min_samples_split': 11, 'min_samples_leaf': 2, 'max_features': 0.46755865957696435}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:41:53,526] Trial 14 finished with value: -9239.66672566679 and parameters: {'n_estimators': 550, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': 0.5994313270730907}. Best is trial 3 with value: -11092.34241235886.\n",
      "[I 2025-11-15 20:41:55,936] A new study created in memory with name: no-name-c72256cd-df7e-4be5-a383-2bf48904b88e\n",
      "[I 2025-11-15 20:41:55,985] Trial 0 finished with value: -10839.402447340406 and parameters: {'alpha': 0.8111827019624849, 'l1_ratio': 0.19013815135666}. Best is trial 0 with value: -10839.402447340406.\n",
      "[I 2025-11-15 20:41:56,094] Trial 1 finished with value: -15763.004532729527 and parameters: {'alpha': 0.07272492020870018, 'l1_ratio': 0.2630863394941385}. Best is trial 1 with value: -15763.004532729527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'RandomForest' guardados en results\\experiment_logs.csv\n",
      "Optimizando ElasticNet (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 20:41:56,253] Trial 2 finished with value: -18043.043782997614 and parameters: {'alpha': 0.06880832755147874, 'l1_ratio': 0.6828877498453998}. Best is trial 2 with value: -18043.043782997614.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.320e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.398e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.342e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.385e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:56,626] Trial 3 finished with value: -22207.41719519789 and parameters: {'alpha': 0.005181147581166048, 'l1_ratio': 0.4283244326313336}. Best is trial 3 with value: -22207.41719519789.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.356e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.470e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.500e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:56,958] Trial 4 finished with value: -23215.876520855916 and parameters: {'alpha': 0.003957932328463702, 'l1_ratio': 0.7640527888218961}. Best is trial 4 with value: -23215.876520855916.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.517e+08, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.248e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.255e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:57,270] Trial 5 finished with value: -23122.489630553024 and parameters: {'alpha': 0.07425678779995096, 'l1_ratio': 0.9853741187281612}. Best is trial 4 with value: -23215.876520855916.\n",
      "[I 2025-11-15 20:41:57,298] Trial 6 finished with value: -8833.40052095589 and parameters: {'alpha': 5.285242985147806, 'l1_ratio': 0.14396540809132508}. Best is trial 4 with value: -23215.876520855916.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.943e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.379e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.278e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.378e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:57,625] Trial 7 finished with value: -22539.658180729988 and parameters: {'alpha': 0.006378747019246673, 'l1_ratio': 0.6537268312453904}. Best is trial 4 with value: -23215.876520855916.\n",
      "[I 2025-11-15 20:41:57,832] Trial 8 finished with value: -21227.044928554456 and parameters: {'alpha': 3.2966044189601402, 'l1_ratio': 0.9983368223479483}. Best is trial 4 with value: -23215.876520855916.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.326e+10, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.069e+09, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.440e+09, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.128e+09, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:58,146] Trial 9 finished with value: -20804.764805961284 and parameters: {'alpha': 0.025091203010916618, 'l1_ratio': 0.7209623741841682}. Best is trial 4 with value: -23215.876520855916.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.470e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.498e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:58,433] Trial 10 finished with value: -23693.35355977373 and parameters: {'alpha': 0.0013660095144075095, 'l1_ratio': 0.828659207554467}. Best is trial 10 with value: -23693.35355977373.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.601e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.470e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.503e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:58,723] Trial 11 finished with value: -23717.783306712394 and parameters: {'alpha': 0.0010997070829364746, 'l1_ratio': 0.8162269366502577}. Best is trial 11 with value: -23717.783306712394.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.600e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.468e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:58,998] Trial 12 finished with value: -23763.752423010716 and parameters: {'alpha': 0.0013284768756706836, 'l1_ratio': 0.8916494787867553}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.622e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.476e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.501e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.508e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:59,264] Trial 13 finished with value: -23466.639089333403 and parameters: {'alpha': 0.0010340651086633062, 'l1_ratio': 0.4711406231545664}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.267e+08, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.936e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.908e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.440e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:59,560] Trial 14 finished with value: -22270.424185065564 and parameters: {'alpha': 0.01890212305811444, 'l1_ratio': 0.8511830617750014}. Best is trial 12 with value: -23763.752423010716.\n",
      "[I 2025-11-15 20:41:59,609] Trial 15 finished with value: -11041.867053126261 and parameters: {'alpha': 0.5818239199370703, 'l1_ratio': 0.0100882028979431}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.484e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.480e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.503e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.509e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:41:59,928] Trial 16 finished with value: -23115.39929986805 and parameters: {'alpha': 0.0025207976693104183, 'l1_ratio': 0.5634875424164598}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.840e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.451e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.471e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.480e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.467e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:00,360] Trial 17 finished with value: -23263.452266666012 and parameters: {'alpha': 0.00946097378445314, 'l1_ratio': 0.909407194624949}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.445e+10, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.248e+09, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.429e+10, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.085e+10, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:00,653] Trial 18 finished with value: -20733.412981042573 and parameters: {'alpha': 0.01758609029042384, 'l1_ratio': 0.58755242102011}. Best is trial 12 with value: -23763.752423010716.\n",
      "[I 2025-11-15 20:42:00,717] Trial 19 finished with value: -13371.41822410383 and parameters: {'alpha': 0.26111508042560055, 'l1_ratio': 0.39310103580950767}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.540e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.470e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.498e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.503e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:00,991] Trial 20 finished with value: -23697.066826320144 and parameters: {'alpha': 0.0020001031000731273, 'l1_ratio': 0.8854255917146721}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.525e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.471e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.498e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:01,261] Trial 21 finished with value: -23680.636517207044 and parameters: {'alpha': 0.002026239844016451, 'l1_ratio': 0.8762270789858114}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.588e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.471e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.498e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:01,523] Trial 22 finished with value: -23686.277164540643 and parameters: {'alpha': 0.0011504475273939466, 'l1_ratio': 0.7884543218598217}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.487e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.470e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.503e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:01,785] Trial 23 finished with value: -23698.34929275921 and parameters: {'alpha': 0.0029040365790695194, 'l1_ratio': 0.9216758572400477}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.960e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.469e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.472e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:02,060] Trial 24 finished with value: -23404.13952262178 and parameters: {'alpha': 0.009636430293550893, 'l1_ratio': 0.933673638855797}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.441e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.475e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.498e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.495e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:02,336] Trial 25 finished with value: -23362.948610442472 and parameters: {'alpha': 0.003475248413217309, 'l1_ratio': 0.7980609942381384}. Best is trial 12 with value: -23763.752423010716.\n",
      "[I 2025-11-15 20:42:02,592] Trial 26 finished with value: -19839.22526537117 and parameters: {'alpha': 0.028344756506259575, 'l1_ratio': 0.6168466965352436}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.133e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.387e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.355e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.235e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.377e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:02,924] Trial 27 finished with value: -22629.014591408642 and parameters: {'alpha': 0.007894921411528978, 'l1_ratio': 0.7444135526719435}. Best is trial 12 with value: -23763.752423010716.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.570e+09, tolerance: 4.230e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.468e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+12, tolerance: 7.755e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.495e+12, tolerance: 7.757e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.502e+12, tolerance: 7.754e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:03,273] Trial 28 finished with value: -23785.900424457774 and parameters: {'alpha': 0.002333279884404135, 'l1_ratio': 0.9500650868850711}. Best is trial 28 with value: -23785.900424457774.\n",
      "[I 2025-11-15 20:42:03,568] Trial 29 finished with value: -20993.204869438323 and parameters: {'alpha': 0.21309004248981658, 'l1_ratio': 0.9701817555024839}. Best is trial 28 with value: -23785.900424457774.\n",
      "c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.558e+12, tolerance: 7.768e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "[I 2025-11-15 20:42:03,634] A new study created in memory with name: no-name-bc07bf7c-1768-4748-9c43-1be81f4594ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de 'Pipeline_ElasticNet' guardados en results\\experiment_logs.csv\n",
      "Optimizando XGBoost (MAE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 20:42:20,685] Trial 0 finished with value: -14308.502190085806 and parameters: {'n_estimators': 800, 'learning_rate': 0.23720045769523368, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_alpha': 1.8016193468134926e-05, 'reg_lambda': 0.0007089746118842438}. Best is trial 0 with value: -14308.502190085806.\n",
      "[I 2025-11-15 20:42:23,082] Trial 1 finished with value: -10773.62625600413 and parameters: {'n_estimators': 300, 'learning_rate': 0.04493292675645245, 'max_depth': 4, 'subsample': 0.8, 'colsample_bytree': 0.7, 'reg_alpha': 5.5811534500765627e-05, 'reg_lambda': 0.6075320661292568}. Best is trial 0 with value: -14308.502190085806.\n",
      "[I 2025-11-15 20:42:59,295] Trial 2 finished with value: -11663.277184253706 and parameters: {'n_estimators': 2000, 'learning_rate': 0.01770500100652277, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 1.0, 'reg_alpha': 2.7802464885467577e-05, 'reg_lambda': 0.02806956384529288}. Best is trial 0 with value: -14308.502190085806.\n",
      "[W 2025-11-15 20:43:39,415] Trial 3 failed with parameters: {'n_estimators': 1000, 'learning_rate': 0.04408448964349153, 'max_depth': 10, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.003798096178127878, 'reg_lambda': 1.2341082325943052e-05} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_11840\\1813211451.py\", line 22, in objective_xgb_mae\n",
      "    model.fit(X_tr, y_tr)\n",
      "  File \"c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1365, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\training.py\", line 199, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\core.py\", line 2434, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-15 20:43:39,446] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m study_xgb_mae = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m, pruner=optuna.pruners.MedianPruner(n_warmup_steps=\u001b[32m5\u001b[39m))\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# (Aquí asumimos que 'objective_xgb_mae' está definida)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mstudy_xgb_mae\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_xgb_mae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# --- 4. Registrar el resultado ---\u001b[39;00m\n\u001b[32m     39\u001b[39m log_experiment(\n\u001b[32m     40\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m     model_object=xgb.XGBRegressor, \u001b[38;5;66;03m# Pasa la clase del modelo\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     X_test=X_test, y_test=y_test\n\u001b[32m     45\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mobjective_xgb_mae\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     19\u001b[39m X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n\u001b[32m     20\u001b[39m y_tr, y_val = y_train[train_idx], y_train[val_idx]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m preds = model.predict(X_val)\n\u001b[32m     25\u001b[39m mae = mean_absolute_error(y_val, preds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1365\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1363\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\franc\\Desktop\\MostoElMostro\\MostoElMostro\\.venv\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 1. Ejecutar el estudio (ejemplo con Random Forest) ---\n",
    "print(\"Optimizando Random Forest (MAE)...\")\n",
    "study_rf_mae = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "# (Aquí asumimos que 'objective_rf_mae' está definida)\n",
    "study_rf_mae.optimize(objective_rf_mae, n_trials=15) \n",
    "\n",
    "# --- 2. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"RandomForest\",\n",
    "    model_object=RandomForestRegressor, # Pasa la clase del modelo\n",
    "    study=study_rf_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 5. Ejecutar el estudio (ejemplo con ElasticNet) ---\n",
    "print(\"Optimizando ElasticNet (MAE)...\")\n",
    "study_elasticnet_mae = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "# (Aquí asumimos que 'objective_elasticnet_mae' está definida)\n",
    "study_elasticnet_mae.optimize(objective_elasticnet_mae, n_trials=30)\n",
    "\n",
    "# --- 6. Registrar el resultado ---\n",
    "# Nota: 'model_object' es None porque se maneja dentro del 'if'\n",
    "log_experiment(\n",
    "    model_name=\"Pipeline_ElasticNet\",\n",
    "    model_object=None, \n",
    "    study=study_elasticnet_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 3. Ejecutar el estudio (ejemplo con XGBoost) ---\n",
    "print(\"Optimizando XGBoost (MAE)...\")\n",
    "study_xgb_mae = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "# (Aquí asumimos que 'objective_xgb_mae' está definida)\n",
    "study_xgb_mae.optimize(objective_xgb_mae, n_trials=5)\n",
    "\n",
    "# --- 4. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"XGBoost\",\n",
    "    model_object=xgb.XGBRegressor, # Pasa la clase del modelo\n",
    "    study=study_xgb_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# --- 5. Ejecutar el estudio (ejemplo con LGBM) ---\n",
    "print(\"Optimizando LightGBM (MAE)...\")\n",
    "study_lgbm_mae = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study_lgbm_mae.optimize(objective_lgbm_mae, n_trials=10)\n",
    "\n",
    "# --- 6. Registrar el resultado ---\n",
    "log_experiment(\n",
    "    model_name=\"LightGBM\",\n",
    "    model_object=lgb.LGBMRegressor,  # Pasamos la clase del modelo\n",
    "    study=study_lgbm_mae,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MostoElMostro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
